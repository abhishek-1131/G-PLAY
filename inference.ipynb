{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4536eea-1c59-44e9-b6cd-b4d898b3350f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0')\n",
      "2.1.0\n",
      "graph-tool version: 2.46\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(torch.zeros(1).cuda())\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "print(torch_geometric.__version__)\n",
    "\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType, OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "import networkx as nx\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import deepsnap\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "from pathlib import Path as Data_Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import json\n",
    "print(\"graph-tool version: {}\".format(gt.__version__.split(' ')[0]))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a0b8ab8-6def-4240-bdf5-7a6d0efd7571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../g_nx.pickle', 'rb') as f:\n",
    "    g_nx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5981b942-44d5-46b9-ba00-ee1a7270a8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35042"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary to store the attribute values\n",
    "attr_dict = {}\n",
    "for node in g_nx.nodes():\n",
    "    attr = g_nx.nodes[node]['uri']\n",
    "    if attr in attr_dict:\n",
    "        attr_dict[attr].append(node)\n",
    "    else:\n",
    "        attr_dict[attr] = [node]\n",
    "        \n",
    "del attr_dict['']\n",
    "len(attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34ab757a-af6d-4b71-94fc-59e9755d1081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n"
     ]
    }
   ],
   "source": [
    "keys_with_duplicates = [key for key, values in attr_dict.items() if len(values) > 1]\n",
    "\n",
    "# print the keys with duplicates\n",
    "print(len(keys_with_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8ddde2c-3b52-4add-8ab4-6afa0fb8a304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spotify:track:0WqIKmW4BTrj3eJFmnCKMv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_with_duplicates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b1fe166-5f89-44e4-825d-ea64cec11a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2001, 38488]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_dict['spotify:track:0WqIKmW4BTrj3eJFmnCKMv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e58361-f316-4277-a39e-7eecf6ea1d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../datasets_deepsnap_dict.pkl', 'rb') as f:\n",
    "    # load the pickled dictionary from the file\n",
    "    datasets = pickle.load(f)\n",
    "    \n",
    "ds_graph = Graph(g_nx)\n",
    "\n",
    "task = 'link_pred'\n",
    "dataset = GraphDataset([ds_graph], task=task, edge_train_mode='disjoint')\n",
    "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "num_test_edges = dataset_test[0].edge_label_index.shape[1]\n",
    "\n",
    "print(\"Test set has {} supervision (positive) edges\".format(num_test_edges // 2))\n",
    "print(\"Test set has {} message passing edges\".format(dataset_test[0].edge_index.shape[1]))+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb4425-2e72-4b2f-b58f-3d08eb407bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(LightGCNConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        out = self.propagate(edge_index, x=(x, x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        out = x_j\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        node_dim = self.node_dim\n",
    "        out = torch_scatter.scatter(inputs, index, dim=node_dim, reduce='mean')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cfe51-2013-4d09-868f-0dd14490787a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCN(torch.nn.Module):\n",
    "    def __init__(self, train_data, num_layers, emb_size=16, initialize_with_words=False):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        assert (num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(num_layers):\n",
    "            self.convs.append(LightGCNConv(input_dim, input_dim))\n",
    "\n",
    "        # Initialize using custom embeddings if provided\n",
    "        num_nodes = train_data.node_label_index.size()[0]\n",
    "        self.embeddings = nn.Embedding(num_nodes, emb_size)\n",
    "        if initialize_with_words:\n",
    "            self.embeddings.weight.data.copy_(train_datanode_features)\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_size = emb_size\n",
    "        self.num_modes = num_nodes\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_label_index, node_label_index = data.edge_index, data.edge_label_index, data.node_label_index\n",
    "        layer_embeddings = []\n",
    "        \n",
    "        x = self.embeddings(node_label_index)\n",
    "        mean_layer = x\n",
    "\n",
    "        # We take an average of ever layer's node embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            # print(\"x shape\",x.shape)\n",
    "            # print(\"mean_layer shape\",mean_layer.shape)\n",
    "            mean_layer += x\n",
    "\n",
    "        mean_layer /= 4\n",
    "\n",
    "        # Prediction head is simply dot product\n",
    "        nodes_first = torch.index_select(x, 0, edge_label_index[0,:].long())\n",
    "        nodes_second = torch.index_select(x, 0, edge_label_index[1,:].long())\n",
    "\n",
    "        # Since we don't want a rank output, we create a sigmoid of the dot product\n",
    "        out = torch.sum(nodes_first * nodes_second, dim=-1) # FOR RANKING\n",
    "        pred = torch.sigmoid(out)\n",
    "\n",
    "        return torch.flatten(pred)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return self.loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359012e-bd34-4a7e-a8a4-0349e6bf9367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830eb5e-6d27-4983-950e-38f5a7ed8273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#     'num_layers' : 3,\n",
    "#     'emb_size' : 32,\n",
    "#     'weight_decay': 1e-5,\n",
    "#     'lr': 0.01,\n",
    "#     'epochs': 600\n",
    "# }\n",
    "\n",
    "# datasets = {\n",
    "#     'train': dataset_train[0],\n",
    "#     'val': dataset_val[0],\n",
    "#     'test': dataset_test[0]\n",
    "# }\n",
    "            \n",
    "# input_dim = datasets['train'].num_node_features\n",
    "# print(input_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df58390-2c2a-4d2e-8786-ba27f04f4458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = torch.load('../best_lgcn.pkl')\n",
    "\n",
    "def test(model, mode, args):\n",
    "    model.eval()\n",
    "    score = 0\n",
    "    loss_score = 0\n",
    "\n",
    "    data = datasets[mode]\n",
    "    data.to(args[\"device\"])\n",
    "\n",
    "    pred = model(data)\n",
    "    loss = model.loss(pred, data.edge_label.type(pred.dtype))\n",
    "    score += roc_auc_score(data.edge_label.flatten().cpu().numpy(), pred.flatten().data.cpu().numpy())\n",
    "    loss_score += loss.item()\n",
    "\n",
    "    return pred, score, loss_score\n",
    "\n",
    "preds_test, best_test_roc, test_loss = test(best_model, 'test', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31a34e-b338-4594-9c33-50db25a5f604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = datasets['test']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53b43d-ae3b-4cb4-a964-30dc3791d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'preds':pred.to('cpu').detach().numpy(), 'edge_label': ds.edge_label.to('cpu').detach().numpy(), 'src_node': ds.edge_label_index[0].to('cpu').detach().numpy(), 'dest_node': ds.edge_label_index[1].to('cpu').detach().numpy()}\n",
    "df = pd.DataFrame(dict1)\n",
    "\n",
    "df_1 = df[df['edge_label'] == 1]\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0392fd-c328-4e8c-a4d4-0fce74761f48",
   "metadata": {},
   "source": [
    "TODO: new playlist inference:\n",
    "\n",
    "- if new playlist node under consideration is from the existing graph:\n",
    "    - make bipartite graph, where one side is the playlist node and the other side are all track nodes except the ones the playlist under consideration is already connected to\n",
    "    - run model on above graph and get top results.\n",
    "    \n",
    "- if new playlist does not exist in the existing graph (caveat: but some of its track nodes do):\n",
    "    - add new nodes from the new playlist network (minus overlapping track nodes) to the existing, main graph\n",
    "    - randomly initialize embeddings for the new nodes\n",
    "    - figure out how to perform message passing and aggregation only on the new nodes and only update their (initially randomly initialized) embeddings\n",
    "      ^^train over new nodes (freeze existing nodes updation) to get updated, better embeddings of the new nodes.\n",
    "    - once the above is done, proceed with bipartite graph with new playlist node at one end and all other non-connected track nodes at the other.\n",
    "    - run model on above graph and get top results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e6ee6-be6e-4d5b-b55a-9b081376c046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
