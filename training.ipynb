{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089916e0-d3a9-46d6-a1b0-f3d1d46c4728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0')\n",
      "2.3.0\n",
      "graph-tool version: 2.44\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(torch.zeros(1).cuda())\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "print(torch_geometric.__version__)\n",
    "\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType, OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "import networkx as nx\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import deepsnap\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "from pathlib import Path as Data_Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import json\n",
    "print(\"graph-tool version: {}\".format(gt.__version__.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b44dcc-a8f9-42ad-8d4c-bf6bb5459424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/asa489/Downloads/updated_fixed_graph.pickle\", \"rb\") as f:\n",
    "    g_nx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30978f4-29e7-4fb8-91b1-f435007437a2",
   "metadata": {},
   "source": [
    "### N-hop neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c5c22-2819-4ae6-88cc-7b97ae1ed7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b137d90-07e9-4816-be08-3370a8a33e2a",
   "metadata": {},
   "source": [
    "### Deepsnap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4561d2-c5a2-4ef1-adb8-c4db66a671f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asa489/.conda/envs/ayush_project/lib/python3.10/site-packages/deepsnap/graph.py:522: UserWarning: Node related key is required.\n",
      "  warnings.warn(\"Node related key is required.\")\n"
     ]
    }
   ],
   "source": [
    "# Create a DeepSNAP graph from NetworkX graph\n",
    "ds_graph = Graph(g_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6f263f-9347-4ecc-9f15-ddf36cd2f968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 690613\n",
      "Number of edges: 6655333\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices:\", g_nx.number_of_nodes())\n",
    "print(\"Number of edges:\", g_nx.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5773f4-4af4-49ea-ba46-781b4fa3aca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deepsnap.dataset.GraphDataset'>\n",
      "Graph(G=[], edge_index=[2, 8518824], edge_label=[4259416], edge_label_index=[2, 4259416], name=[690613], negative_label_val=[1], node_label_index=[690613], type=[690613], uri=[690613])\n",
      "<class 'deepsnap.graph.Graph'>\n",
      "Train set has 2129708 supervision (positive) edges\n",
      "Validation set has 1331066 supervision (positive) edges\n",
      "Test set has 1331068 supervision (positive) edges\n",
      "Train set has 8518824 message passing edges\n",
      "Validation set has 10648532 message passing edges\n",
      "Test set has 11979598 message passing edges\n",
      "GraphDataset(1)\n"
     ]
    }
   ],
   "source": [
    "task = 'link_pred'\n",
    "dataset = GraphDataset([ds_graph], task=task, edge_train_mode='disjoint')\n",
    "\n",
    "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "# dataset_train[0].to('cuda')\n",
    "# dataset_val[0].to('cuda')\n",
    "# dataset_test[0].to('cuda')\n",
    "\n",
    "# dataset_train.to('cuda:0')\n",
    "# dataset_val.to('cuda:0')\n",
    "# dataset_test.to('cuda:0')\n",
    "\n",
    "print(type(dataset_train))\n",
    "print(dataset_train[0])\n",
    "print(type(dataset_train[0]))\n",
    "\n",
    "num_train_edges = dataset_train[0].edge_label_index.shape[1]\n",
    "num_val_edges = dataset_val[0].edge_label_index.shape[1]\n",
    "num_test_edges = dataset_test[0].edge_label_index.shape[1]\n",
    "\n",
    "print(\"Train set has {} supervision (positive) edges\".format(num_train_edges // 2))\n",
    "print(\"Validation set has {} supervision (positive) edges\".format(num_val_edges // 2))\n",
    "print(\"Test set has {} supervision (positive) edges\".format(num_test_edges // 2))\n",
    "\n",
    "print(\"Train set has {} message passing edges\".format(dataset_train[0].edge_index.shape[1]))\n",
    "print(\"Validation set has {} message passing edges\".format(dataset_val[0].edge_index.shape[1]))\n",
    "print(\"Test set has {} message passing edges\".format(dataset_test[0].edge_index.shape[1]))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fa4e94-d8a1-433b-a88c-961e33870a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(dataset_train, open('./fixed_graphs/train.graph', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8828e39d-23b7-45f8-9d9a-cc3fa7fc8d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(dataset_val, open('./fixed_graphs/val.graph', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a09dcb-a22d-4b97-9200-dcc4fddd6a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(dataset_test, open('./fixed_graphs/test.graph', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124d69ee-b24a-4097-9f11-118f745d3214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = pickle.load(open('/home/asa489/fixed_graphs/train.graph', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddbeb4a-e009-462e-b80f-a1ae1cf9caed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val = pickle.load(open('/home/asa489/fixed_graphs/val.graph', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6b4776-f7c8-4d25-80a0-6bde823c866b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset_test = pickle.load(open('/home/asd27/scratch/fixed_graphs/test.graph', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59dec8f-30e8-4c5b-ab48-3cccfff33387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(LightGCNConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        out = self.propagate(edge_index, x=(x, x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        out = x_j\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        node_dim = self.node_dim\n",
    "        out = torch_scatter.scatter(inputs, index, dim=node_dim, reduce='mean')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ff20e2-0434-4fd8-89a1-e8db325c3e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCN(torch.nn.Module):\n",
    "    def __init__(self, train_data, num_layers, emb_size=16, initialize_with_words=False):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        assert (num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(num_layers):\n",
    "            self.convs.append(LightGCNConv(input_dim, input_dim))\n",
    "\n",
    "        # Initialize using custom embeddings if provided\n",
    "        num_nodes = train_data.node_label_index.size()[0]\n",
    "        self.embeddings = nn.Embedding(num_nodes, emb_size)\n",
    "        if initialize_with_words:\n",
    "            self.embeddings.weight.data.copy_(train_datanode_features)\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_size = emb_size\n",
    "        self.num_modes = num_nodes\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_label_index, node_label_index = data.edge_index, data.edge_label_index, data.node_label_index\n",
    "        layer_embeddings = []\n",
    "        \n",
    "        x = self.embeddings(node_label_index)\n",
    "        mean_layer = x\n",
    "\n",
    "        # We take an average of ever layer's node embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            # print(\"x shape\",x.shape)\n",
    "            # print(\"mean_layer shape\",mean_layer.shape)\n",
    "            mean_layer += x\n",
    "\n",
    "        mean_layer /= 4\n",
    "\n",
    "        # Prediction head is simply dot product\n",
    "        nodes_first = torch.index_select(x, 0, edge_label_index[0,:].long())\n",
    "        nodes_second = torch.index_select(x, 0, edge_label_index[1,:].long())\n",
    "\n",
    "        # Since we don't want a rank output, we create a sigmoid of the dot product\n",
    "        out = torch.sum(nodes_first * nodes_second, dim=-1) # FOR RANKING\n",
    "        pred = torch.sigmoid(out)\n",
    "\n",
    "        return torch.flatten(pred)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return self.loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fffe68-e6e4-4892-9b18-13be04810acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8633e9c3-115c-4ed1-a559-c6d2d9267b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_layers' : 3,\n",
    "    'emb_size' : 32,\n",
    "    'weight_decay': 1e-6,\n",
    "    'lr': 0.4,\n",
    "    'epochs': 600\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebe5156-60c7-4699-93f5-03466c7433d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'device': 'cuda', 'num_layers': 3, 'emb_size': 32, 'weight_decay': 1e-06, 'lr': 0.4, 'epochs': 600}\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "datasets = {\n",
    "    'train': dataset_train[0],\n",
    "    'val': dataset_val[0],\n",
    "#     'test': dataset_test[0]\n",
    "}     \n",
    "input_dim = datasets['train'].num_node_features\n",
    "print(input_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3dacca33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      1,      2,  ..., 690610, 690611, 690612], device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].node_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58552313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      1,      2,  ..., 690610, 690611, 690612], device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['val'].node_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65aa42-8be4-450f-bdec-aa2206feb866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets['train'].to(args['device'])\n",
    "# datasets['val'].to(args['device'])\n",
    "# datasets['test'].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941df0fc-fc53-4f9d-b457-b2c562175a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, optimizer, args):\n",
    "    val_max = 0\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        datasets['train'].to(args[\"device\"])\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(datasets['train'])\n",
    "        loss = model.loss(pred, datasets['train'].edge_label.type(pred.dtype))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Loss: {:.5f}, Val Loss: {:.5f}'\n",
    "        score_train, train_loss = test(model, 'train', args)\n",
    "        score_val, val_loss = test(model, 'val', args)\n",
    "#         score_test, test_loss = test(model, 'test', args)\n",
    "\n",
    "        losses.append((train_loss, val_loss))\n",
    "\n",
    "        print(log.format(epoch, score_train, score_val, train_loss, val_loss))\n",
    "        if val_max < score_val:\n",
    "            val_max = score_val\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def test(model, mode, args):\n",
    "    model.eval()\n",
    "    score = 0\n",
    "    loss_score = 0\n",
    "\n",
    "    data = datasets[mode]\n",
    "    data.to(args[\"device\"])\n",
    "\n",
    "    pred = model(data)\n",
    "    loss = model.loss(pred, data.edge_label.type(pred.dtype))\n",
    "    score += roc_auc_score(data.edge_label.flatten().cpu().numpy(), pred.flatten().data.cpu().numpy())\n",
    "    loss_score += loss.item()\n",
    "\n",
    "    return score, loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a086b9-3cef-4997-b090-d44296f688d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed78571-2201-450e-a3aa-6bd29ba4529e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.6830, Val: 0.6182, Loss: 0.69299, Val Loss: 0.69640\n",
      "Epoch: 002, Train: 0.8120, Val: 0.7742, Loss: 0.67594, Val Loss: 0.67927\n",
      "Epoch: 003, Train: 0.8745, Val: 0.8598, Loss: 0.60988, Val Loss: 0.61604\n",
      "Epoch: 004, Train: 0.8808, Val: 0.8701, Loss: 0.55814, Val Loss: 0.57589\n",
      "Epoch: 005, Train: 0.8784, Val: 0.8680, Loss: 0.54047, Val Loss: 0.55366\n",
      "Epoch: 006, Train: 0.8749, Val: 0.8647, Loss: 0.53826, Val Loss: 0.54585\n",
      "Epoch: 007, Train: 0.8868, Val: 0.8807, Loss: 0.53550, Val Loss: 0.54033\n",
      "Epoch: 008, Train: 0.8943, Val: 0.8883, Loss: 0.53690, Val Loss: 0.54510\n",
      "Epoch: 009, Train: 0.8967, Val: 0.8899, Loss: 0.54031, Val Loss: 0.55114\n",
      "Epoch: 010, Train: 0.9000, Val: 0.8942, Loss: 0.53715, Val Loss: 0.54548\n",
      "Epoch: 011, Train: 0.8973, Val: 0.8913, Loss: 0.53634, Val Loss: 0.54254\n",
      "Epoch: 012, Train: 0.8924, Val: 0.8849, Loss: 0.53537, Val Loss: 0.54202\n",
      "Epoch: 013, Train: 0.8902, Val: 0.8819, Loss: 0.53301, Val Loss: 0.54155\n",
      "Epoch: 014, Train: 0.8897, Val: 0.8812, Loss: 0.53336, Val Loss: 0.54362\n",
      "Epoch: 015, Train: 0.8901, Val: 0.8815, Loss: 0.53415, Val Loss: 0.54386\n",
      "Epoch: 016, Train: 0.8914, Val: 0.8830, Loss: 0.53650, Val Loss: 0.54418\n",
      "Epoch: 017, Train: 0.8938, Val: 0.8860, Loss: 0.53984, Val Loss: 0.54634\n",
      "Epoch: 018, Train: 0.8958, Val: 0.8888, Loss: 0.54008, Val Loss: 0.54697\n",
      "Epoch: 019, Train: 0.8965, Val: 0.8899, Loss: 0.53839, Val Loss: 0.54648\n",
      "Epoch: 020, Train: 0.8962, Val: 0.8899, Loss: 0.53611, Val Loss: 0.54446\n",
      "Epoch: 021, Train: 0.8948, Val: 0.8887, Loss: 0.53398, Val Loss: 0.54130\n",
      "Epoch: 022, Train: 0.8923, Val: 0.8857, Loss: 0.53351, Val Loss: 0.54000\n",
      "Epoch: 023, Train: 0.8907, Val: 0.8835, Loss: 0.53310, Val Loss: 0.53969\n",
      "Epoch: 024, Train: 0.8913, Val: 0.8840, Loss: 0.53205, Val Loss: 0.53939\n",
      "Epoch: 025, Train: 0.8927, Val: 0.8855, Loss: 0.53225, Val Loss: 0.54033\n",
      "Epoch: 026, Train: 0.8938, Val: 0.8866, Loss: 0.53399, Val Loss: 0.54199\n",
      "Epoch: 027, Train: 0.8946, Val: 0.8872, Loss: 0.53667, Val Loss: 0.54400\n",
      "Epoch: 028, Train: 0.8952, Val: 0.8878, Loss: 0.53846, Val Loss: 0.54539\n",
      "Epoch: 029, Train: 0.8956, Val: 0.8884, Loss: 0.53751, Val Loss: 0.54473\n",
      "Epoch: 030, Train: 0.8956, Val: 0.8885, Loss: 0.53507, Val Loss: 0.54300\n",
      "Epoch: 031, Train: 0.8951, Val: 0.8880, Loss: 0.53304, Val Loss: 0.54123\n",
      "Epoch: 032, Train: 0.8941, Val: 0.8867, Loss: 0.53218, Val Loss: 0.54001\n",
      "Epoch: 033, Train: 0.8931, Val: 0.8855, Loss: 0.53245, Val Loss: 0.53992\n",
      "Epoch: 034, Train: 0.8933, Val: 0.8858, Loss: 0.53255, Val Loss: 0.54004\n",
      "Epoch: 035, Train: 0.8944, Val: 0.8871, Loss: 0.53244, Val Loss: 0.54037\n",
      "Epoch: 036, Train: 0.8953, Val: 0.8882, Loss: 0.53306, Val Loss: 0.54135\n",
      "Epoch: 037, Train: 0.8956, Val: 0.8885, Loss: 0.53417, Val Loss: 0.54227\n",
      "Epoch: 038, Train: 0.8955, Val: 0.8882, Loss: 0.53505, Val Loss: 0.54275\n",
      "Epoch: 039, Train: 0.8951, Val: 0.8877, Loss: 0.53472, Val Loss: 0.54240\n",
      "Epoch: 040, Train: 0.8949, Val: 0.8874, Loss: 0.53328, Val Loss: 0.54137\n",
      "Epoch: 041, Train: 0.8947, Val: 0.8871, Loss: 0.53211, Val Loss: 0.54058\n",
      "Epoch: 042, Train: 0.8944, Val: 0.8867, Loss: 0.53195, Val Loss: 0.54035\n",
      "Epoch: 043, Train: 0.8942, Val: 0.8864, Loss: 0.53264, Val Loss: 0.54070\n",
      "Epoch: 044, Train: 0.8945, Val: 0.8868, Loss: 0.53328, Val Loss: 0.54122\n",
      "Epoch: 045, Train: 0.8952, Val: 0.8876, Loss: 0.53338, Val Loss: 0.54157\n",
      "Epoch: 046, Train: 0.8956, Val: 0.8881, Loss: 0.53339, Val Loss: 0.54184\n",
      "Epoch: 047, Train: 0.8955, Val: 0.8880, Loss: 0.53353, Val Loss: 0.54187\n",
      "Epoch: 048, Train: 0.8951, Val: 0.8875, Loss: 0.53364, Val Loss: 0.54172\n",
      "Epoch: 049, Train: 0.8947, Val: 0.8870, Loss: 0.53327, Val Loss: 0.54134\n",
      "Epoch: 050, Train: 0.8947, Val: 0.8869, Loss: 0.53254, Val Loss: 0.54086\n",
      "Epoch: 051, Train: 0.8947, Val: 0.8870, Loss: 0.53220, Val Loss: 0.54071\n",
      "Epoch: 052, Train: 0.8948, Val: 0.8871, Loss: 0.53258, Val Loss: 0.54094\n",
      "Epoch: 053, Train: 0.8949, Val: 0.8872, Loss: 0.53327, Val Loss: 0.54140\n",
      "Epoch: 054, Train: 0.8952, Val: 0.8875, Loss: 0.53353, Val Loss: 0.54167\n",
      "Epoch: 055, Train: 0.8954, Val: 0.8878, Loss: 0.53328, Val Loss: 0.54161\n",
      "Epoch: 056, Train: 0.8954, Val: 0.8877, Loss: 0.53299, Val Loss: 0.54141\n",
      "Epoch: 057, Train: 0.8951, Val: 0.8874, Loss: 0.53292, Val Loss: 0.54122\n",
      "Epoch: 058, Train: 0.8948, Val: 0.8870, Loss: 0.53291, Val Loss: 0.54110\n",
      "Epoch: 059, Train: 0.8948, Val: 0.8870, Loss: 0.53270, Val Loss: 0.54098\n",
      "Epoch: 060, Train: 0.8950, Val: 0.8872, Loss: 0.53258, Val Loss: 0.54100\n",
      "Epoch: 061, Train: 0.8951, Val: 0.8874, Loss: 0.53282, Val Loss: 0.54122\n",
      "Epoch: 062, Train: 0.8951, Val: 0.8874, Loss: 0.53324, Val Loss: 0.54150\n",
      "Epoch: 063, Train: 0.8952, Val: 0.8874, Loss: 0.53336, Val Loss: 0.54158\n",
      "Epoch: 064, Train: 0.8952, Val: 0.8875, Loss: 0.53306, Val Loss: 0.54141\n",
      "Epoch: 065, Train: 0.8951, Val: 0.8874, Loss: 0.53276, Val Loss: 0.54119\n",
      "Epoch: 066, Train: 0.8950, Val: 0.8872, Loss: 0.53273, Val Loss: 0.54108\n",
      "Epoch: 067, Train: 0.8949, Val: 0.8870, Loss: 0.53282, Val Loss: 0.54110\n",
      "Epoch: 068, Train: 0.8950, Val: 0.8872, Loss: 0.53281, Val Loss: 0.54114\n",
      "Epoch: 069, Train: 0.8951, Val: 0.8874, Loss: 0.53281, Val Loss: 0.54122\n",
      "Epoch: 070, Train: 0.8952, Val: 0.8875, Loss: 0.53296, Val Loss: 0.54134\n",
      "Epoch: 071, Train: 0.8952, Val: 0.8874, Loss: 0.53313, Val Loss: 0.54143\n",
      "Epoch: 072, Train: 0.8951, Val: 0.8874, Loss: 0.53306, Val Loss: 0.54136\n",
      "Epoch: 073, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54119\n",
      "Epoch: 074, Train: 0.8950, Val: 0.8873, Loss: 0.53267, Val Loss: 0.54110\n",
      "Epoch: 075, Train: 0.8950, Val: 0.8872, Loss: 0.53277, Val Loss: 0.54114\n",
      "Epoch: 076, Train: 0.8950, Val: 0.8872, Loss: 0.53290, Val Loss: 0.54123\n",
      "Epoch: 077, Train: 0.8951, Val: 0.8873, Loss: 0.53291, Val Loss: 0.54129\n",
      "Epoch: 078, Train: 0.8952, Val: 0.8874, Loss: 0.53290, Val Loss: 0.54132\n",
      "Epoch: 079, Train: 0.8952, Val: 0.8874, Loss: 0.53295, Val Loss: 0.54133\n",
      "Epoch: 080, Train: 0.8951, Val: 0.8873, Loss: 0.53295, Val Loss: 0.54129\n",
      "Epoch: 081, Train: 0.8951, Val: 0.8873, Loss: 0.53283, Val Loss: 0.54120\n",
      "Epoch: 082, Train: 0.8951, Val: 0.8873, Loss: 0.53272, Val Loss: 0.54115\n",
      "Epoch: 083, Train: 0.8951, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54118\n",
      "Epoch: 084, Train: 0.8951, Val: 0.8873, Loss: 0.53290, Val Loss: 0.54126\n",
      "Epoch: 085, Train: 0.8951, Val: 0.8873, Loss: 0.53293, Val Loss: 0.54130\n",
      "Epoch: 086, Train: 0.8952, Val: 0.8874, Loss: 0.53288, Val Loss: 0.54129\n",
      "Epoch: 087, Train: 0.8952, Val: 0.8874, Loss: 0.53286, Val Loss: 0.54126\n",
      "Epoch: 088, Train: 0.8951, Val: 0.8873, Loss: 0.53286, Val Loss: 0.54124\n",
      "Epoch: 089, Train: 0.8951, Val: 0.8872, Loss: 0.53281, Val Loss: 0.54120\n",
      "Epoch: 090, Train: 0.8951, Val: 0.8873, Loss: 0.53276, Val Loss: 0.54119\n",
      "Epoch: 091, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54122\n",
      "Epoch: 092, Train: 0.8951, Val: 0.8873, Loss: 0.53289, Val Loss: 0.54128\n",
      "Epoch: 093, Train: 0.8952, Val: 0.8873, Loss: 0.53290, Val Loss: 0.54129\n",
      "Epoch: 094, Train: 0.8952, Val: 0.8873, Loss: 0.53284, Val Loss: 0.54126\n",
      "Epoch: 095, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54123\n",
      "Epoch: 096, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54122\n",
      "Epoch: 097, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54122\n",
      "Epoch: 098, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54122\n",
      "Epoch: 099, Train: 0.8951, Val: 0.8873, Loss: 0.53283, Val Loss: 0.54125\n",
      "Epoch: 100, Train: 0.8951, Val: 0.8873, Loss: 0.53287, Val Loss: 0.54128\n",
      "Epoch: 101, Train: 0.8951, Val: 0.8873, Loss: 0.53285, Val Loss: 0.54126\n",
      "Epoch: 102, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54123\n",
      "Epoch: 103, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54122\n",
      "Epoch: 104, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54122\n",
      "Epoch: 105, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54124\n",
      "Epoch: 106, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54125\n",
      "Epoch: 107, Train: 0.8951, Val: 0.8873, Loss: 0.53284, Val Loss: 0.54126\n",
      "Epoch: 108, Train: 0.8951, Val: 0.8873, Loss: 0.53284, Val Loss: 0.54126\n",
      "Epoch: 109, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 110, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54122\n",
      "Epoch: 111, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54123\n",
      "Epoch: 112, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54124\n",
      "Epoch: 113, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Train: 0.8952, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54125\n",
      "Epoch: 115, Train: 0.8951, Val: 0.8873, Loss: 0.53283, Val Loss: 0.54125\n",
      "Epoch: 116, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 117, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54122\n",
      "Epoch: 118, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54123\n",
      "Epoch: 119, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54124\n",
      "Epoch: 120, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54125\n",
      "Epoch: 121, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54125\n",
      "Epoch: 122, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 123, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 124, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54123\n",
      "Epoch: 125, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54123\n",
      "Epoch: 126, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 127, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54125\n",
      "Epoch: 128, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 129, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 130, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 131, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54123\n",
      "Epoch: 132, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 133, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 134, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54125\n",
      "Epoch: 135, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 136, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 137, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 138, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 139, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 140, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 141, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 142, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 143, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 144, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 145, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 146, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 147, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 148, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 149, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 150, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 151, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 152, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 153, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 154, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 155, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 156, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 157, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 158, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 159, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 160, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 161, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 162, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 163, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 164, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 165, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 166, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 167, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 168, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 169, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 170, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 171, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 172, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 173, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 174, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 175, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 176, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 177, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 178, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 179, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 180, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 181, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 182, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 183, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 184, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 185, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 186, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 187, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 188, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 189, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 190, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 191, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 192, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 193, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 194, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 195, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 196, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 197, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 198, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 199, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 200, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 201, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 202, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 203, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 204, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 205, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 206, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 207, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 208, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 209, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 210, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 211, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 212, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 213, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 214, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 215, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 216, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 217, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 218, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 219, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 220, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 221, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 222, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 223, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 224, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 225, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 226, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 228, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 229, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 230, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 231, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 232, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 233, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 234, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 235, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 236, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 237, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 238, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 239, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 240, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 241, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 242, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 243, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 244, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 245, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 246, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 247, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 248, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 249, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 250, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 251, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 252, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 253, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 254, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 255, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 256, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 257, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 258, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 259, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 260, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 261, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 262, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 263, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 264, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 265, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 266, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 267, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 268, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 269, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 270, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 271, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 272, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 273, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 274, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 275, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 276, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 277, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 278, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 279, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 280, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 281, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 282, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 283, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 284, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 285, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 286, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 287, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 288, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 289, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 290, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 291, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 292, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 293, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 294, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 295, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 296, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 297, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 298, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 299, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 300, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 301, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 302, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 303, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 304, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 305, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 306, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 307, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 308, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 309, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 310, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 311, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 312, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 313, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 314, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 315, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 316, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 317, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 318, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 319, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 320, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 321, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 322, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 323, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 324, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 325, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 326, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 327, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 328, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 329, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 330, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 331, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 332, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 333, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 334, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 335, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 336, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 337, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 338, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 339, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 341, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 342, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 343, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 344, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 345, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 346, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 347, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 348, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 349, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 350, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 351, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 352, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 353, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 354, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 355, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 356, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 357, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 358, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 359, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 360, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 361, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 362, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 363, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 364, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 365, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 366, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 367, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 368, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 369, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 370, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 371, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 372, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 373, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 374, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 375, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 376, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 377, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 378, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 379, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 380, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 381, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 382, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 383, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 384, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 385, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 386, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 387, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 388, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 389, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 390, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 391, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 392, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 393, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 394, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 395, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 396, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 397, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 398, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 399, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 400, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 401, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 402, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 403, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 404, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 405, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 406, Train: 0.8952, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 407, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54123\n",
      "Epoch: 408, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 409, Train: 0.8952, Val: 0.8873, Loss: 0.53276, Val Loss: 0.54123\n",
      "Epoch: 410, Train: 0.8951, Val: 0.8873, Loss: 0.53283, Val Loss: 0.54125\n",
      "Epoch: 411, Train: 0.8952, Val: 0.8873, Loss: 0.53273, Val Loss: 0.54122\n",
      "Epoch: 412, Train: 0.8951, Val: 0.8872, Loss: 0.53288, Val Loss: 0.54127\n",
      "Epoch: 413, Train: 0.8952, Val: 0.8874, Loss: 0.53265, Val Loss: 0.54119\n",
      "Epoch: 414, Train: 0.8950, Val: 0.8871, Loss: 0.53302, Val Loss: 0.54133\n",
      "Epoch: 415, Train: 0.8953, Val: 0.8875, Loss: 0.53247, Val Loss: 0.54114\n",
      "Epoch: 416, Train: 0.8949, Val: 0.8869, Loss: 0.53339, Val Loss: 0.54152\n",
      "Epoch: 417, Train: 0.8955, Val: 0.8878, Loss: 0.53210, Val Loss: 0.54110\n",
      "Epoch: 418, Train: 0.8944, Val: 0.8862, Loss: 0.53455, Val Loss: 0.54223\n",
      "Epoch: 419, Train: 0.8958, Val: 0.8883, Loss: 0.53168, Val Loss: 0.54157\n",
      "Epoch: 420, Train: 0.8926, Val: 0.8836, Loss: 0.53860, Val Loss: 0.54539\n",
      "Epoch: 421, Train: 0.8960, Val: 0.8886, Loss: 0.53309, Val Loss: 0.54522\n",
      "Epoch: 422, Train: 0.8866, Val: 0.8758, Loss: 0.54956, Val Loss: 0.55543\n",
      "Epoch: 423, Train: 0.8960, Val: 0.8886, Loss: 0.53614, Val Loss: 0.55002\n",
      "Epoch: 424, Train: 0.8892, Val: 0.8793, Loss: 0.54471, Val Loss: 0.55077\n",
      "Epoch: 425, Train: 0.8954, Val: 0.8878, Loss: 0.53215, Val Loss: 0.54167\n",
      "Epoch: 426, Train: 0.8955, Val: 0.8879, Loss: 0.53185, Val Loss: 0.54210\n",
      "Epoch: 427, Train: 0.8896, Val: 0.8797, Loss: 0.54305, Val Loss: 0.54935\n",
      "Epoch: 428, Train: 0.8961, Val: 0.8887, Loss: 0.53330, Val Loss: 0.54495\n",
      "Epoch: 429, Train: 0.8942, Val: 0.8859, Loss: 0.53537, Val Loss: 0.54316\n",
      "Epoch: 430, Train: 0.8941, Val: 0.8858, Loss: 0.53475, Val Loss: 0.54234\n",
      "Epoch: 431, Train: 0.8957, Val: 0.8881, Loss: 0.53207, Val Loss: 0.54313\n",
      "Epoch: 432, Train: 0.8917, Val: 0.8824, Loss: 0.53861, Val Loss: 0.54570\n",
      "Epoch: 433, Train: 0.8958, Val: 0.8881, Loss: 0.53260, Val Loss: 0.54183\n",
      "Epoch: 434, Train: 0.8957, Val: 0.8881, Loss: 0.53266, Val Loss: 0.54237\n",
      "Epoch: 435, Train: 0.8922, Val: 0.8830, Loss: 0.53799, Val Loss: 0.54509\n",
      "Epoch: 436, Train: 0.8956, Val: 0.8880, Loss: 0.53158, Val Loss: 0.54165\n",
      "Epoch: 437, Train: 0.8951, Val: 0.8872, Loss: 0.53280, Val Loss: 0.54149\n",
      "Epoch: 438, Train: 0.8937, Val: 0.8852, Loss: 0.53620, Val Loss: 0.54351\n",
      "Epoch: 439, Train: 0.8960, Val: 0.8885, Loss: 0.53196, Val Loss: 0.54206\n",
      "Epoch: 440, Train: 0.8947, Val: 0.8866, Loss: 0.53350, Val Loss: 0.54162\n",
      "Epoch: 441, Train: 0.8941, Val: 0.8858, Loss: 0.53427, Val Loss: 0.54208\n",
      "Epoch: 442, Train: 0.8958, Val: 0.8882, Loss: 0.53173, Val Loss: 0.54162\n",
      "Epoch: 443, Train: 0.8945, Val: 0.8864, Loss: 0.53436, Val Loss: 0.54217\n",
      "Epoch: 444, Train: 0.8949, Val: 0.8870, Loss: 0.53361, Val Loss: 0.54174\n",
      "Epoch: 445, Train: 0.8957, Val: 0.8882, Loss: 0.53182, Val Loss: 0.54137\n",
      "Epoch: 446, Train: 0.8941, Val: 0.8857, Loss: 0.53436, Val Loss: 0.54213\n",
      "Epoch: 447, Train: 0.8951, Val: 0.8872, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 448, Train: 0.8957, Val: 0.8880, Loss: 0.53220, Val Loss: 0.54145\n",
      "Epoch: 449, Train: 0.8943, Val: 0.8861, Loss: 0.53465, Val Loss: 0.54231\n",
      "Epoch: 450, Train: 0.8953, Val: 0.8875, Loss: 0.53248, Val Loss: 0.54119\n",
      "Epoch: 451, Train: 0.8954, Val: 0.8877, Loss: 0.53211, Val Loss: 0.54110\n",
      "Epoch: 452, Train: 0.8943, Val: 0.8860, Loss: 0.53434, Val Loss: 0.54207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 453, Train: 0.8955, Val: 0.8877, Loss: 0.53240, Val Loss: 0.54124\n",
      "Epoch: 454, Train: 0.8954, Val: 0.8877, Loss: 0.53248, Val Loss: 0.54124\n",
      "Epoch: 455, Train: 0.8944, Val: 0.8863, Loss: 0.53410, Val Loss: 0.54193\n",
      "Epoch: 456, Train: 0.8954, Val: 0.8877, Loss: 0.53218, Val Loss: 0.54108\n",
      "Epoch: 457, Train: 0.8952, Val: 0.8874, Loss: 0.53257, Val Loss: 0.54118\n",
      "Epoch: 458, Train: 0.8947, Val: 0.8866, Loss: 0.53381, Val Loss: 0.54175\n",
      "Epoch: 459, Train: 0.8955, Val: 0.8878, Loss: 0.53233, Val Loss: 0.54124\n",
      "Epoch: 460, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54125\n",
      "Epoch: 461, Train: 0.8947, Val: 0.8867, Loss: 0.53340, Val Loss: 0.54151\n",
      "Epoch: 462, Train: 0.8955, Val: 0.8877, Loss: 0.53223, Val Loss: 0.54112\n",
      "Epoch: 463, Train: 0.8951, Val: 0.8872, Loss: 0.53303, Val Loss: 0.54136\n",
      "Epoch: 464, Train: 0.8950, Val: 0.8870, Loss: 0.53325, Val Loss: 0.54146\n",
      "Epoch: 465, Train: 0.8955, Val: 0.8877, Loss: 0.53229, Val Loss: 0.54114\n",
      "Epoch: 466, Train: 0.8949, Val: 0.8870, Loss: 0.53309, Val Loss: 0.54135\n",
      "Epoch: 467, Train: 0.8950, Val: 0.8871, Loss: 0.53296, Val Loss: 0.54129\n",
      "Epoch: 468, Train: 0.8954, Val: 0.8876, Loss: 0.53239, Val Loss: 0.54117\n",
      "Epoch: 469, Train: 0.8950, Val: 0.8870, Loss: 0.53323, Val Loss: 0.54144\n",
      "Epoch: 470, Train: 0.8951, Val: 0.8873, Loss: 0.53283, Val Loss: 0.54126\n",
      "Epoch: 471, Train: 0.8953, Val: 0.8875, Loss: 0.53243, Val Loss: 0.54112\n",
      "Epoch: 472, Train: 0.8949, Val: 0.8869, Loss: 0.53318, Val Loss: 0.54139\n",
      "Epoch: 473, Train: 0.8952, Val: 0.8874, Loss: 0.53272, Val Loss: 0.54123\n",
      "Epoch: 474, Train: 0.8953, Val: 0.8875, Loss: 0.53261, Val Loss: 0.54120\n",
      "Epoch: 475, Train: 0.8949, Val: 0.8870, Loss: 0.53318, Val Loss: 0.54140\n",
      "Epoch: 476, Train: 0.8952, Val: 0.8874, Loss: 0.53259, Val Loss: 0.54116\n",
      "Epoch: 477, Train: 0.8952, Val: 0.8874, Loss: 0.53266, Val Loss: 0.54119\n",
      "Epoch: 478, Train: 0.8950, Val: 0.8871, Loss: 0.53312, Val Loss: 0.54138\n",
      "Epoch: 479, Train: 0.8953, Val: 0.8875, Loss: 0.53260, Val Loss: 0.54120\n",
      "Epoch: 480, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 481, Train: 0.8950, Val: 0.8871, Loss: 0.53299, Val Loss: 0.54131\n",
      "Epoch: 482, Train: 0.8953, Val: 0.8874, Loss: 0.53256, Val Loss: 0.54116\n",
      "Epoch: 483, Train: 0.8951, Val: 0.8872, Loss: 0.53288, Val Loss: 0.54128\n",
      "Epoch: 484, Train: 0.8951, Val: 0.8872, Loss: 0.53293, Val Loss: 0.54130\n",
      "Epoch: 485, Train: 0.8953, Val: 0.8874, Loss: 0.53260, Val Loss: 0.54118\n",
      "Epoch: 486, Train: 0.8951, Val: 0.8872, Loss: 0.53291, Val Loss: 0.54128\n",
      "Epoch: 487, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 488, Train: 0.8952, Val: 0.8874, Loss: 0.53265, Val Loss: 0.54120\n",
      "Epoch: 489, Train: 0.8951, Val: 0.8872, Loss: 0.53297, Val Loss: 0.54131\n",
      "Epoch: 490, Train: 0.8952, Val: 0.8873, Loss: 0.53275, Val Loss: 0.54123\n",
      "Epoch: 491, Train: 0.8952, Val: 0.8874, Loss: 0.53269, Val Loss: 0.54120\n",
      "Epoch: 492, Train: 0.8951, Val: 0.8872, Loss: 0.53293, Val Loss: 0.54129\n",
      "Epoch: 493, Train: 0.8952, Val: 0.8874, Loss: 0.53271, Val Loss: 0.54121\n",
      "Epoch: 494, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54124\n",
      "Epoch: 495, Train: 0.8951, Val: 0.8872, Loss: 0.53290, Val Loss: 0.54128\n",
      "Epoch: 496, Train: 0.8952, Val: 0.8874, Loss: 0.53268, Val Loss: 0.54120\n",
      "Epoch: 497, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 498, Train: 0.8951, Val: 0.8872, Loss: 0.53285, Val Loss: 0.54126\n",
      "Epoch: 499, Train: 0.8952, Val: 0.8874, Loss: 0.53270, Val Loss: 0.54121\n",
      "Epoch: 500, Train: 0.8951, Val: 0.8872, Loss: 0.53286, Val Loss: 0.54126\n",
      "Epoch: 501, Train: 0.8951, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 502, Train: 0.8952, Val: 0.8873, Loss: 0.53271, Val Loss: 0.54121\n",
      "Epoch: 503, Train: 0.8951, Val: 0.8872, Loss: 0.53287, Val Loss: 0.54127\n",
      "Epoch: 504, Train: 0.8952, Val: 0.8873, Loss: 0.53276, Val Loss: 0.54123\n",
      "Epoch: 505, Train: 0.8952, Val: 0.8873, Loss: 0.53275, Val Loss: 0.54123\n",
      "Epoch: 506, Train: 0.8951, Val: 0.8872, Loss: 0.53286, Val Loss: 0.54126\n",
      "Epoch: 507, Train: 0.8952, Val: 0.8873, Loss: 0.53273, Val Loss: 0.54122\n",
      "Epoch: 508, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 509, Train: 0.8951, Val: 0.8873, Loss: 0.53284, Val Loss: 0.54126\n",
      "Epoch: 510, Train: 0.8952, Val: 0.8873, Loss: 0.53273, Val Loss: 0.54122\n",
      "Epoch: 511, Train: 0.8951, Val: 0.8873, Loss: 0.53282, Val Loss: 0.54125\n",
      "Epoch: 512, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 513, Train: 0.8952, Val: 0.8873, Loss: 0.53275, Val Loss: 0.54123\n",
      "Epoch: 514, Train: 0.8951, Val: 0.8873, Loss: 0.53284, Val Loss: 0.54126\n",
      "Epoch: 515, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 516, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 517, Train: 0.8951, Val: 0.8873, Loss: 0.53283, Val Loss: 0.54125\n",
      "Epoch: 518, Train: 0.8952, Val: 0.8873, Loss: 0.53276, Val Loss: 0.54123\n",
      "Epoch: 519, Train: 0.8952, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 520, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 521, Train: 0.8952, Val: 0.8873, Loss: 0.53275, Val Loss: 0.54122\n",
      "Epoch: 522, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54125\n",
      "Epoch: 523, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 524, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 525, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54125\n",
      "Epoch: 526, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 527, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 528, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54125\n",
      "Epoch: 529, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 530, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 531, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 532, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 533, Train: 0.8951, Val: 0.8873, Loss: 0.53281, Val Loss: 0.54124\n",
      "Epoch: 534, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54123\n",
      "Epoch: 535, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 536, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 537, Train: 0.8952, Val: 0.8873, Loss: 0.53277, Val Loss: 0.54123\n",
      "Epoch: 538, Train: 0.8952, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 539, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 540, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54123\n",
      "Epoch: 541, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 542, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 543, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 544, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 545, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54123\n",
      "Epoch: 546, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 547, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 548, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 549, Train: 0.8951, Val: 0.8873, Loss: 0.53280, Val Loss: 0.54124\n",
      "Epoch: 550, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 551, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 552, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 553, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 554, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 555, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 556, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 557, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 558, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 559, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 560, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 561, Train: 0.8952, Val: 0.8873, Loss: 0.53278, Val Loss: 0.54124\n",
      "Epoch: 562, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 563, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 564, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 565, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 566, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 567, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 568, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 569, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 570, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 571, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 572, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 573, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 574, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 575, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 576, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 577, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 578, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 579, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 580, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 581, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 582, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 583, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 584, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 585, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 586, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 587, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 588, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 589, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 590, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 591, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 592, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 593, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 594, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 595, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 596, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 597, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 598, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 599, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n",
      "Epoch: 600, Train: 0.8952, Val: 0.8873, Loss: 0.53279, Val Loss: 0.54124\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m best_train_roc, train_loss \u001b[38;5;241m=\u001b[39m test(best_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, args)\n\u001b[1;32m      7\u001b[0m best_val_roc, val_loss \u001b[38;5;241m=\u001b[39m test(best_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, args)\n\u001b[0;32m----> 8\u001b[0m best_test_roc, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(log\u001b[38;5;241m.\u001b[39mformat(best_train_roc, best_val_roc, best_test_roc, train_loss, val_loss, test_loss))\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, mode, args)\u001b[0m\n\u001b[1;32m     33\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m loss_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 36\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m data\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     39\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(data)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "model = LightGCN(datasets['train'], args['num_layers'], emb_size=args['emb_size']).to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "best_model = train(model, optimizer, args)\n",
    "log = \"Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Train Loss: {:.5f}, Val Loss: {:.5f}, Test Loss: {:.5f}\"\n",
    "best_train_roc, train_loss = test(best_model, 'train', args)\n",
    "best_val_roc, val_loss = test(best_model, 'val', args)\n",
    "best_test_roc, test_loss = test(best_model, 'test', args)\n",
    "print(log.format(best_train_roc, best_val_roc, best_test_roc, train_loss, val_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "639674ee-a6bb-4de5-8d98-6dc8ea3222ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File model11.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#model_save\u001b[39;00m\n\u001b[1;32m      2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_lgcn1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel11.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File model11.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "#model_save\n",
    "filename = 'best_lgcn1.pkl'\n",
    "torch.save(model.state_dict(), 'model11.pth')\n",
    "#datasets save\n",
    "# with open('datasets_deepsnap_dict.pkl', 'wb') as f:\n",
    "#     # pickle the dictionary and write it to the file\n",
    "#     pickle.dump(datasets, f)\n",
    "\n",
    "# model = torch.load('../lgcn.pkl')\n",
    "# with open('../datasets_deepsnap_dict.pkl', 'rb') as f:\n",
    "#     # load the pickled dictionary from the file\n",
    "#     datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69ef0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=model.state_dict()\n",
    "torch.save(dic, 'pm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dea244fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embeddings.weight',\n",
       "              tensor([[-1.5305e-02,  1.0913e-02,  6.7270e-02,  ...,  1.6636e-02,\n",
       "                       -1.0406e-01, -1.8328e-04],\n",
       "                      [-4.7601e-03,  3.7323e-03,  2.2234e-02,  ...,  6.0261e-03,\n",
       "                       -3.4850e-02, -1.6471e-03],\n",
       "                      [ 1.0785e-04, -6.6176e-04,  1.2223e-03,  ...,  3.9171e-04,\n",
       "                       -8.1193e-04, -2.1851e-03],\n",
       "                      ...,\n",
       "                      [ 5.1203e-15,  2.0817e-14, -3.2795e-15,  ..., -5.5993e-15,\n",
       "                        5.1512e-15, -5.3834e-15],\n",
       "                      [ 1.4106e-06, -7.8756e-05,  4.3057e-04,  ...,  1.6524e-04,\n",
       "                       -5.7623e-04,  4.2500e-05],\n",
       "                      [ 1.4185e-06, -7.8778e-05,  4.3050e-04,  ...,  1.6529e-04,\n",
       "                       -5.7598e-04,  4.2497e-05]], device='cuda:0'))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06e0ad-076a-4fb0-aa5f-1104ca357558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(losses, title):\n",
    "    train_loss, val_loss = zip(*losses)\n",
    "    steps = list(range(1, len(train_loss) + 1))\n",
    "    \n",
    "    min_val_loss = np.round(np.min(val_loss), 3)\n",
    "    # train_list = [math.log10(x) for x in train_loss]\n",
    "    # val_list = [math.log10(x) for x in val_loss]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(steps, train_loss, '-r', label='Training Loss')\n",
    "    plt.plot(steps, val_loss, '-b', label='Validation Loss')\n",
    "    plt.hlines(min_val_loss, 1, 300, colors='k', linestyles='dotted', label='Min Validation Loss: {}'.format(min_val_loss))\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    # plt.ylim((0.58, 0.71))\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bc6a1-38ca-49d8-9ecc-1d9be4dd8d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(losses, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4dfc4-cc06-41fd-a294-bf2cf2690035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = datasets['test']\n",
    "pred = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b551daf-fd42-455c-ac80-b9aa0908fe81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newModel = LightGCN(datasets['train'], args['num_layers'], emb_size=args['emb_size']).to(args['device'])\n",
    "\n",
    "newModel.load_state_dict(torch.load('pm.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77372322-0b1e-4470-92ae-a839a2abfd74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.03 GiB (GPU 0; 7.80 GiB total capacity; 4.26 GiB already allocated; 1.27 GiB free; 6.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnewModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mLightGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# We take an average of ever layer's node embeddings\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# print(\"x shape\",x.shape)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# print(\"mean_layer shape\",mean_layer.shape)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     mean_layer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mLightGCNConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:484\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    487\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mLightGCNConv.aggregate\u001b[0;34m(self, inputs, index, dim_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, index, dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m     node_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim\n\u001b[0;32m---> 20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_scatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch_scatter/scatter.py:156\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_mul(src, index, dim, out, dim_size)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_min(src, index, dim, out, dim_size)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch_scatter/scatter.py:41\u001b[0m, in \u001b[0;36mscatter_mean\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_mean\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, index: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     39\u001b[0m                  out: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                  dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 41\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msize(dim)\n\u001b[1;32m     44\u001b[0m     index_dim \u001b[38;5;241m=\u001b[39m dim\n",
      "File \u001b[0;32m~/.conda/envs/ayush_project/lib/python3.10/site-packages/torch_scatter/scatter.py:19\u001b[0m, in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m     size[dim] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     size[dim] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size, dtype\u001b[38;5;241m=\u001b[39msrc\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39msrc\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.03 GiB (GPU 0; 7.80 GiB total capacity; 4.26 GiB already allocated; 1.27 GiB free; 6.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e03714-d91b-4cc9-b2fb-ceffeb950090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = datasets['test']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8a5a8-056c-4dc1-a4c0-c4b272c0af85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e088514-d2e7-48b0-80a7-def67abae7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(ds.edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba87275-4205-40ac-b8f6-774768d24d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict1 = {'preds':pred.to('cpu').detach().numpy(), 'edge_label': ds.edge_label.to('cpu').detach().numpy(), 'src_node': ds.edge_label_index[0].to('cpu').detach().numpy(), 'dest_node': ds.edge_label_index[1].to('cpu').detach().numpy()}\n",
    "df = pd.DataFrame(dict1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71fb42-fc5b-4151-931b-dc027d0cf224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_0 = df[df['edge_label'] == 0]\n",
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15da10-f9e0-405f-97be-39d17965ea92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1 = df[df['edge_label'] == 1]\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0e93e-b815-47f1-8a14-6b56b4ac0f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lessthan = df_1[df_1['preds'] > 0.5]\n",
    "df_lessthan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549d1f9-a11c-4316-8db8-46f1eba89cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lessthan.iloc[0]['src_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9737a6-f63e-43e4-86c2-3f7817d9120f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_nx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a70249-0d4c-4d83-9356-4927956b0d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lessthan.iloc[0][\"src_node\"]\n",
    "counter=0\n",
    "neg=0\n",
    "for x in g_nx.nodes:\n",
    "    if(counter!=x):\n",
    "        neg=neg+x-counter\n",
    "        counter=x\n",
    "    counter=counter+1\n",
    "print(len(g_nx.nodes))\n",
    "neg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84feddca-e722-4ba3-88f6-52870d95a38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc283a02-b0bc-49ea-8776-0b79f321be6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def row_op(row):\n",
    "\n",
    "    if g_nx.nodes[row['src_node']]['name'] == g_nx.nodes[row['dest_node']]['name'] or g_nx.nodes[row['src_node']]['uri'] == g_nx.nodes[row['dest_node']]['uri']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df_lessthan['is_valid'] = df_lessthan.apply(row_op,axis=1)\n",
    "df_filter=df_lessthan[df_lessthan['is_valid']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d3ae7-48bf-480d-a8de-883fa4c6085f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getId(row):\n",
    "    return pd.Series([g_nx.nodes[row['src_node']]['name'],g_nx.nodes[row['src_node']]['uri'],g_nx.nodes[row['src_node']]['type'],g_nx.nodes[row['dest_node']]['name'],g_nx.nodes[row['dest_node']]['uri'],g_nx.nodes[row['dest_node']]['type']])\n",
    "    \n",
    "df_filter[[\"src_name\",\"src_uri\",\"src_type\",\"dest_name\",\"dest_uri\",\"dest_type\"]]=df_filter[df_lessthan['is_valid']].apply(getId,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95775e86-1b83-4f29-b8b5-373e2f0de91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f80f71-0bae-46b0-bb2d-411ee0015f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = df_filter[(df_filter['src_name']==\"Party mix\") | ( df_filter['dest_name']==\"Party mix\") ]\n",
    "df_final.sort_values('preds',ascending=False)\n",
    "uri_list=list(df_final.apply(lambda x: x['src_uri'] if x['src_uri']!='' else x['dest_uri'], axis=1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8a113-86ca-4a1d-96d3-4fba616b28c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5ba22-cf68-4f75-9e03-22f582a16315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uri_list=[]\n",
    "uri_list = list(df_final[\"dest_uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083828e-03cc-4ce3-9494-fd35a83ae9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555332b-4acd-4e25-8649-8c794c3fbb23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uri_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07ffd0-0996-4152-8860-2de9e3b8762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "def gettrackname(uri_list):\n",
    "    # Replace the values below with your own Spotify API credentials\n",
    "    client_id = 'd5566a60926740f3a8070889731a2d21'\n",
    "    client_secret = 'eb5fc0638a1241c3a611186ff8d167e3'\n",
    "\n",
    "    # Initialize the Spotify API client\n",
    "    client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "    sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "    info = []\n",
    "    for uri in uri_list:\n",
    "        # Use the track method to get information about the track\n",
    "        track_info = sp.track(uri)\n",
    "\n",
    "        # Get the track name from the track information\n",
    "        track_name = track_info['name']\n",
    "        track_info = sp.track(uri)\n",
    "\n",
    "        # Get the artist name from the track information\n",
    "        artist_name = track_info['artists'][0]['name']\n",
    "        info.append((track_name, artist_name))\n",
    "        \n",
    "    return info\n",
    "\n",
    "print(gettrackname(uri_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97554ce9-7e68-4b62-8185-f6266bf56a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g_nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m node_attrs \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mget_node_attributes(\u001b[43mg_nx\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g_nx' is not defined"
     ]
    }
   ],
   "source": [
    "node_attrs = nx.get_node_attributes(g_nx, 'color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81b8d7-8780-46d1-8f13-6b622f83928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b4c06-d86a-4359-9420-63bdb8742c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df22e9-6716-4101-a0a9-7a06fdace80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0b57a-939a-43c7-b90e-81f7a512aa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d4504-c4c3-41a6-a184-7c10579874af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
