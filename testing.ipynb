{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089916e0-d3a9-46d6-a1b0-f3d1d46c4728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0')\n",
      "2.1.0\n",
      "graph-tool version: 2.46\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(torch.zeros(1).cuda())\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "print(torch_geometric.__version__)\n",
    "\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType, OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "import networkx as nx\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import deepsnap\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "from pathlib import Path as Data_Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import json\n",
    "print(\"graph-tool version: {}\".format(gt.__version__.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33b44dcc-a8f9-42ad-8d4c-bf6bb5459424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../graph_pt_100.pickle\", \"rb\") as f:\n",
    "    final_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d4dac8-bd7d-448d-a997-38828f47670f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 3644438\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices:\", final_graph.num_vertices())\n",
    "print(\"Number of edges:\", final_graph.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ce8b6a-52c4-4ea4-8abc-9ddf8076e1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without artists as nodes:\n",
      "Number of vertices: 3644438\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print('without artists as nodes:')\n",
    "print(\"Number of vertices:\", final_graph.num_vertices())\n",
    "print(\"Number of edges:\", final_graph.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1d7fb2-5821-48d9-9f8d-2bff99d281ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count_pl = sum(1 for v in final_graph.vertices() if final_graph.vp.pl_type[v] == \"playlist\")\n",
    "# print(f'num_playlists: {count_pl}')\n",
    "# count_tr = sum(1 for v in final_graph.vertices() if final_graph.vp.tr_type[v] == \"track\")\n",
    "# print(f'num_tracks: {count_tr}')\n",
    "# count_ar = sum(1 for v in final_graph.vertices() if final_graph.vp.ar_type[v] == \"artist\")\n",
    "# print(f'num_artist: {count_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ff9dcb-d4d0-4d0e-96fd-87d382133bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without artists as nodes:\n",
      "num_playlists: 102000\n",
      "num_tracks: 3542438\n",
      "num_artist: 0\n"
     ]
    }
   ],
   "source": [
    "print('without artists as nodes:')\n",
    "count_pl = sum(1 for v in final_graph.vertices() if final_graph.vp.pl_type[v] == \"playlist\")\n",
    "print(f'num_playlists: {count_pl}')\n",
    "count_tr = sum(1 for v in final_graph.vertices() if final_graph.vp.tr_type[v] == \"track\")\n",
    "print(f'num_tracks: {count_tr}')\n",
    "count_ar = sum(1 for v in final_graph.vertices() if final_graph.vp.ar_type[v] == \"artist\")\n",
    "print(f'num_artist: {count_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f07f7199-da31-4214-b161-fb78c4bd3fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Removal of duplicate nodes from pickle file\n",
    "\n",
    "\n",
    "# prop_name = \"pl_type\"\n",
    "\n",
    "# # Create a dictionary of property values to vertex IDs\n",
    "# vertex_dict = {}\n",
    "# for v in final_graph.vertices():\n",
    "#     value = final_graph.vp[prop_name][v]\n",
    "#     if value not in vertex_dict:\n",
    "#         vertex_dict[value] = [int(v)]\n",
    "#     else:\n",
    "#         vertex_dict[value].append(int(v))\n",
    "\n",
    "# # Remove duplicate nodes\n",
    "# for value, ids in vertex_dict.items():\n",
    "#     if len(ids) > 1:\n",
    "#         print(value,ids)\n",
    "#         # # Merge the duplicate nodes into the first node\n",
    "#         # first_id = ids[0]\n",
    "#         # for other_id in ids[1:]:\n",
    "#         #     final_graph.vertex(first_id).out_edges()[:] = gt.find_edge(final_graph, final_graph.vertex(first_id), final_graph.vertex(other_id))\n",
    "#         #     final_graph.vertex(first_id).out_edges()[:] = gt.find_edge(final_graph, final_graph.vertex(other_id), final_graph.vertex(first_id))\n",
    "#         #     final_graph.remove_vertex(final_graph.vertex(other_id))\n",
    "\n",
    "\n",
    "# print(\"Number of vertices:\", final_graph.num_vertices())\n",
    "# print(\"Number of edges:\", final_graph.num_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc727a-3d43-464c-80cc-a632d1734eaf",
   "metadata": {},
   "source": [
    "### Largest component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c0c05-68c6-472b-bc4a-82b34f522814",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Graph-tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbdef74b-ab91-42cd-a3a8-c6417616e4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "largest_comp = gt.extract_largest_component(final_graph)\n",
    "# largest_comp = gt.GraphView(final_graph, vfilt = gt.label_largest_component(final_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e55c0969-d7e1-4f03-8944-c69bc2d3c082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no artists:\n",
      "Number of vertices: 38565\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print('no artists:')\n",
    "print(\"Number of vertices:\", largest_comp.num_vertices()) \n",
    "print(\"Number of edges:\", largest_comp.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f262e497-bab7-4459-b4c6-ffbd4d889e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no artists stats:\n",
      "----------------\n",
      "num_playlists: 2999\n",
      "num_tracks: 35566\n",
      "num_artist: 0\n"
     ]
    }
   ],
   "source": [
    "print('no artists stats:')\n",
    "print('----------------')\n",
    "count_pl = sum(1 for v in largest_comp.vertices() if largest_comp.vp.pl_type[v] == \"playlist\")\n",
    "print(f'num_playlists: {count_pl}')\n",
    "count_tr = sum(1 for v in largest_comp.vertices() if largest_comp.vp.tr_type[v] == \"track\")\n",
    "print(f'num_tracks: {count_tr}')\n",
    "count_ar = sum(1 for v in largest_comp.vertices() if largest_comp.vp.ar_type[v] == \"artist\")\n",
    "print(f'num_artist: {count_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c1e9154-e09f-436d-95b9-cd91d9a37bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #### Networkx\n",
    "# nx_graph = nx.Graph()\n",
    "# for node in final_graph.vertices():\n",
    "#     nx_graph.add_node(int(node))\n",
    "# for edge in final_graph.edges():\n",
    "#     nx_graph.add_edge(int(edge.source()), int(edge.target()))\n",
    "\n",
    "# from networkx.algorithms.components import is_connected\n",
    "# is_connected(nx_graph)\n",
    "\n",
    "# largest_cc = max(nx.connected_components(nx_graph), key=len)\n",
    "# nx_largest_comp = nx.Graph(nx_graph.subgraph(largest_cc))\n",
    "# print('Num nodes:', nx_largest_comp.number_of_nodes(), '. Num edges:', nx_largest_comp.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7eeef-d91c-4a0e-9a33-e2593ccedc3a",
   "metadata": {},
   "source": [
    "### Dataframe generation for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f164b8d-ec9a-493c-8417-d5ac9de71ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_playlist_df(playlist_id, playlist_json):\n",
    "    cols = ['plst_id', 'plst_name', 'track_id', 'track_name']\n",
    "    data_col = []\n",
    "    plst_name = playlist_json['name']\n",
    "    \n",
    "    for track in playlist_json['tracks']:\n",
    "        track_id = ''.join(track['track_uri'].split(':')[2:])\n",
    "        data_col.append([f'plst_{playlist_id}', plst_name, track_id, track['track_name']])\n",
    "        \n",
    "    plst_df = pd.DataFrame(data=data_col, columns=cols)\n",
    "    return plst_df\n",
    "\n",
    "def generate_file_df(start_index, file_json):\n",
    "    dfs = []\n",
    "    for i, plst_json in enumerate(file_json['playlists']):\n",
    "        dfs.append(generate_playlist_df(start_index + i, plst_json))\n",
    "        \n",
    "    df_sum = pd.concat(dfs)\n",
    "    return df_sum\n",
    "\n",
    "def generate_spotify_df(data_path, start_index=None, end_index=None):\n",
    "    assert (start_index is None and end_index is None) or \\\n",
    "        (start_index is not None and end_index is not None), 'Set both or none indices.'\n",
    "    json_names = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '.json' in f]\n",
    "    \n",
    "    num_playlists = start_index if start_index is not None else 0\n",
    "    section = json_names if start_index is None else json_names[start_index:end_index]\n",
    "    dfs = []\n",
    "    \n",
    "    for file_name in tqdm(section, desc='Files processed: ', unit='files', total=len(section)):\n",
    "        with open(join(data_path, file_name)) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        dfs.append(generate_file_df(num_playlists, data))\n",
    "        num_playlists += len(data['playlists'])\n",
    "        \n",
    "    df_total = pd.concat(dfs)\n",
    "    return df_total\n",
    "\n",
    "# data_path = '../data/'\n",
    "# df = generate_spotify_df(data_path, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30978f4-29e7-4fb8-91b1-f435007437a2",
   "metadata": {},
   "source": [
    "### N-hop neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "590c5c22-2819-4ae6-88cc-7b97ae1ed7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b137d90-07e9-4816-be08-3370a8a33e2a",
   "metadata": {},
   "source": [
    "### Deepsnap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aefdf07-327c-4f75-9070-3c49a5acb4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_nx = nx.Graph()\n",
    "\n",
    "g_nx = nx.Graph()\n",
    "for v in largest_comp.vertices():\n",
    "    g_nx.add_node(int(v))\n",
    "\n",
    "# Add edges to NetworkX graph in batches\n",
    "batch_size = 100000\n",
    "edges = [(int(e.source()), int(e.target())) for e in largest_comp.edges()]\n",
    "num_edges = len(edges)\n",
    "for i in range(0, num_edges, batch_size):\n",
    "    batch_edges = edges[i:i+batch_size]\n",
    "    g_nx.add_edges_from(batch_edges)\n",
    "\n",
    "        \n",
    "# Create a DeepSNAP graph from NetworkX graph\n",
    "ds_graph = Graph(g_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6d20f51-8c12-4508-9960-95f10c63f857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 38565\n",
      "Number of edges: 1813198\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nodes:\", g_nx.number_of_nodes())\n",
    "print(\"Number of edges:\", g_nx.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee3c05b5-8a28-4d3b-94d4-af5eb1a96032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 38565\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices:\", largest_comp.num_vertices()) \n",
    "print(\"Number of edges:\", largest_comp.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6b5773f4-4af4-49ea-ba46-781b4fa3aca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphDataset(1)\n",
      "<class 'deepsnap.dataset.GraphDataset'>\n",
      "Graph(G=[], edge_index=[2, 2320892], edge_label=[1160448], edge_label_index=[2, 1160448], negative_label_val=[1], node_label_index=[38565])\n",
      "<class 'deepsnap.graph.Graph'>\n",
      "Train set has 580224 supervision (positive) edges\n",
      "Validation set has 362638 supervision (positive) edges\n",
      "Test set has 362642 supervision (positive) edges\n",
      "Train set has 2320892 message passing edges\n",
      "Validation set has 2901116 message passing edges\n",
      "Test set has 3263754 message passing edges\n"
     ]
    }
   ],
   "source": [
    "task = 'link_pred'\n",
    "dataset = GraphDataset([ds_graph], task=task, edge_train_mode='disjoint')\n",
    "\n",
    "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "# dataset_train[0].to('cuda')\n",
    "# dataset_val[0].to('cuda')\n",
    "# dataset_test[0].to('cuda')\n",
    "\n",
    "# dataset_train.to('cuda:0')\n",
    "# dataset_val.to('cuda:0')\n",
    "# dataset_test.to('cuda:0')\n",
    "\n",
    "print(type(dataset_train))\n",
    "print(dataset_train[0])\n",
    "print(type(dataset_train[0]))\n",
    "\n",
    "num_train_edges = dataset_train[0].edge_label_index.shape[1]\n",
    "num_val_edges = dataset_val[0].edge_label_index.shape[1]\n",
    "num_test_edges = dataset_test[0].edge_label_index.shape[1]\n",
    "\n",
    "print(\"Train set has {} supervision (positive) edges\".format(num_train_edges // 2))\n",
    "print(\"Validation set has {} supervision (positive) edges\".format(num_val_edges // 2))\n",
    "print(\"Test set has {} supervision (positive) edges\".format(num_test_edges // 2))\n",
    "\n",
    "print(\"Train set has {} message passing edges\".format(dataset_train[0].edge_index.shape[1]))\n",
    "print(\"Validation set has {} message passing edges\".format(dataset_val[0].edge_index.shape[1]))\n",
    "print(\"Test set has {} message passing edges\".format(dataset_test[0].edge_index.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404e392-347d-444e-8cf1-3a898b6eae87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pickle.dump(dataset_train, open('./graphs/train.graph', 'wb'))\n",
    "# pickle.dump(dataset_val, open('./graphs/val.graph', 'wb'))\n",
    "# pickle.dump(dataset_test, open('./graphs/test.graph', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cb885-4942-439a-8766-a0b7f9ebbc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_train = pickle.load(open('./graphs/train.graph', 'rb'))\n",
    "# dataset_val = pickle.load(open('./graphs/val.graph', 'rb'))\n",
    "# dataset_test = pickle.load(open('./graphs/test.graph', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b59dec8f-30e8-4c5b-ab48-3cccfff33387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(LightGCNConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        out = self.propagate(edge_index, x=(x, x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        out = x_j\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        node_dim = self.node_dim\n",
    "        out = torch_scatter.scatter(inputs, index, dim=node_dim, reduce='mean')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "63ff20e2-0434-4fd8-89a1-e8db325c3e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCN(torch.nn.Module):\n",
    "    def __init__(self, train_data, num_layers, emb_size=16, initialize_with_words=False):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        assert (num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(num_layers):\n",
    "            self.convs.append(LightGCNConv(input_dim, input_dim))\n",
    "\n",
    "        # Initialize using custom embeddings if provided\n",
    "        num_nodes = train_data.node_label_index.size()[0]\n",
    "        self.embeddings = nn.Embedding(num_nodes, emb_size)\n",
    "        if initialize_with_words:\n",
    "            self.embeddings.weight.data.copy_(train_datanode_features)\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_size = emb_size\n",
    "        self.num_modes = num_nodes\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_label_index, node_label_index = data.edge_index, data.edge_label_index, data.node_label_index\n",
    "        layer_embeddings = []\n",
    "        \n",
    "        x = self.embeddings(node_label_index)\n",
    "        mean_layer = x\n",
    "\n",
    "        # We take an average of ever layer's node embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            mean_layer += x\n",
    "\n",
    "        mean_layer /= 4\n",
    "\n",
    "        # Prediction head is simply dot product\n",
    "        nodes_first = torch.index_select(x, 0, edge_label_index[0,:].long())\n",
    "        nodes_second = torch.index_select(x, 0, edge_label_index[1,:].long())\n",
    "\n",
    "        # Since we don't want a rank output, we create a sigmoid of the dot product\n",
    "        out = torch.sum(nodes_first * nodes_second, dim=-1) # FOR RANKING\n",
    "        pred = torch.sigmoid(out)\n",
    "\n",
    "        return torch.flatten(pred)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return self.loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e8fffe68-e6e4-4892-9b18-13be04810acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8633e9c3-115c-4ed1-a559-c6d2d9267b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deepsnap.dataset.GraphDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1ebe5156-60c7-4699-93f5-03466c7433d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'device': 'cuda', 'num_layers': 3, 'emb_size': 32, 'epochs': 300, 'weight_decay': 1e-05, 'lr': 0.01}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_layers' : 3,\n",
    "    'emb_size' : 32,\n",
    "    'epochs' : 1,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 300\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'train': dataset_train[0],\n",
    "    'val': dataset_val[0],\n",
    "    'test': dataset_test[0]\n",
    "}\n",
    "            \n",
    "input_dim = datasets['train'].num_node_features\n",
    "print(input_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ed65aa42-8be4-450f-bdec-aa2206feb866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets['train'].to(args['device'])\n",
    "# datasets['val'].to(args['device'])\n",
    "# datasets['test'].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "941df0fc-fc53-4f9d-b457-b2c562175a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, optimizer, args):\n",
    "    val_max = 0\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        datasets['train'].to(args[\"device\"])\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(datasets['train'])\n",
    "        loss = model.loss(pred, datasets['train'].edge_label.type(pred.dtype))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Loss: {:.5f}, Val Loss: {:.5f}'\n",
    "        score_train, train_loss = test(model, 'train', args)\n",
    "        score_val, val_loss = test(model, 'val', args)\n",
    "        score_test, test_loss = test(model, 'test', args)\n",
    "\n",
    "        losses.append((train_loss, val_loss))\n",
    "\n",
    "        print(log.format(epoch, score_train, score_val, score_test, train_loss, val_loss))\n",
    "        if val_max < score_val:\n",
    "            val_max = score_val\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def test(model, mode, args):\n",
    "    model.eval()\n",
    "    score = 0\n",
    "    loss_score = 0\n",
    "\n",
    "    data = datasets[mode]\n",
    "    data.to(args[\"device\"])\n",
    "\n",
    "    pred = model(data)\n",
    "    loss = model.loss(pred, data.edge_label.type(pred.dtype))\n",
    "    score += roc_auc_score(data.edge_label.flatten().cpu().numpy(), pred.flatten().data.cpu().numpy())\n",
    "    loss_score += loss.item()\n",
    "\n",
    "    return score, loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8ed78571-2201-450e-a3aa-6bd29ba4529e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.5244, Val: 0.5228, Test: 0.5232, Loss: 0.69823, Val Loss: 0.69841\n",
      "Epoch: 002, Train: 0.5728, Val: 0.5719, Test: 0.5720, Loss: 0.69597, Val Loss: 0.69614\n",
      "Epoch: 003, Train: 0.6084, Val: 0.6072, Test: 0.6074, Loss: 0.69435, Val Loss: 0.69451\n",
      "Epoch: 004, Train: 0.6348, Val: 0.6330, Test: 0.6332, Loss: 0.69321, Val Loss: 0.69335\n",
      "Epoch: 005, Train: 0.6550, Val: 0.6530, Test: 0.6530, Loss: 0.69241, Val Loss: 0.69254\n",
      "Epoch: 006, Train: 0.6703, Val: 0.6683, Test: 0.6682, Loss: 0.69184, Val Loss: 0.69197\n",
      "Epoch: 007, Train: 0.6813, Val: 0.6795, Test: 0.6793, Loss: 0.69142, Val Loss: 0.69155\n",
      "Epoch: 008, Train: 0.6900, Val: 0.6882, Test: 0.6881, Loss: 0.69109, Val Loss: 0.69121\n",
      "Epoch: 009, Train: 0.6978, Val: 0.6960, Test: 0.6961, Loss: 0.69080, Val Loss: 0.69091\n",
      "Epoch: 010, Train: 0.7052, Val: 0.7036, Test: 0.7038, Loss: 0.69051, Val Loss: 0.69063\n",
      "Epoch: 011, Train: 0.7128, Val: 0.7112, Test: 0.7115, Loss: 0.69023, Val Loss: 0.69034\n",
      "Epoch: 012, Train: 0.7204, Val: 0.7187, Test: 0.7191, Loss: 0.68993, Val Loss: 0.69004\n",
      "Epoch: 013, Train: 0.7275, Val: 0.7257, Test: 0.7262, Loss: 0.68962, Val Loss: 0.68973\n",
      "Epoch: 014, Train: 0.7334, Val: 0.7314, Test: 0.7319, Loss: 0.68930, Val Loss: 0.68941\n",
      "Epoch: 015, Train: 0.7383, Val: 0.7361, Test: 0.7368, Loss: 0.68896, Val Loss: 0.68907\n",
      "Epoch: 016, Train: 0.7426, Val: 0.7404, Test: 0.7412, Loss: 0.68861, Val Loss: 0.68872\n",
      "Epoch: 017, Train: 0.7462, Val: 0.7440, Test: 0.7448, Loss: 0.68825, Val Loss: 0.68836\n",
      "Epoch: 018, Train: 0.7493, Val: 0.7471, Test: 0.7479, Loss: 0.68789, Val Loss: 0.68800\n",
      "Epoch: 019, Train: 0.7517, Val: 0.7495, Test: 0.7503, Loss: 0.68752, Val Loss: 0.68763\n",
      "Epoch: 020, Train: 0.7537, Val: 0.7514, Test: 0.7521, Loss: 0.68714, Val Loss: 0.68726\n",
      "Epoch: 021, Train: 0.7553, Val: 0.7529, Test: 0.7536, Loss: 0.68675, Val Loss: 0.68687\n",
      "Epoch: 022, Train: 0.7567, Val: 0.7542, Test: 0.7548, Loss: 0.68636, Val Loss: 0.68648\n",
      "Epoch: 023, Train: 0.7579, Val: 0.7554, Test: 0.7559, Loss: 0.68595, Val Loss: 0.68608\n",
      "Epoch: 024, Train: 0.7591, Val: 0.7566, Test: 0.7570, Loss: 0.68554, Val Loss: 0.68567\n",
      "Epoch: 025, Train: 0.7603, Val: 0.7577, Test: 0.7581, Loss: 0.68511, Val Loss: 0.68525\n",
      "Epoch: 026, Train: 0.7613, Val: 0.7588, Test: 0.7592, Loss: 0.68466, Val Loss: 0.68480\n",
      "Epoch: 027, Train: 0.7624, Val: 0.7598, Test: 0.7603, Loss: 0.68420, Val Loss: 0.68434\n",
      "Epoch: 028, Train: 0.7634, Val: 0.7609, Test: 0.7614, Loss: 0.68371, Val Loss: 0.68386\n",
      "Epoch: 029, Train: 0.7644, Val: 0.7619, Test: 0.7624, Loss: 0.68320, Val Loss: 0.68335\n",
      "Epoch: 030, Train: 0.7654, Val: 0.7630, Test: 0.7634, Loss: 0.68266, Val Loss: 0.68281\n",
      "Epoch: 031, Train: 0.7664, Val: 0.7640, Test: 0.7645, Loss: 0.68210, Val Loss: 0.68225\n",
      "Epoch: 032, Train: 0.7673, Val: 0.7651, Test: 0.7655, Loss: 0.68150, Val Loss: 0.68166\n",
      "Epoch: 033, Train: 0.7684, Val: 0.7662, Test: 0.7666, Loss: 0.68088, Val Loss: 0.68103\n",
      "Epoch: 034, Train: 0.7694, Val: 0.7673, Test: 0.7677, Loss: 0.68022, Val Loss: 0.68037\n",
      "Epoch: 035, Train: 0.7703, Val: 0.7683, Test: 0.7688, Loss: 0.67953, Val Loss: 0.67968\n",
      "Epoch: 036, Train: 0.7712, Val: 0.7692, Test: 0.7697, Loss: 0.67880, Val Loss: 0.67896\n",
      "Epoch: 037, Train: 0.7717, Val: 0.7698, Test: 0.7703, Loss: 0.67804, Val Loss: 0.67820\n",
      "Epoch: 038, Train: 0.7720, Val: 0.7701, Test: 0.7706, Loss: 0.67725, Val Loss: 0.67740\n",
      "Epoch: 039, Train: 0.7721, Val: 0.7702, Test: 0.7708, Loss: 0.67642, Val Loss: 0.67657\n",
      "Epoch: 040, Train: 0.7721, Val: 0.7702, Test: 0.7708, Loss: 0.67555, Val Loss: 0.67571\n",
      "Epoch: 041, Train: 0.7720, Val: 0.7701, Test: 0.7707, Loss: 0.67465, Val Loss: 0.67481\n",
      "Epoch: 042, Train: 0.7718, Val: 0.7699, Test: 0.7705, Loss: 0.67371, Val Loss: 0.67387\n",
      "Epoch: 043, Train: 0.7716, Val: 0.7697, Test: 0.7702, Loss: 0.67274, Val Loss: 0.67290\n",
      "Epoch: 044, Train: 0.7715, Val: 0.7695, Test: 0.7700, Loss: 0.67173, Val Loss: 0.67189\n",
      "Epoch: 045, Train: 0.7713, Val: 0.7694, Test: 0.7699, Loss: 0.67069, Val Loss: 0.67085\n",
      "Epoch: 046, Train: 0.7712, Val: 0.7693, Test: 0.7698, Loss: 0.66962, Val Loss: 0.66978\n",
      "Epoch: 047, Train: 0.7712, Val: 0.7693, Test: 0.7698, Loss: 0.66851, Val Loss: 0.66867\n",
      "Epoch: 048, Train: 0.7713, Val: 0.7694, Test: 0.7699, Loss: 0.66737, Val Loss: 0.66754\n",
      "Epoch: 049, Train: 0.7715, Val: 0.7696, Test: 0.7701, Loss: 0.66621, Val Loss: 0.66638\n",
      "Epoch: 050, Train: 0.7717, Val: 0.7699, Test: 0.7704, Loss: 0.66502, Val Loss: 0.66519\n",
      "Epoch: 051, Train: 0.7721, Val: 0.7703, Test: 0.7707, Loss: 0.66381, Val Loss: 0.66398\n",
      "Epoch: 052, Train: 0.7726, Val: 0.7708, Test: 0.7712, Loss: 0.66258, Val Loss: 0.66275\n",
      "Epoch: 053, Train: 0.7732, Val: 0.7715, Test: 0.7718, Loss: 0.66132, Val Loss: 0.66150\n",
      "Epoch: 054, Train: 0.7739, Val: 0.7722, Test: 0.7726, Loss: 0.66006, Val Loss: 0.66023\n",
      "Epoch: 055, Train: 0.7747, Val: 0.7731, Test: 0.7734, Loss: 0.65878, Val Loss: 0.65895\n",
      "Epoch: 056, Train: 0.7756, Val: 0.7740, Test: 0.7743, Loss: 0.65749, Val Loss: 0.65766\n",
      "Epoch: 057, Train: 0.7765, Val: 0.7749, Test: 0.7752, Loss: 0.65619, Val Loss: 0.65637\n",
      "Epoch: 058, Train: 0.7775, Val: 0.7759, Test: 0.7761, Loss: 0.65489, Val Loss: 0.65507\n",
      "Epoch: 059, Train: 0.7784, Val: 0.7769, Test: 0.7771, Loss: 0.65359, Val Loss: 0.65377\n",
      "Epoch: 060, Train: 0.7794, Val: 0.7779, Test: 0.7781, Loss: 0.65230, Val Loss: 0.65248\n",
      "Epoch: 061, Train: 0.7804, Val: 0.7789, Test: 0.7791, Loss: 0.65100, Val Loss: 0.65119\n",
      "Epoch: 062, Train: 0.7814, Val: 0.7799, Test: 0.7801, Loss: 0.64972, Val Loss: 0.64990\n",
      "Epoch: 063, Train: 0.7824, Val: 0.7810, Test: 0.7811, Loss: 0.64844, Val Loss: 0.64863\n",
      "Epoch: 064, Train: 0.7835, Val: 0.7820, Test: 0.7821, Loss: 0.64717, Val Loss: 0.64736\n",
      "Epoch: 065, Train: 0.7845, Val: 0.7830, Test: 0.7831, Loss: 0.64592, Val Loss: 0.64611\n",
      "Epoch: 066, Train: 0.7855, Val: 0.7840, Test: 0.7841, Loss: 0.64468, Val Loss: 0.64488\n",
      "Epoch: 067, Train: 0.7865, Val: 0.7850, Test: 0.7850, Loss: 0.64346, Val Loss: 0.64365\n",
      "Epoch: 068, Train: 0.7875, Val: 0.7860, Test: 0.7860, Loss: 0.64225, Val Loss: 0.64245\n",
      "Epoch: 069, Train: 0.7884, Val: 0.7870, Test: 0.7870, Loss: 0.64106, Val Loss: 0.64127\n",
      "Epoch: 070, Train: 0.7894, Val: 0.7880, Test: 0.7879, Loss: 0.63989, Val Loss: 0.64010\n",
      "Epoch: 071, Train: 0.7903, Val: 0.7889, Test: 0.7888, Loss: 0.63874, Val Loss: 0.63895\n",
      "Epoch: 072, Train: 0.7912, Val: 0.7898, Test: 0.7897, Loss: 0.63761, Val Loss: 0.63782\n",
      "Epoch: 073, Train: 0.7921, Val: 0.7907, Test: 0.7906, Loss: 0.63650, Val Loss: 0.63672\n",
      "Epoch: 074, Train: 0.7929, Val: 0.7916, Test: 0.7914, Loss: 0.63542, Val Loss: 0.63563\n",
      "Epoch: 075, Train: 0.7938, Val: 0.7924, Test: 0.7923, Loss: 0.63435, Val Loss: 0.63457\n",
      "Epoch: 076, Train: 0.7946, Val: 0.7933, Test: 0.7931, Loss: 0.63330, Val Loss: 0.63352\n",
      "Epoch: 077, Train: 0.7954, Val: 0.7941, Test: 0.7939, Loss: 0.63227, Val Loss: 0.63250\n",
      "Epoch: 078, Train: 0.7961, Val: 0.7949, Test: 0.7947, Loss: 0.63127, Val Loss: 0.63150\n",
      "Epoch: 079, Train: 0.7969, Val: 0.7956, Test: 0.7954, Loss: 0.63029, Val Loss: 0.63052\n",
      "Epoch: 080, Train: 0.7976, Val: 0.7964, Test: 0.7961, Loss: 0.62932, Val Loss: 0.62956\n",
      "Epoch: 081, Train: 0.7984, Val: 0.7971, Test: 0.7969, Loss: 0.62838, Val Loss: 0.62862\n",
      "Epoch: 082, Train: 0.7991, Val: 0.7978, Test: 0.7976, Loss: 0.62746, Val Loss: 0.62770\n",
      "Epoch: 083, Train: 0.7998, Val: 0.7985, Test: 0.7983, Loss: 0.62656, Val Loss: 0.62681\n",
      "Epoch: 084, Train: 0.8005, Val: 0.7992, Test: 0.7990, Loss: 0.62569, Val Loss: 0.62593\n",
      "Epoch: 085, Train: 0.8012, Val: 0.7999, Test: 0.7996, Loss: 0.62483, Val Loss: 0.62508\n",
      "Epoch: 086, Train: 0.8018, Val: 0.8006, Test: 0.8003, Loss: 0.62399, Val Loss: 0.62424\n",
      "Epoch: 087, Train: 0.8025, Val: 0.8012, Test: 0.8010, Loss: 0.62318, Val Loss: 0.62343\n",
      "Epoch: 088, Train: 0.8031, Val: 0.8019, Test: 0.8016, Loss: 0.62238, Val Loss: 0.62264\n",
      "Epoch: 089, Train: 0.8038, Val: 0.8025, Test: 0.8023, Loss: 0.62160, Val Loss: 0.62186\n",
      "Epoch: 090, Train: 0.8044, Val: 0.8031, Test: 0.8029, Loss: 0.62084, Val Loss: 0.62111\n",
      "Epoch: 091, Train: 0.8050, Val: 0.8037, Test: 0.8035, Loss: 0.62010, Val Loss: 0.62037\n",
      "Epoch: 092, Train: 0.8056, Val: 0.8043, Test: 0.8041, Loss: 0.61938, Val Loss: 0.61965\n",
      "Epoch: 093, Train: 0.8062, Val: 0.8049, Test: 0.8046, Loss: 0.61867, Val Loss: 0.61895\n",
      "Epoch: 094, Train: 0.8067, Val: 0.8055, Test: 0.8052, Loss: 0.61798, Val Loss: 0.61826\n",
      "Epoch: 095, Train: 0.8073, Val: 0.8060, Test: 0.8057, Loss: 0.61731, Val Loss: 0.61759\n",
      "Epoch: 096, Train: 0.8078, Val: 0.8065, Test: 0.8063, Loss: 0.61666, Val Loss: 0.61694\n",
      "Epoch: 097, Train: 0.8083, Val: 0.8071, Test: 0.8068, Loss: 0.61601, Val Loss: 0.61630\n",
      "Epoch: 098, Train: 0.8088, Val: 0.8076, Test: 0.8073, Loss: 0.61539, Val Loss: 0.61567\n",
      "Epoch: 099, Train: 0.8093, Val: 0.8080, Test: 0.8078, Loss: 0.61478, Val Loss: 0.61507\n",
      "Epoch: 100, Train: 0.8098, Val: 0.8085, Test: 0.8082, Loss: 0.61418, Val Loss: 0.61447\n",
      "Epoch: 101, Train: 0.8103, Val: 0.8090, Test: 0.8087, Loss: 0.61359, Val Loss: 0.61389\n",
      "Epoch: 102, Train: 0.8107, Val: 0.8095, Test: 0.8091, Loss: 0.61302, Val Loss: 0.61332\n",
      "Epoch: 103, Train: 0.8112, Val: 0.8099, Test: 0.8096, Loss: 0.61246, Val Loss: 0.61276\n",
      "Epoch: 104, Train: 0.8116, Val: 0.8104, Test: 0.8100, Loss: 0.61192, Val Loss: 0.61222\n",
      "Epoch: 105, Train: 0.8120, Val: 0.8108, Test: 0.8104, Loss: 0.61138, Val Loss: 0.61169\n",
      "Epoch: 106, Train: 0.8125, Val: 0.8112, Test: 0.8109, Loss: 0.61086, Val Loss: 0.61117\n",
      "Epoch: 107, Train: 0.8129, Val: 0.8116, Test: 0.8113, Loss: 0.61035, Val Loss: 0.61066\n",
      "Epoch: 108, Train: 0.8133, Val: 0.8120, Test: 0.8117, Loss: 0.60985, Val Loss: 0.61016\n",
      "Epoch: 109, Train: 0.8137, Val: 0.8124, Test: 0.8121, Loss: 0.60936, Val Loss: 0.60968\n",
      "Epoch: 110, Train: 0.8141, Val: 0.8128, Test: 0.8125, Loss: 0.60889, Val Loss: 0.60920\n",
      "Epoch: 111, Train: 0.8145, Val: 0.8132, Test: 0.8129, Loss: 0.60842, Val Loss: 0.60873\n",
      "Epoch: 112, Train: 0.8148, Val: 0.8136, Test: 0.8132, Loss: 0.60796, Val Loss: 0.60828\n",
      "Epoch: 113, Train: 0.8152, Val: 0.8140, Test: 0.8136, Loss: 0.60751, Val Loss: 0.60783\n",
      "Epoch: 114, Train: 0.8156, Val: 0.8143, Test: 0.8140, Loss: 0.60707, Val Loss: 0.60739\n",
      "Epoch: 115, Train: 0.8159, Val: 0.8147, Test: 0.8143, Loss: 0.60664, Val Loss: 0.60697\n",
      "Epoch: 116, Train: 0.8163, Val: 0.8150, Test: 0.8147, Loss: 0.60622, Val Loss: 0.60655\n",
      "Epoch: 117, Train: 0.8166, Val: 0.8154, Test: 0.8150, Loss: 0.60581, Val Loss: 0.60614\n",
      "Epoch: 118, Train: 0.8169, Val: 0.8157, Test: 0.8154, Loss: 0.60540, Val Loss: 0.60573\n",
      "Epoch: 119, Train: 0.8172, Val: 0.8161, Test: 0.8157, Loss: 0.60501, Val Loss: 0.60534\n",
      "Epoch: 120, Train: 0.8176, Val: 0.8164, Test: 0.8160, Loss: 0.60462, Val Loss: 0.60495\n",
      "Epoch: 121, Train: 0.8179, Val: 0.8167, Test: 0.8163, Loss: 0.60423, Val Loss: 0.60457\n",
      "Epoch: 122, Train: 0.8182, Val: 0.8170, Test: 0.8166, Loss: 0.60386, Val Loss: 0.60420\n",
      "Epoch: 123, Train: 0.8185, Val: 0.8173, Test: 0.8169, Loss: 0.60349, Val Loss: 0.60383\n",
      "Epoch: 124, Train: 0.8188, Val: 0.8176, Test: 0.8172, Loss: 0.60313, Val Loss: 0.60347\n",
      "Epoch: 125, Train: 0.8191, Val: 0.8179, Test: 0.8175, Loss: 0.60278, Val Loss: 0.60312\n",
      "Epoch: 126, Train: 0.8193, Val: 0.8182, Test: 0.8178, Loss: 0.60243, Val Loss: 0.60277\n",
      "Epoch: 127, Train: 0.8196, Val: 0.8184, Test: 0.8181, Loss: 0.60209, Val Loss: 0.60243\n",
      "Epoch: 128, Train: 0.8199, Val: 0.8187, Test: 0.8184, Loss: 0.60176, Val Loss: 0.60210\n",
      "Epoch: 129, Train: 0.8202, Val: 0.8190, Test: 0.8186, Loss: 0.60143, Val Loss: 0.60177\n",
      "Epoch: 130, Train: 0.8204, Val: 0.8193, Test: 0.8189, Loss: 0.60110, Val Loss: 0.60145\n",
      "Epoch: 131, Train: 0.8207, Val: 0.8195, Test: 0.8192, Loss: 0.60079, Val Loss: 0.60114\n",
      "Epoch: 132, Train: 0.8209, Val: 0.8198, Test: 0.8194, Loss: 0.60048, Val Loss: 0.60083\n",
      "Epoch: 133, Train: 0.8212, Val: 0.8200, Test: 0.8197, Loss: 0.60017, Val Loss: 0.60052\n",
      "Epoch: 134, Train: 0.8214, Val: 0.8203, Test: 0.8199, Loss: 0.59987, Val Loss: 0.60022\n",
      "Epoch: 135, Train: 0.8217, Val: 0.8205, Test: 0.8201, Loss: 0.59957, Val Loss: 0.59993\n",
      "Epoch: 136, Train: 0.8219, Val: 0.8208, Test: 0.8204, Loss: 0.59928, Val Loss: 0.59964\n",
      "Epoch: 137, Train: 0.8221, Val: 0.8210, Test: 0.8206, Loss: 0.59900, Val Loss: 0.59936\n",
      "Epoch: 138, Train: 0.8224, Val: 0.8212, Test: 0.8208, Loss: 0.59872, Val Loss: 0.59908\n",
      "Epoch: 139, Train: 0.8226, Val: 0.8214, Test: 0.8211, Loss: 0.59844, Val Loss: 0.59880\n",
      "Epoch: 140, Train: 0.8228, Val: 0.8217, Test: 0.8213, Loss: 0.59817, Val Loss: 0.59853\n",
      "Epoch: 141, Train: 0.8230, Val: 0.8219, Test: 0.8215, Loss: 0.59791, Val Loss: 0.59827\n",
      "Epoch: 142, Train: 0.8232, Val: 0.8221, Test: 0.8217, Loss: 0.59764, Val Loss: 0.59800\n",
      "Epoch: 143, Train: 0.8234, Val: 0.8223, Test: 0.8219, Loss: 0.59739, Val Loss: 0.59775\n",
      "Epoch: 144, Train: 0.8236, Val: 0.8225, Test: 0.8221, Loss: 0.59713, Val Loss: 0.59750\n",
      "Epoch: 145, Train: 0.8238, Val: 0.8227, Test: 0.8223, Loss: 0.59688, Val Loss: 0.59725\n",
      "Epoch: 146, Train: 0.8240, Val: 0.8229, Test: 0.8225, Loss: 0.59664, Val Loss: 0.59700\n",
      "Epoch: 147, Train: 0.8242, Val: 0.8231, Test: 0.8227, Loss: 0.59640, Val Loss: 0.59676\n",
      "Epoch: 148, Train: 0.8244, Val: 0.8233, Test: 0.8229, Loss: 0.59616, Val Loss: 0.59652\n",
      "Epoch: 149, Train: 0.8246, Val: 0.8235, Test: 0.8231, Loss: 0.59592, Val Loss: 0.59629\n",
      "Epoch: 150, Train: 0.8248, Val: 0.8237, Test: 0.8233, Loss: 0.59569, Val Loss: 0.59606\n",
      "Epoch: 151, Train: 0.8250, Val: 0.8239, Test: 0.8235, Loss: 0.59547, Val Loss: 0.59584\n",
      "Epoch: 152, Train: 0.8251, Val: 0.8240, Test: 0.8237, Loss: 0.59524, Val Loss: 0.59561\n",
      "Epoch: 153, Train: 0.8253, Val: 0.8242, Test: 0.8239, Loss: 0.59502, Val Loss: 0.59540\n",
      "Epoch: 154, Train: 0.8255, Val: 0.8244, Test: 0.8240, Loss: 0.59481, Val Loss: 0.59518\n",
      "Epoch: 155, Train: 0.8256, Val: 0.8246, Test: 0.8242, Loss: 0.59460, Val Loss: 0.59497\n",
      "Epoch: 156, Train: 0.8258, Val: 0.8247, Test: 0.8244, Loss: 0.59439, Val Loss: 0.59476\n",
      "Epoch: 157, Train: 0.8260, Val: 0.8249, Test: 0.8246, Loss: 0.59418, Val Loss: 0.59455\n",
      "Epoch: 158, Train: 0.8261, Val: 0.8251, Test: 0.8247, Loss: 0.59398, Val Loss: 0.59435\n",
      "Epoch: 159, Train: 0.8263, Val: 0.8252, Test: 0.8249, Loss: 0.59378, Val Loss: 0.59415\n",
      "Epoch: 160, Train: 0.8264, Val: 0.8254, Test: 0.8250, Loss: 0.59358, Val Loss: 0.59395\n",
      "Epoch: 161, Train: 0.8266, Val: 0.8255, Test: 0.8252, Loss: 0.59338, Val Loss: 0.59376\n",
      "Epoch: 162, Train: 0.8268, Val: 0.8257, Test: 0.8254, Loss: 0.59319, Val Loss: 0.59357\n",
      "Epoch: 163, Train: 0.8269, Val: 0.8258, Test: 0.8255, Loss: 0.59300, Val Loss: 0.59338\n",
      "Epoch: 164, Train: 0.8270, Val: 0.8260, Test: 0.8257, Loss: 0.59282, Val Loss: 0.59320\n",
      "Epoch: 165, Train: 0.8272, Val: 0.8261, Test: 0.8258, Loss: 0.59263, Val Loss: 0.59301\n",
      "Epoch: 166, Train: 0.8273, Val: 0.8263, Test: 0.8259, Loss: 0.59245, Val Loss: 0.59283\n",
      "Epoch: 167, Train: 0.8275, Val: 0.8264, Test: 0.8261, Loss: 0.59228, Val Loss: 0.59266\n",
      "Epoch: 168, Train: 0.8276, Val: 0.8266, Test: 0.8262, Loss: 0.59210, Val Loss: 0.59248\n",
      "Epoch: 169, Train: 0.8277, Val: 0.8267, Test: 0.8264, Loss: 0.59193, Val Loss: 0.59231\n",
      "Epoch: 170, Train: 0.8279, Val: 0.8268, Test: 0.8265, Loss: 0.59176, Val Loss: 0.59214\n",
      "Epoch: 171, Train: 0.8280, Val: 0.8270, Test: 0.8266, Loss: 0.59159, Val Loss: 0.59197\n",
      "Epoch: 172, Train: 0.8281, Val: 0.8271, Test: 0.8268, Loss: 0.59142, Val Loss: 0.59181\n",
      "Epoch: 173, Train: 0.8283, Val: 0.8272, Test: 0.8269, Loss: 0.59126, Val Loss: 0.59164\n",
      "Epoch: 174, Train: 0.8284, Val: 0.8273, Test: 0.8270, Loss: 0.59110, Val Loss: 0.59148\n",
      "Epoch: 175, Train: 0.8285, Val: 0.8275, Test: 0.8271, Loss: 0.59094, Val Loss: 0.59133\n",
      "Epoch: 176, Train: 0.8286, Val: 0.8276, Test: 0.8273, Loss: 0.59078, Val Loss: 0.59117\n",
      "Epoch: 177, Train: 0.8287, Val: 0.8277, Test: 0.8274, Loss: 0.59063, Val Loss: 0.59101\n",
      "Epoch: 178, Train: 0.8289, Val: 0.8278, Test: 0.8275, Loss: 0.59048, Val Loss: 0.59086\n",
      "Epoch: 179, Train: 0.8290, Val: 0.8280, Test: 0.8276, Loss: 0.59033, Val Loss: 0.59071\n",
      "Epoch: 180, Train: 0.8291, Val: 0.8281, Test: 0.8277, Loss: 0.59018, Val Loss: 0.59057\n",
      "Epoch: 181, Train: 0.8292, Val: 0.8282, Test: 0.8279, Loss: 0.59003, Val Loss: 0.59042\n",
      "Epoch: 182, Train: 0.8293, Val: 0.8283, Test: 0.8280, Loss: 0.58989, Val Loss: 0.59028\n",
      "Epoch: 183, Train: 0.8294, Val: 0.8284, Test: 0.8281, Loss: 0.58975, Val Loss: 0.59013\n",
      "Epoch: 184, Train: 0.8295, Val: 0.8285, Test: 0.8282, Loss: 0.58960, Val Loss: 0.58999\n",
      "Epoch: 185, Train: 0.8296, Val: 0.8286, Test: 0.8283, Loss: 0.58947, Val Loss: 0.58986\n",
      "Epoch: 186, Train: 0.8297, Val: 0.8287, Test: 0.8284, Loss: 0.58933, Val Loss: 0.58972\n",
      "Epoch: 187, Train: 0.8298, Val: 0.8288, Test: 0.8285, Loss: 0.58920, Val Loss: 0.58959\n",
      "Epoch: 188, Train: 0.8299, Val: 0.8289, Test: 0.8286, Loss: 0.58906, Val Loss: 0.58945\n",
      "Epoch: 189, Train: 0.8300, Val: 0.8290, Test: 0.8287, Loss: 0.58893, Val Loss: 0.58932\n",
      "Epoch: 190, Train: 0.8301, Val: 0.8291, Test: 0.8288, Loss: 0.58880, Val Loss: 0.58919\n",
      "Epoch: 191, Train: 0.8302, Val: 0.8292, Test: 0.8289, Loss: 0.58867, Val Loss: 0.58907\n",
      "Epoch: 192, Train: 0.8303, Val: 0.8293, Test: 0.8290, Loss: 0.58855, Val Loss: 0.58894\n",
      "Epoch: 193, Train: 0.8304, Val: 0.8294, Test: 0.8291, Loss: 0.58842, Val Loss: 0.58882\n",
      "Epoch: 194, Train: 0.8305, Val: 0.8295, Test: 0.8292, Loss: 0.58830, Val Loss: 0.58869\n",
      "Epoch: 195, Train: 0.8306, Val: 0.8296, Test: 0.8293, Loss: 0.58818, Val Loss: 0.58857\n",
      "Epoch: 196, Train: 0.8307, Val: 0.8297, Test: 0.8294, Loss: 0.58806, Val Loss: 0.58845\n",
      "Epoch: 197, Train: 0.8308, Val: 0.8298, Test: 0.8295, Loss: 0.58794, Val Loss: 0.58834\n",
      "Epoch: 198, Train: 0.8309, Val: 0.8299, Test: 0.8296, Loss: 0.58782, Val Loss: 0.58822\n",
      "Epoch: 199, Train: 0.8310, Val: 0.8300, Test: 0.8296, Loss: 0.58771, Val Loss: 0.58810\n",
      "Epoch: 200, Train: 0.8311, Val: 0.8301, Test: 0.8297, Loss: 0.58760, Val Loss: 0.58799\n",
      "Epoch: 201, Train: 0.8311, Val: 0.8302, Test: 0.8298, Loss: 0.58748, Val Loss: 0.58788\n",
      "Epoch: 202, Train: 0.8312, Val: 0.8302, Test: 0.8299, Loss: 0.58737, Val Loss: 0.58777\n",
      "Epoch: 203, Train: 0.8313, Val: 0.8303, Test: 0.8300, Loss: 0.58726, Val Loss: 0.58766\n",
      "Epoch: 204, Train: 0.8314, Val: 0.8304, Test: 0.8301, Loss: 0.58716, Val Loss: 0.58755\n",
      "Epoch: 205, Train: 0.8315, Val: 0.8305, Test: 0.8301, Loss: 0.58705, Val Loss: 0.58745\n",
      "Epoch: 206, Train: 0.8315, Val: 0.8306, Test: 0.8302, Loss: 0.58694, Val Loss: 0.58734\n",
      "Epoch: 207, Train: 0.8316, Val: 0.8307, Test: 0.8303, Loss: 0.58684, Val Loss: 0.58724\n",
      "Epoch: 208, Train: 0.8317, Val: 0.8307, Test: 0.8304, Loss: 0.58674, Val Loss: 0.58713\n",
      "Epoch: 209, Train: 0.8318, Val: 0.8308, Test: 0.8305, Loss: 0.58664, Val Loss: 0.58703\n",
      "Epoch: 210, Train: 0.8318, Val: 0.8309, Test: 0.8305, Loss: 0.58654, Val Loss: 0.58693\n",
      "Epoch: 211, Train: 0.8319, Val: 0.8310, Test: 0.8306, Loss: 0.58644, Val Loss: 0.58684\n",
      "Epoch: 212, Train: 0.8320, Val: 0.8310, Test: 0.8307, Loss: 0.58634, Val Loss: 0.58674\n",
      "Epoch: 213, Train: 0.8321, Val: 0.8311, Test: 0.8308, Loss: 0.58624, Val Loss: 0.58664\n",
      "Epoch: 214, Train: 0.8321, Val: 0.8312, Test: 0.8308, Loss: 0.58615, Val Loss: 0.58655\n",
      "Epoch: 215, Train: 0.8322, Val: 0.8313, Test: 0.8309, Loss: 0.58605, Val Loss: 0.58645\n",
      "Epoch: 216, Train: 0.8323, Val: 0.8313, Test: 0.8310, Loss: 0.58596, Val Loss: 0.58636\n",
      "Epoch: 217, Train: 0.8323, Val: 0.8314, Test: 0.8310, Loss: 0.58587, Val Loss: 0.58627\n",
      "Epoch: 218, Train: 0.8324, Val: 0.8315, Test: 0.8311, Loss: 0.58578, Val Loss: 0.58618\n",
      "Epoch: 219, Train: 0.8325, Val: 0.8315, Test: 0.8312, Loss: 0.58569, Val Loss: 0.58609\n",
      "Epoch: 220, Train: 0.8325, Val: 0.8316, Test: 0.8312, Loss: 0.58560, Val Loss: 0.58600\n",
      "Epoch: 221, Train: 0.8326, Val: 0.8317, Test: 0.8313, Loss: 0.58551, Val Loss: 0.58591\n",
      "Epoch: 222, Train: 0.8327, Val: 0.8317, Test: 0.8314, Loss: 0.58543, Val Loss: 0.58583\n",
      "Epoch: 223, Train: 0.8327, Val: 0.8318, Test: 0.8314, Loss: 0.58534, Val Loss: 0.58574\n",
      "Epoch: 224, Train: 0.8328, Val: 0.8319, Test: 0.8315, Loss: 0.58526, Val Loss: 0.58566\n",
      "Epoch: 225, Train: 0.8329, Val: 0.8319, Test: 0.8316, Loss: 0.58517, Val Loss: 0.58557\n",
      "Epoch: 226, Train: 0.8329, Val: 0.8320, Test: 0.8316, Loss: 0.58509, Val Loss: 0.58549\n",
      "Epoch: 227, Train: 0.8330, Val: 0.8320, Test: 0.8317, Loss: 0.58501, Val Loss: 0.58541\n",
      "Epoch: 228, Train: 0.8330, Val: 0.8321, Test: 0.8317, Loss: 0.58493, Val Loss: 0.58533\n",
      "Epoch: 229, Train: 0.8331, Val: 0.8322, Test: 0.8318, Loss: 0.58485, Val Loss: 0.58525\n",
      "Epoch: 230, Train: 0.8332, Val: 0.8322, Test: 0.8319, Loss: 0.58477, Val Loss: 0.58517\n",
      "Epoch: 231, Train: 0.8332, Val: 0.8323, Test: 0.8319, Loss: 0.58469, Val Loss: 0.58510\n",
      "Epoch: 232, Train: 0.8333, Val: 0.8323, Test: 0.8320, Loss: 0.58462, Val Loss: 0.58502\n",
      "Epoch: 233, Train: 0.8333, Val: 0.8324, Test: 0.8320, Loss: 0.58454, Val Loss: 0.58494\n",
      "Epoch: 234, Train: 0.8334, Val: 0.8325, Test: 0.8321, Loss: 0.58447, Val Loss: 0.58487\n",
      "Epoch: 235, Train: 0.8334, Val: 0.8325, Test: 0.8321, Loss: 0.58439, Val Loss: 0.58480\n",
      "Epoch: 236, Train: 0.8335, Val: 0.8326, Test: 0.8322, Loss: 0.58432, Val Loss: 0.58472\n",
      "Epoch: 237, Train: 0.8336, Val: 0.8326, Test: 0.8323, Loss: 0.58425, Val Loss: 0.58465\n",
      "Epoch: 238, Train: 0.8336, Val: 0.8327, Test: 0.8323, Loss: 0.58418, Val Loss: 0.58458\n",
      "Epoch: 239, Train: 0.8337, Val: 0.8327, Test: 0.8324, Loss: 0.58411, Val Loss: 0.58451\n",
      "Epoch: 240, Train: 0.8337, Val: 0.8328, Test: 0.8324, Loss: 0.58404, Val Loss: 0.58444\n",
      "Epoch: 241, Train: 0.8338, Val: 0.8328, Test: 0.8325, Loss: 0.58397, Val Loss: 0.58437\n",
      "Epoch: 242, Train: 0.8338, Val: 0.8329, Test: 0.8325, Loss: 0.58390, Val Loss: 0.58430\n",
      "Epoch: 243, Train: 0.8339, Val: 0.8329, Test: 0.8326, Loss: 0.58383, Val Loss: 0.58423\n",
      "Epoch: 244, Train: 0.8339, Val: 0.8330, Test: 0.8326, Loss: 0.58377, Val Loss: 0.58417\n",
      "Epoch: 245, Train: 0.8340, Val: 0.8330, Test: 0.8327, Loss: 0.58370, Val Loss: 0.58410\n",
      "Epoch: 246, Train: 0.8340, Val: 0.8331, Test: 0.8327, Loss: 0.58363, Val Loss: 0.58404\n",
      "Epoch: 247, Train: 0.8341, Val: 0.8331, Test: 0.8328, Loss: 0.58357, Val Loss: 0.58397\n",
      "Epoch: 248, Train: 0.8341, Val: 0.8332, Test: 0.8328, Loss: 0.58351, Val Loss: 0.58391\n",
      "Epoch: 249, Train: 0.8342, Val: 0.8332, Test: 0.8329, Loss: 0.58344, Val Loss: 0.58385\n",
      "Epoch: 250, Train: 0.8342, Val: 0.8333, Test: 0.8329, Loss: 0.58338, Val Loss: 0.58379\n",
      "Epoch: 251, Train: 0.8342, Val: 0.8333, Test: 0.8329, Loss: 0.58332, Val Loss: 0.58372\n",
      "Epoch: 252, Train: 0.8343, Val: 0.8334, Test: 0.8330, Loss: 0.58326, Val Loss: 0.58366\n",
      "Epoch: 253, Train: 0.8343, Val: 0.8334, Test: 0.8330, Loss: 0.58320, Val Loss: 0.58360\n",
      "Epoch: 254, Train: 0.8344, Val: 0.8334, Test: 0.8331, Loss: 0.58314, Val Loss: 0.58354\n",
      "Epoch: 255, Train: 0.8344, Val: 0.8335, Test: 0.8331, Loss: 0.58308, Val Loss: 0.58349\n",
      "Epoch: 256, Train: 0.8345, Val: 0.8335, Test: 0.8332, Loss: 0.58302, Val Loss: 0.58343\n",
      "Epoch: 257, Train: 0.8345, Val: 0.8336, Test: 0.8332, Loss: 0.58297, Val Loss: 0.58337\n",
      "Epoch: 258, Train: 0.8345, Val: 0.8336, Test: 0.8333, Loss: 0.58291, Val Loss: 0.58331\n",
      "Epoch: 259, Train: 0.8346, Val: 0.8337, Test: 0.8333, Loss: 0.58285, Val Loss: 0.58326\n",
      "Epoch: 260, Train: 0.8346, Val: 0.8337, Test: 0.8333, Loss: 0.58280, Val Loss: 0.58320\n",
      "Epoch: 261, Train: 0.8347, Val: 0.8337, Test: 0.8334, Loss: 0.58274, Val Loss: 0.58315\n",
      "Epoch: 262, Train: 0.8347, Val: 0.8338, Test: 0.8334, Loss: 0.58269, Val Loss: 0.58309\n",
      "Epoch: 263, Train: 0.8347, Val: 0.8338, Test: 0.8335, Loss: 0.58264, Val Loss: 0.58304\n",
      "Epoch: 264, Train: 0.8348, Val: 0.8339, Test: 0.8335, Loss: 0.58258, Val Loss: 0.58299\n",
      "Epoch: 265, Train: 0.8348, Val: 0.8339, Test: 0.8335, Loss: 0.58253, Val Loss: 0.58294\n",
      "Epoch: 266, Train: 0.8349, Val: 0.8339, Test: 0.8336, Loss: 0.58248, Val Loss: 0.58288\n",
      "Epoch: 267, Train: 0.8349, Val: 0.8340, Test: 0.8336, Loss: 0.58243, Val Loss: 0.58283\n",
      "Epoch: 268, Train: 0.8349, Val: 0.8340, Test: 0.8336, Loss: 0.58238, Val Loss: 0.58278\n",
      "Epoch: 269, Train: 0.8350, Val: 0.8341, Test: 0.8337, Loss: 0.58233, Val Loss: 0.58273\n",
      "Epoch: 270, Train: 0.8350, Val: 0.8341, Test: 0.8337, Loss: 0.58228, Val Loss: 0.58268\n",
      "Epoch: 271, Train: 0.8350, Val: 0.8341, Test: 0.8338, Loss: 0.58223, Val Loss: 0.58263\n",
      "Epoch: 272, Train: 0.8351, Val: 0.8342, Test: 0.8338, Loss: 0.58218, Val Loss: 0.58259\n",
      "Epoch: 273, Train: 0.8351, Val: 0.8342, Test: 0.8338, Loss: 0.58213, Val Loss: 0.58254\n",
      "Epoch: 274, Train: 0.8352, Val: 0.8342, Test: 0.8339, Loss: 0.58209, Val Loss: 0.58249\n",
      "Epoch: 275, Train: 0.8352, Val: 0.8343, Test: 0.8339, Loss: 0.58204, Val Loss: 0.58244\n",
      "Epoch: 276, Train: 0.8352, Val: 0.8343, Test: 0.8339, Loss: 0.58199, Val Loss: 0.58240\n",
      "Epoch: 277, Train: 0.8353, Val: 0.8343, Test: 0.8340, Loss: 0.58195, Val Loss: 0.58235\n",
      "Epoch: 278, Train: 0.8353, Val: 0.8344, Test: 0.8340, Loss: 0.58190, Val Loss: 0.58231\n",
      "Epoch: 279, Train: 0.8353, Val: 0.8344, Test: 0.8340, Loss: 0.58186, Val Loss: 0.58226\n",
      "Epoch: 280, Train: 0.8354, Val: 0.8344, Test: 0.8341, Loss: 0.58181, Val Loss: 0.58222\n",
      "Epoch: 281, Train: 0.8354, Val: 0.8345, Test: 0.8341, Loss: 0.58177, Val Loss: 0.58217\n",
      "Epoch: 282, Train: 0.8354, Val: 0.8345, Test: 0.8341, Loss: 0.58172, Val Loss: 0.58213\n",
      "Epoch: 283, Train: 0.8354, Val: 0.8345, Test: 0.8342, Loss: 0.58168, Val Loss: 0.58209\n",
      "Epoch: 284, Train: 0.8355, Val: 0.8346, Test: 0.8342, Loss: 0.58164, Val Loss: 0.58204\n",
      "Epoch: 285, Train: 0.8355, Val: 0.8346, Test: 0.8342, Loss: 0.58160, Val Loss: 0.58200\n",
      "Epoch: 286, Train: 0.8355, Val: 0.8346, Test: 0.8343, Loss: 0.58156, Val Loss: 0.58196\n",
      "Epoch: 287, Train: 0.8356, Val: 0.8347, Test: 0.8343, Loss: 0.58151, Val Loss: 0.58192\n",
      "Epoch: 288, Train: 0.8356, Val: 0.8347, Test: 0.8343, Loss: 0.58147, Val Loss: 0.58188\n",
      "Epoch: 289, Train: 0.8356, Val: 0.8347, Test: 0.8343, Loss: 0.58143, Val Loss: 0.58184\n",
      "Epoch: 290, Train: 0.8357, Val: 0.8348, Test: 0.8344, Loss: 0.58139, Val Loss: 0.58180\n",
      "Epoch: 291, Train: 0.8357, Val: 0.8348, Test: 0.8344, Loss: 0.58136, Val Loss: 0.58176\n",
      "Epoch: 292, Train: 0.8357, Val: 0.8348, Test: 0.8344, Loss: 0.58132, Val Loss: 0.58172\n",
      "Epoch: 293, Train: 0.8357, Val: 0.8348, Test: 0.8345, Loss: 0.58128, Val Loss: 0.58168\n",
      "Epoch: 294, Train: 0.8358, Val: 0.8349, Test: 0.8345, Loss: 0.58124, Val Loss: 0.58164\n",
      "Epoch: 295, Train: 0.8358, Val: 0.8349, Test: 0.8345, Loss: 0.58120, Val Loss: 0.58161\n",
      "Epoch: 296, Train: 0.8358, Val: 0.8349, Test: 0.8345, Loss: 0.58116, Val Loss: 0.58157\n",
      "Epoch: 297, Train: 0.8359, Val: 0.8350, Test: 0.8346, Loss: 0.58113, Val Loss: 0.58153\n",
      "Epoch: 298, Train: 0.8359, Val: 0.8350, Test: 0.8346, Loss: 0.58109, Val Loss: 0.58150\n",
      "Epoch: 299, Train: 0.8359, Val: 0.8350, Test: 0.8346, Loss: 0.58106, Val Loss: 0.58146\n",
      "Epoch: 300, Train: 0.8359, Val: 0.8350, Test: 0.8347, Loss: 0.58102, Val Loss: 0.58142\n",
      "Train: 0.8359, Val: 0.8350, Test: 0.8347, Train Loss: 0.58102, Val Loss: 0.58142, Test Loss: 0.58160\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(datasets['train'], args['num_layers'], emb_size=args['emb_size']).to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "best_model = train(model, optimizer, args)\n",
    "log = \"Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Train Loss: {:.5f}, Val Loss: {:.5f}, Test Loss: {:.5f}\"\n",
    "best_train_roc, train_loss = test(best_model, 'train', args)\n",
    "best_val_roc, val_loss = test(best_model, 'val', args)\n",
    "best_test_roc, test_loss = test(best_model, 'test', args)\n",
    "print(log.format(best_train_roc, best_val_roc, best_test_roc, train_loss, val_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "13af95d3-465c-481c-bdcc-bc5dcd24ce4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = datasets['test']\n",
    "pred = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a027f0af-1dc9-4ea0-93ce-6be874cd4a94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(G=[], edge_index=[2, 3263754], edge_label=[725284], edge_label_index=[2, 725284], negative_label_val=[1], node_label_index=[38565])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "20500072-4fcc-4ddd-a8c2-7f6759a36a54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ..., 36464, 37910, 35325],\n",
       "        [ 1171,  1105,  1187,  ...,   999,   999,   999]], device='cuda:0')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "12d50dc2-3893-4b71-ba8f-2b21f1eacf87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2a3155d1-c182-414c-a7e0-2d18bdefe422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  366,   242,   690,  ...,  5703, 10719, 27465],\n",
       "        [17083, 13053, 28496,  ..., 19909, 17538,  5413]], device='cuda:0')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d8927d12-62f7-4ea8-b5de-acc651f29f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(G=[], edge_index=[2, 3263754], edge_label=[725284], edge_label_index=[2, 725284], negative_label_val=[1], node_label_index=[38565])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7e273922-c2d4-46fd-be12-edba36b8fc81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deepsnap.graph.Graph"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "28395097-18d4-4194-8eff-3412c9513cef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  366,   242,   690,  ...,  5703, 10719, 27465],\n",
       "        [17083, 13053, 28496,  ..., 19909, 17538,  5413]], device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cf06e0ad-076a-4fb0-aa5f-1104ca357558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(losses, title):\n",
    "    train_loss, val_loss = zip(*losses)\n",
    "    steps = list(range(1, len(train_loss) + 1))\n",
    "    \n",
    "    min_val_loss = np.round(np.min(val_loss), 3)\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(steps, train_loss, '-r', label='Training Loss')\n",
    "    plt.plot(steps, val_loss, '-b', label='Validation Loss')\n",
    "    plt.hlines(min_val_loss, 1, 300, colors='k', linestyles='dotted', label='Min Validation Loss: {}'.format(min_val_loss))\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim((0.58, 0.71))\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fb1bc6a1-38ca-49d8-9ecc-1d9be4dd8d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(losses, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4dfc4-cc06-41fd-a294-bf2cf2690035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
