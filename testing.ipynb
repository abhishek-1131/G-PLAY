{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "089916e0-d3a9-46d6-a1b0-f3d1d46c4728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], device='cuda:0')\n",
      "2.1.0\n",
      "graph-tool version: 2.46\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(torch.zeros(1).cuda())\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "print(torch_geometric.__version__)\n",
    "\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType, OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "import networkx as nx\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import deepsnap\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "from pathlib import Path as Data_Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import json\n",
    "print(\"graph-tool version: {}\".format(gt.__version__.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "33b44dcc-a8f9-42ad-8d4c-bf6bb5459424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../graph_pt_100.pickle\", \"rb\") as f:\n",
    "    final_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "c1d4dac8-bd7d-448d-a997-38828f47670f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 3644438\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices:\", final_graph.num_vertices())\n",
    "print(\"Number of edges:\", final_graph.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "82ce8b6a-52c4-4ea4-8abc-9ddf8076e1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without artists as nodes:\n",
      "Number of vertices: 3644438\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print('without artists as nodes:')\n",
    "print(\"Number of vertices:\", final_graph.num_vertices())\n",
    "print(\"Number of edges:\", final_graph.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "2a1d7fb2-5821-48d9-9f8d-2bff99d281ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count_pl = sum(1 for v in final_graph.vertices() if final_graph.vp.pl_type[v] == \"playlist\")\n",
    "# print(f'num_playlists: {count_pl}')\n",
    "# count_tr = sum(1 for v in final_graph.vertices() if final_graph.vp.tr_type[v] == \"track\")\n",
    "# print(f'num_tracks: {count_tr}')\n",
    "# count_ar = sum(1 for v in final_graph.vertices() if final_graph.vp.ar_type[v] == \"artist\")\n",
    "# print(f'num_artist: {count_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "b3ff9dcb-d4d0-4d0e-96fd-87d382133bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without artists as nodes:\n",
      "num_playlists: 102000\n",
      "num_tracks: 3542438\n",
      "num_artist: 0\n"
     ]
    }
   ],
   "source": [
    "print('without artists as nodes:')\n",
    "count_pl = sum(1 for v in final_graph.vertices() if final_graph.vp.pl_type[v] == \"playlist\")\n",
    "print(f'num_playlists: {count_pl}')\n",
    "count_tr = sum(1 for v in final_graph.vertices() if final_graph.vp.tr_type[v] == \"track\")\n",
    "print(f'num_tracks: {count_tr}')\n",
    "count_ar = sum(1 for v in final_graph.vertices() if final_graph.vp.ar_type[v] == \"artist\")\n",
    "print(f'num_artist: {count_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "f07f7199-da31-4214-b161-fb78c4bd3fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Removal of duplicate nodes from pickle file\n",
    "\n",
    "\n",
    "# prop_name = \"pl_type\"\n",
    "\n",
    "# # Create a dictionary of property values to vertex IDs\n",
    "# vertex_dict = {}\n",
    "# for v in final_graph.vertices():\n",
    "#     value = final_graph.vp[prop_name][v]\n",
    "#     if value not in vertex_dict:\n",
    "#         vertex_dict[value] = [int(v)]\n",
    "#     else:\n",
    "#         vertex_dict[value].append(int(v))\n",
    "\n",
    "# # Remove duplicate nodes\n",
    "# for value, ids in vertex_dict.items():\n",
    "#     if len(ids) > 1:\n",
    "#         print(value,ids)\n",
    "#         # # Merge the duplicate nodes into the first node\n",
    "#         # first_id = ids[0]\n",
    "#         # for other_id in ids[1:]:\n",
    "#         #     final_graph.vertex(first_id).out_edges()[:] = gt.find_edge(final_graph, final_graph.vertex(first_id), final_graph.vertex(other_id))\n",
    "#         #     final_graph.vertex(first_id).out_edges()[:] = gt.find_edge(final_graph, final_graph.vertex(other_id), final_graph.vertex(first_id))\n",
    "#         #     final_graph.remove_vertex(final_graph.vertex(other_id))\n",
    "\n",
    "\n",
    "# print(\"Number of vertices:\", final_graph.num_vertices())\n",
    "# print(\"Number of edges:\", final_graph.num_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc727a-3d43-464c-80cc-a632d1734eaf",
   "metadata": {},
   "source": [
    "### Largest component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c0c05-68c6-472b-bc4a-82b34f522814",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Graph-tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "cbdef74b-ab91-42cd-a3a8-c6417616e4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "largest_comp = gt.extract_largest_component(final_graph)\n",
    "# largest_comp = gt.GraphView(final_graph, vfilt = gt.label_largest_component(final_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "e55c0969-d7e1-4f03-8944-c69bc2d3c082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no artists:\n",
      "Number of vertices: 38565\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print('no artists:')\n",
    "print(\"Number of vertices:\", largest_comp.num_vertices()) \n",
    "print(\"Number of edges:\", largest_comp.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "f262e497-bab7-4459-b4c6-ffbd4d889e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no artists stats:\n",
      "----------------\n",
      "num_playlists: 2999\n",
      "num_tracks: 35566\n",
      "num_artist: 0\n"
     ]
    }
   ],
   "source": [
    "print('no artists stats:')\n",
    "print('----------------')\n",
    "count_pl = sum(1 for v in largest_comp.vertices() if largest_comp.vp.pl_type[v] == \"playlist\")\n",
    "print(f'num_playlists: {count_pl}')\n",
    "count_tr = sum(1 for v in largest_comp.vertices() if largest_comp.vp.tr_type[v] == \"track\")\n",
    "print(f'num_tracks: {count_tr}')\n",
    "count_ar = sum(1 for v in largest_comp.vertices() if largest_comp.vp.ar_type[v] == \"artist\")\n",
    "print(f'num_artist: {count_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "0c1e9154-e09f-436d-95b9-cd91d9a37bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #### Networkx\n",
    "# nx_graph = nx.Graph()\n",
    "# for node in final_graph.vertices():\n",
    "#     nx_graph.add_node(int(node))\n",
    "# for edge in final_graph.edges():\n",
    "#     nx_graph.add_edge(int(edge.source()), int(edge.target()))\n",
    "\n",
    "# from networkx.algorithms.components import is_connected\n",
    "# is_connected(nx_graph)\n",
    "\n",
    "# largest_cc = max(nx.connected_components(nx_graph), key=len)\n",
    "# nx_largest_comp = nx.Graph(nx_graph.subgraph(largest_cc))\n",
    "# print('Num nodes:', nx_largest_comp.number_of_nodes(), '. Num edges:', nx_largest_comp.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30978f4-29e7-4fb8-91b1-f435007437a2",
   "metadata": {},
   "source": [
    "### N-hop neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "590c5c22-2819-4ae6-88cc-7b97ae1ed7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b137d90-07e9-4816-be08-3370a8a33e2a",
   "metadata": {},
   "source": [
    "### Deepsnap"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7bcca46-a0aa-48b9-a122-882b90e0c151",
   "metadata": {
    "tags": []
   },
   "source": [
    "g_nx = nx.Graph()\n",
    "\n",
    "# Add edges to NetworkX graph in batches\n",
    "nx_g = nx.Graph()\n",
    "for v in largest_comp.vertices():\n",
    "    if(largest_comp.vp.pl_type[v]=='playlist'):\n",
    "        typ='playlist'\n",
    "    elif(largest_comp.vp.tr_type[v] == \"track\"):\n",
    "        typ='track'\n",
    "    node_attributes = {'name':largest_comp.vp.pl_name[v], 'uri':largest_comp.vp.tr_uri[v], 'type':typ}\n",
    "    g_nx.add_node(int(v), **node_attributes)\n",
    "for e in largest_comp.edges():\n",
    "    g_nx.add_edge(int(e.source()), int(e.target()))\n",
    "    \n",
    "mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(g_nx.nodes()))}\n",
    "\n",
    "# reindex the nodes in the graph\n",
    "g_nx = nx.relabel_nodes(g_nx, mapping)\n",
    "        \n",
    "# Create a DeepSNAP graph from NetworkX graph\n",
    "ds_graph = Graph(g_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "9dfdfc3d-2c82-4cd1-bdf2-d29916f54315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Awesome Playlist', 'uri': ''}\n"
     ]
    }
   ],
   "source": [
    "print(g_nx.nodes[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8392c6-9ca4-4c19-b7c3-e1e144a8e27b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(560, 23579), (560, 23580), (560, 23581), (560, 23582), (560, 23583), (560, 23584), (560, 23585), (560, 23586), (560, 23587), (560, 23588), (560, 23589), (560, 23590), (560, 23591), (560, 23592), (560, 23593), (560, 23594), (560, 23595), (560, 23596), (560, 23597), (560, 23598), (560, 23599), (560, 23600), (560, 24627), (560, 24628), (560, 24629), (560, 24630), (560, 24631), (560, 24632), (560, 24633), (560, 24634), (560, 24635), (560, 24636), (560, 24637), (560, 24638), (560, 24639), (560, 24640), (560, 24641), (560, 24642), (560, 24643), (560, 24644), (560, 24645), (560, 24646), (560, 24647), (560, 24648), (560, 24649), (560, 24650), (560, 24651), (560, 24652), (560, 24653), (560, 24654), (560, 24655), (560, 24656), (560, 24657), (560, 24658), (560, 24659), (560, 24660), (560, 24661), (560, 24662), (560, 24663), (560, 24664), (560, 24665), (560, 24666), (560, 24667), (560, 24668), (560, 24669), (560, 24670), (560, 24671), (560, 24672), (560, 24673), (560, 24674), (560, 24675), (560, 24676), (560, 24677), (560, 24678), (560, 24679), (560, 24565), (560, 24566), (560, 24567), (560, 24568), (560, 24569), (560, 24570), (560, 23811), (560, 23812), (560, 23813), (560, 23814), (560, 23815), (560, 23816), (560, 23817), (560, 23818), (560, 23819), (560, 23820), (560, 23821), (560, 23822), (560, 23823), (560, 23824), (560, 23825), (560, 23826), (560, 23827), (560, 23828), (560, 23829), (560, 23830), (560, 23831), (560, 23832), (560, 23833), (560, 23834), (560, 23835), (560, 23836), (560, 23837), (560, 23838), (560, 23839), (560, 23840), (560, 23841), (560, 23842), (560, 23843), (560, 23844), (560, 23845), (560, 23846), (560, 23847), (560, 23848), (560, 23849), (560, 23850), (560, 23851), (560, 23852), (560, 23853), (560, 23854), (560, 23855), (560, 23856), (560, 23857), (560, 23858), (560, 23859), (560, 23860), (560, 23861), (560, 23862), (560, 24680), (560, 24681), (560, 24682), (560, 24683), (560, 24684), (560, 24685), (560, 24686), (560, 24687), (560, 24688), (560, 24689), (560, 24690), (560, 24691), (560, 24692), (560, 24693), (560, 24694), (560, 24695), (560, 24696), (560, 24697), (560, 24698), (560, 24699), (560, 24700), (560, 24701), (560, 24702), (560, 24703), (560, 24704), (560, 24705), (560, 24706), (560, 24752), (560, 24753), (560, 24754), (560, 24755), (560, 24756), (560, 24757), (560, 24758), (560, 24759), (560, 24760), (560, 24761), (560, 24762), (560, 24763), (560, 24764), (560, 24765), (560, 24766), (560, 24767), (560, 24768), (560, 24769), (560, 24770), (560, 24771), (560, 24772), (560, 24773), (560, 24774), (560, 24775), (560, 24776), (560, 24777), (560, 24778), (560, 24779), (560, 24780), (560, 24781), (560, 24782), (560, 24783), (560, 24784), (560, 24785), (560, 24786), (560, 24787), (560, 24788), (560, 24789), (560, 24790), (560, 24791), (560, 24792), (560, 24793), (560, 24794), (560, 24795), (560, 24796), (560, 24797), (560, 22871), (560, 22872), (560, 22873), (560, 22874), (560, 22875), (560, 22876), (560, 23134), (560, 23135), (560, 23136), (560, 23137), (560, 23138), (560, 23139), (560, 23140), (560, 23141), (560, 23142), (560, 23143), (560, 23144), (560, 23145), (560, 23146), (560, 23147), (560, 23148), (560, 23149), (560, 23150), (560, 23151), (560, 23152), (560, 23153), (560, 23154), (560, 23155), (560, 23156), (560, 23157), (560, 23158), (560, 23159), (560, 23160), (560, 23161), (560, 23162), (560, 23163), (560, 23164), (560, 23165), (560, 23166), (560, 23167), (560, 23168), (560, 23169), (560, 23170), (560, 23171), (560, 23172), (560, 23173), (560, 23174), (560, 23175), (560, 24860), (560, 24861), (560, 24862), (560, 24863), (560, 24864), (560, 24865), (560, 24866), (560, 24867), (560, 24868), (560, 22578), (560, 22579), (560, 22580), (560, 22581), (560, 22582), (560, 22583), (560, 22584), (560, 22585), (560, 22586), (560, 22587), (560, 22588), (560, 22589), (560, 22590), (560, 22591), (560, 22592), (560, 22593), (560, 22594), (560, 22595), (560, 22596), (560, 22597), (560, 22598), (560, 22599), (560, 22600), (560, 22601), (560, 22602), (560, 22603), (560, 22604), (560, 22605), (560, 22606), (560, 22607), (560, 22608), (560, 22609), (560, 22610), (560, 22611), (560, 22612), (560, 22613), (560, 22614), (560, 22615), (560, 22616), (560, 22617), (560, 22618), (560, 22619), (560, 22620), (560, 22621), (560, 22622), (560, 22445), (560, 22446), (560, 22447), (560, 22448), (560, 22449), (560, 23881), (560, 23882), (560, 23883), (560, 23884), (560, 23885), (560, 23886), (560, 23887), (560, 23888), (560, 23889), (560, 23890), (560, 23891), (560, 23892), (560, 23893), (560, 23894), (560, 23895), (560, 23896), (560, 23897), (560, 23898), (560, 23899), (560, 23900), (560, 23901), (560, 23902), (560, 23903), (560, 23904), (560, 23905), (560, 23906), (560, 23907), (560, 23908), (560, 23909), (560, 23910), (560, 23911), (560, 23912), (560, 23913), (560, 23914), (560, 23915), (560, 23916), (560, 23917), (560, 23918), (560, 23919), (560, 23920), (560, 23921), (560, 23922), (560, 23923), (560, 23924), (560, 23925), (560, 23926), (560, 23927), (560, 23928), (560, 23929), (560, 23930), (560, 23931), (560, 23932), (560, 23933), (560, 23934), (560, 23935), (560, 23936), (560, 23937), (560, 23938), (560, 23939), (560, 23940), (560, 23941), (560, 23942), (560, 23943), (560, 23944), (560, 23945), (560, 23946), (560, 23947), (560, 23948), (560, 23949), (560, 23950), (560, 23951), (560, 24523), (560, 24524), (560, 24525), (560, 24526), (560, 24527), (560, 24528), (560, 24529), (560, 24530), (560, 24531), (560, 24532), (560, 24533), (560, 24534), (560, 24535), (560, 24536), (560, 24537), (560, 24538), (560, 24539), (560, 23688), (560, 23689), (560, 23690), (560, 23691), (560, 23692), (560, 23693), (560, 23694), (560, 23695), (560, 23696), (560, 23697), (560, 23698), (560, 23699), (560, 23700), (560, 23701), (560, 23702), (560, 23703), (560, 23704), (560, 23705), (560, 23706), (560, 23707), (560, 23708), (560, 23709), (560, 23710), (560, 23711), (560, 23712), (560, 23713), (560, 23714), (560, 23715), (560, 23716), (560, 23717), (560, 23718), (560, 23719), (560, 23720), (560, 23721), (560, 23722), (560, 23723), (560, 23724), (560, 23725), (560, 23726), (560, 23727), (560, 23728), (560, 23729), (560, 23730), (560, 23731), (560, 23732), (560, 23733), (560, 23734), (560, 23735), (560, 23736), (560, 23737), (560, 23738), (560, 23739), (560, 23740), (560, 23741), (560, 23742), (560, 23743), (560, 23744), (560, 23745), (560, 23746), (560, 23747), (560, 23748), (560, 23749), (560, 23750), (560, 23751), (560, 23752), (560, 23753), (560, 23754), (560, 23755), (560, 23756), (560, 23757), (560, 23758), (560, 23759), (560, 23760), (560, 23761), (560, 23762), (560, 23763), (560, 23764), (560, 23765), (560, 23766), (560, 23767), (560, 23768), (560, 23769), (560, 23770), (560, 23771), (560, 23772), (560, 23773), (560, 23774), (560, 23775), (560, 23776), (560, 23777), (560, 23778), (560, 23779), (560, 23780), (560, 23781), (560, 23782), (560, 23783), (560, 23784), (560, 23785), (560, 23786), (560, 23787), (560, 23788), (560, 23789), (560, 23790), (560, 23791), (560, 23792), (560, 23793), (560, 23794), (560, 23795), (560, 23796), (560, 23797), (560, 23798), (560, 23799), (560, 23800), (560, 23801), (560, 23802), (560, 23803), (560, 23804), (560, 23805), (560, 23806), (560, 23807), (560, 23808), (560, 23809), (560, 23810), (560, 23961), (560, 23962), (560, 23963), (560, 24914), (560, 24915), (560, 24916), (560, 24917), (560, 24918), (560, 24919), (560, 24920), (560, 24921), (560, 24922), (560, 24923), (560, 24924), (560, 24925), (560, 24926), (560, 24927), (560, 24928), (560, 24929), (560, 24930), (560, 24931), (560, 24932), (560, 24933), (560, 24934), (560, 24935), (560, 24936), (560, 24937), (560, 24938), (560, 24939), (560, 24940), (560, 24941), (560, 24942), (560, 24943), (560, 24944), (560, 24945), (560, 24946), (560, 24947), (560, 24948), (560, 24949), (560, 24950), (560, 24951), (560, 24952), (560, 24953), (560, 24954), (560, 24955), (560, 24956), (560, 24957), (560, 24958), (560, 24959), (560, 24960), (560, 24961), (560, 24962), (560, 24963), (560, 24964), (560, 24965), (560, 24966), (560, 24967), (560, 24968), (560, 24969), (560, 24970), (560, 24971), (560, 24972), (560, 24973), (560, 24974), (560, 24975), (560, 24976), (560, 24977), (560, 24978), (560, 24979), (560, 24980), (560, 24981), (560, 24982), (560, 23981), (560, 23982), (560, 23983), (560, 23984), (560, 23985), (560, 23986), (560, 23987), (560, 23988), (560, 23989), (560, 23990), (560, 23991), (560, 23992), (560, 23993), (560, 23994), (560, 23995), (560, 23996), (560, 23997), (560, 23998), (560, 23999), (560, 24000), (560, 24001), (560, 24002), (560, 24003), (560, 24004), (560, 24005), (560, 24006), (560, 24007), (560, 24008), (560, 24009), (560, 24010), (560, 24011), (560, 24012), (560, 24013), (560, 24014), (560, 24015), (560, 24016), (560, 24017), (560, 24018), (560, 24019), (560, 24020), (560, 24021), (560, 24022), (560, 24023), (560, 24024), (560, 24025), (560, 24026), (560, 24027), (560, 24028), (560, 24328), (560, 24329), (560, 24330), (560, 24331), (560, 24332), (560, 24333), (560, 24798), (560, 24799), (560, 24800), (560, 24801), (560, 24802), (560, 24803), (560, 24804), (560, 24805), (560, 24806), (560, 24807), (560, 24808), (560, 24809), (560, 24810), (560, 24811), (560, 24812), (560, 24813), (560, 24814), (560, 24815), (560, 24816), (560, 24817), (560, 24818), (560, 24819), (560, 24820), (560, 24821), (560, 24822), (560, 24823), (560, 24824), (560, 24825), (560, 24826), (560, 24827), (560, 24828), (560, 24829), (560, 24830), (560, 24379), (560, 24380), (560, 24381), (560, 24382), (560, 24383), (560, 24384), (560, 24385), (560, 24386), (560, 24387), (560, 24388), (560, 24389), (560, 24390), (560, 24391), (560, 24392), (560, 24393), (560, 24394), (560, 24395), (560, 24396), (560, 24397), (560, 24398), (560, 24399), (560, 24400), (560, 24401), (560, 24402), (560, 24403), (560, 24404), (560, 24405), (560, 24406), (560, 24407), (560, 24408), (560, 24409), (560, 24410), (560, 24411), (560, 24412), (560, 24256), (560, 24257), (560, 24258), (560, 24259), (560, 24260), (560, 24261), (560, 24262), (560, 24263), (560, 24264), (560, 24265), (560, 24266), (560, 24267), (560, 24268), (560, 24269), (560, 24270), (560, 24271), (560, 24272), (560, 24273), (560, 24274), (560, 24275), (560, 24276), (560, 24277), (560, 24278), (560, 24279), (560, 24280), (560, 24281), (560, 24282), (560, 24283), (560, 24284), (560, 24285), (560, 24286), (560, 24287), (560, 24288), (560, 24289), (560, 24290), (560, 24291), (560, 24292), (560, 24293), (560, 24294), (560, 24295), (560, 24296), (560, 24297), (560, 24298), (560, 24299), (560, 24300), (560, 24301), (560, 24302), (560, 24303), (560, 24304), (560, 24305), (560, 24306), (560, 24307), (560, 24308), (560, 24309), (560, 24310), (560, 24311), (560, 24312), (560, 24313), (560, 24314), (560, 24315), (560, 24316), (560, 24317), (560, 24318), (560, 24319), (560, 23980), (560, 24996), (560, 24997), (560, 24998), (560, 24999), (560, 25000), (560, 25001), (560, 25002), (560, 25003), (560, 25004), (560, 25005), (560, 25006), (560, 23042), (560, 23043), (560, 23044), (560, 23045), (560, 23046), (560, 23047), (560, 23048), (560, 23049), (560, 23050), (560, 23051), (560, 23052), (560, 23053), (560, 23054), (560, 24562), (560, 24563), (560, 24564), (560, 24571), (560, 24572), (560, 24573), (560, 24555), (560, 24556), (560, 24557), (560, 24558), (560, 24559), (560, 24560), (560, 24561), (560, 24574), (560, 24575), (560, 24576), (560, 24577), (560, 24578), (560, 24579), (560, 24580), (560, 24581), (560, 24582), (560, 24583), (560, 24584), (560, 24585), (560, 24586), (560, 24587), (560, 24588), (560, 24589), (560, 24590), (560, 24591), (560, 24592), (560, 23952), (560, 23953), (560, 23954), (560, 23955), (560, 23956), (560, 23957), (560, 23958), (560, 23959), (560, 23960), (560, 23450), (560, 23451), (560, 23452), (560, 23453), (560, 23454), (560, 23455), (560, 23456), (560, 23457), (560, 23458), (560, 23459), (560, 23460), (560, 23461), (560, 23462), (560, 23463), (560, 23464), (560, 23465), (560, 23466), (560, 23467), (560, 23468), (560, 23469), (560, 23470), (560, 23471), (560, 23472), (560, 24239), (560, 24240), (560, 24241), (560, 24242), (560, 24243), (560, 24244), (560, 24245), (560, 24246), (560, 24247), (560, 24248), (560, 24249), (560, 24250), (560, 24251), (560, 24252), (560, 24253), (560, 22507), (560, 22508), (560, 22509), (560, 22510), (560, 22511), (560, 22512), (560, 22513), (560, 23379), (560, 23380), (560, 23381), (560, 23382), (560, 23383), (560, 23384), (560, 23385), (560, 23386), (560, 23387), (560, 23388), (560, 23389), (560, 23390), (560, 23404), (560, 23405), (560, 23406), (560, 23407), (560, 23408), (560, 23409), (560, 23425), (560, 23426), (560, 23427), (560, 23428), (560, 23429), (560, 23430), (560, 23431), (560, 23432), (560, 23433), (560, 23434), (560, 23435), (560, 23436), (560, 23437), (560, 23438), (560, 23439), (560, 23440), (560, 23441), (560, 23442), (560, 23443), (560, 23444), (560, 23445), (560, 23446), (560, 23447), (560, 23448), (560, 23449), (560, 22979), (560, 22980), (560, 22981), (560, 22982), (560, 22983), (560, 22984), (560, 22985), (560, 22986), (560, 22987), (560, 22988), (560, 22989), (560, 22990), (560, 22991), (560, 22992), (560, 22993), (560, 24378), (560, 23863), (560, 23864), (560, 23865), (560, 23866), (560, 23867), (560, 23868), (560, 23869), (560, 23870), (560, 23871), (560, 23872), (560, 23873), (560, 23874), (560, 23875), (560, 23876), (560, 23877), (560, 23878), (560, 23879), (560, 23880), (560, 23308), (560, 23309), (560, 23310), (560, 23311), (560, 23312), (560, 23313), (560, 23314), (560, 23315), (560, 23316), (560, 23317), (560, 23318), (560, 23319), (560, 23320), (560, 23321), (560, 23322), (560, 23323), (560, 23324), (560, 23325), (560, 23326), (560, 23327), (560, 23328), (560, 23329), (560, 23330), (560, 23331), (560, 23332), (560, 23333), (560, 23334), (560, 23335), (560, 23336), (560, 23337), (560, 23338), (560, 23339), (560, 23340), (560, 23341), (560, 23342), (560, 23343), (560, 23344), (560, 23971), (560, 23972), (560, 23973), (560, 23974), (560, 23975), (560, 23976), (560, 23977), (560, 23978), (560, 23979), (560, 23649), (560, 23650), (560, 23651), (560, 23652), (560, 23653), (560, 23654), (560, 23655), (560, 23656), (560, 23657), (560, 23658), (560, 22739), (560, 22740), (560, 22741), (560, 22742), (560, 22743), (560, 22744), (560, 22745), (560, 22746), (560, 22747), (560, 22748), (560, 22749), (560, 22750), (560, 22751), (560, 22752), (560, 22753), (560, 22754), (560, 22755), (560, 22756), (560, 22757), (560, 22758), (560, 22759), (560, 22760), (560, 22761), (560, 22762), (560, 22763), (560, 22764), (560, 22765), (560, 22766), (560, 22767), (560, 22768), (560, 24102), (560, 23119), (560, 23120), (560, 23121), (560, 23122), (560, 23123), (560, 23124), (560, 23125), (560, 23126), (560, 23127), (560, 23128), (560, 23129), (560, 23130), (560, 23131), (560, 23132), (560, 23133), (560, 23562), (560, 23563), (560, 23564), (560, 23565), (560, 23566), (560, 23567), (560, 23568), (560, 23569), (560, 23570), (560, 23571), (560, 23572), (560, 23573), (560, 23574), (560, 23575), (560, 23576), (560, 23577), (560, 23578), (560, 23601), (560, 23602), (560, 23603), (560, 23604), (560, 23605), (560, 23606), (560, 23607), (560, 23608), (560, 23609), (560, 23610), (560, 23611), (560, 22338), (560, 22339), (560, 22340), (560, 22341), (560, 22342), (560, 22343), (560, 22344), (560, 22345), (560, 22346), (560, 22347), (560, 22348), (560, 22349), (560, 22350), (560, 22351), (560, 22352), (560, 22353), (560, 22354), (560, 22355), (560, 22356), (560, 22357), (560, 22358), (560, 22359), (560, 22360), (560, 22361), (560, 22362), (560, 22363), (560, 22364), (560, 22365), (560, 22366), (560, 22367), (560, 22368), (560, 22369), (560, 22370), (560, 22371), (560, 22372), (560, 22373), (560, 22374), (560, 22375), (560, 22376), (560, 22377), (560, 22378), (560, 22379), (560, 22380), (560, 22381), (560, 22382), (560, 22383), (560, 22384), (560, 22385), (560, 22386), (560, 22387), (560, 22388), (560, 22389), (560, 22390), (560, 22391), (560, 22392), (560, 22393), (560, 22394), (560, 22395), (560, 22396), (560, 22397), (560, 22398), (560, 22399), (560, 22400), (560, 22401), (560, 22402), (560, 22403), (560, 22404), (560, 22405), (560, 22406), (560, 22407), (560, 22408), (560, 22409), (560, 22410), (560, 22411), (560, 22412), (560, 22413), (560, 22414), (560, 22415), (560, 22416), (560, 22417), (560, 22418), (560, 22419), (560, 22822), (560, 22823), (560, 22824), (560, 22825), (560, 22826), (560, 22827), (560, 22828), (560, 22829), (560, 22830), (560, 22831), (560, 22832), (560, 22833), (560, 22834), (560, 22835), (560, 22836), (560, 22837), (560, 22838), (560, 22839), (560, 22840), (560, 22841), (560, 22842), (560, 22843), (560, 22844), (560, 22845), (560, 22846), (560, 22847), (560, 22848), (560, 22849), (560, 22850), (560, 22851), (560, 22852), (560, 22853), (560, 22854), (560, 22855), (560, 22856), (560, 22857), (560, 22858), (560, 22859), (560, 22902), (560, 22903), (560, 22904), (560, 22905), (560, 22906), (560, 22907), (560, 22908), (560, 22909), (560, 22910), (560, 22911), (560, 22912), (560, 22913), (560, 22914), (560, 22915), (560, 22916), (560, 22917), (560, 22918), (560, 22919), (560, 22920), (560, 22921), (560, 22922), (560, 22923), (560, 22924), (560, 22925), (560, 22926), (560, 22927), (560, 22928), (560, 22929), (560, 22930), (560, 22931), (560, 22932), (560, 22933), (560, 22934), (560, 23480), (560, 23481), (560, 23482), (560, 23483), (560, 23484), (560, 23485), (560, 23486), (560, 23487), (560, 23351), (560, 23352), (560, 23353), (560, 23288), (560, 23289), (560, 23290), (560, 23291), (560, 23292), (560, 23293), (560, 23294), (560, 23295), (560, 23296), (560, 23297), (560, 23298), (560, 23299), (560, 23300), (560, 23301), (560, 23302), (560, 23303), (560, 23304), (560, 23305), (560, 23306), (560, 23307), (560, 25553), (560, 25554), (560, 25555), (560, 23640), (560, 23641), (560, 23642), (560, 23643), (560, 23644), (560, 23645), (560, 23646), (560, 23647), (560, 23648), (560, 25101), (560, 25102), (560, 25103), (560, 25104), (560, 25105), (560, 25106), (560, 25107), (560, 25108), (560, 25109), (560, 25110), (560, 25111), (560, 25112), (560, 25113), (560, 25114), (560, 25115), (560, 25116), (560, 25117), (560, 25118), (560, 25119), (560, 25120), (560, 25121), (560, 25122), (560, 25123), (560, 23417), (560, 23418), (560, 23419), (560, 23420), (560, 23421), (560, 23422), (560, 23423), (560, 23424), (560, 23473), (560, 23474), (560, 23475), (560, 23476), (560, 23477), (560, 23478), (560, 23479), (560, 23488), (560, 23489), (560, 23490), (560, 23491), (560, 23492), (560, 23493), (560, 23494), (560, 23495), (560, 23496), (560, 23497), (560, 23498), (560, 23499), (560, 23500), (560, 23501), (560, 23502), (560, 22420), (560, 22421), (560, 22422), (560, 22423), (560, 22424), (560, 22425), (560, 22426), (560, 22427), (560, 22428), (560, 22429), (560, 22430), (560, 22431), (560, 22432), (560, 22433), (560, 22434), (560, 22435), (560, 22436), (560, 22437), (560, 22438), (560, 22439), (560, 22440), (560, 22441), (560, 22442), (560, 22443), (560, 22444), (560, 22450), (560, 22451), (560, 22452), (560, 22453), (560, 22454), (560, 22455), (560, 22456), (560, 22457), (560, 22458), (560, 22459), (560, 22460), (560, 22461), (560, 22462), (560, 22463), (560, 22464), (560, 22465), (560, 22466), (560, 22467), (560, 22468), (560, 22469), (560, 22470), (560, 22471), (560, 22472), (560, 22473), (560, 22474), (560, 22475), (560, 22476), (560, 22477), (560, 22478), (560, 22479), (560, 22480), (560, 22481), (560, 22482), (560, 22483), (560, 22484), (560, 22485), (560, 22486), (560, 22487), (560, 22488), (560, 22489), (560, 22490), (560, 22491), (560, 22492), (560, 22493), (560, 22494), (560, 22495), (560, 22496), (560, 22497), (560, 22498), (560, 22499), (560, 22500), (560, 22501), (560, 22502), (560, 22503), (560, 22504), (560, 22505), (560, 22506), (560, 22514), (560, 22515), (560, 22516), (560, 22517), (560, 22518), (560, 22519), (560, 22520), (560, 22521), (560, 22522), (560, 22523), (560, 22524), (560, 22525), (560, 22526), (560, 22527), (560, 24545), (560, 24546), (560, 24547), (560, 24548), (560, 24549), (560, 24550), (560, 24551), (560, 24552), (560, 24553), (560, 24554), (560, 23364), (560, 23365), (560, 23366), (560, 23367), (560, 23368), (560, 23369), (560, 23370), (560, 23371), (560, 23372), (560, 23373), (560, 23374), (560, 23375), (560, 23376), (560, 23377), (560, 23378), (560, 23391), (560, 23392), (560, 23393), (560, 23394), (560, 23395), (560, 23396), (560, 23397), (560, 23398), (560, 23399), (560, 23400), (560, 23401), (560, 23402), (560, 23403), (560, 23410), (560, 23411), (560, 23412), (560, 23413), (560, 23414), (560, 23415), (560, 23416), (560, 23544), (560, 23545), (560, 23546), (560, 23547), (560, 23548), (560, 23549), (560, 23550), (560, 23551), (560, 23552), (560, 23553), (560, 23554), (560, 23555), (560, 23556), (560, 23557), (560, 23558), (560, 23559), (560, 23560), (560, 23561), (560, 23612), (560, 23613), (560, 23614), (560, 23615), (560, 23616), (560, 23617), (560, 23618), (560, 23619), (560, 23620), (560, 23621), (560, 23622), (560, 23623), (560, 23624), (560, 23625), (560, 23626), (560, 23627), (560, 23628), (560, 23629), (560, 23630), (560, 23631), (560, 23632), (560, 23633), (560, 23634), (560, 23635), (560, 23636), (560, 22807), (560, 22808), (560, 22809), (560, 22810), (560, 22811), (560, 22812), (560, 22813), (560, 22814), (560, 22815), (560, 22816), (560, 22817), (560, 22818), (560, 22819), (560, 22820), (560, 22821), (560, 24153), (560, 24154), (560, 24155), (560, 24156), (560, 24157), (560, 24158), (560, 24159), (560, 24160), (560, 24161), (560, 24162), (560, 24163), (560, 24164), (560, 24165), (560, 24166), (560, 24167), (560, 24168), (560, 24169), (560, 24170), (560, 24171), (560, 24172), (560, 24173), (560, 24174), (560, 24175), (560, 24176), (560, 24177), (560, 24178), (560, 24179), (560, 24180), (560, 24181), (560, 24182), (560, 24183), (560, 24184), (560, 24185), (560, 24186), (560, 24187), (560, 24188), (560, 24189), (560, 24190), (560, 24191), (560, 24192), (560, 24193), (560, 24194), (560, 24195), (560, 24196), (560, 24197), (560, 24198), (560, 24199), (560, 24200), (560, 24201), (560, 24202), (560, 24203), (560, 24204), (560, 24205), (560, 24206), (560, 24207), (560, 24208), (560, 24209), (560, 24210), (560, 24211), (560, 24212), (560, 24213), (560, 24214), (560, 24215), (560, 24216), (560, 24217), (560, 24218), (560, 24219), (560, 24220), (560, 24221), (560, 24222), (560, 24223), (560, 24224), (560, 24225), (560, 24226), (560, 24227), (560, 24228), (560, 24229), (560, 24230), (560, 24231), (560, 24232), (560, 24233), (560, 24234), (560, 24235), (560, 24236), (560, 24237), (560, 24238), (560, 24254), (560, 24255), (560, 22877), (560, 22878), (560, 22879), (560, 22880), (560, 22881), (560, 22882), (560, 22883), (560, 22884), (560, 22885), (560, 22886), (560, 22887), (560, 22888), (560, 22889), (560, 22890), (560, 22891), (560, 22892), (560, 22893), (560, 22894), (560, 22895), (560, 22896), (560, 22897), (560, 22898), (560, 22899), (560, 22900), (560, 22901), (560, 24540), (560, 24541), (560, 24542), (560, 24543), (560, 24544), (560, 22249), (560, 22250), (560, 22251), (560, 22252), (560, 23964), (560, 23965), (560, 23966), (560, 23967), (560, 23968), (560, 23969), (560, 23970), (560, 24449), (560, 24450), (560, 24451), (560, 24452), (560, 24453), (560, 24454), (560, 24455), (560, 24423), (560, 24424), (560, 24425), (560, 24426), (560, 24427), (560, 22962), (560, 22963), (560, 22964), (560, 22965), (560, 22966), (560, 22967), (560, 22968), (560, 22969), (560, 22970), (560, 22971), (560, 22972), (560, 22973), (560, 23222), (560, 23223), (560, 23224), (560, 23225), (560, 23226), (560, 23227), (560, 23228), (560, 23229), (560, 23230), (560, 23231), (560, 23232), (560, 23233), (560, 23234), (560, 23235), (560, 23236), (560, 23237), (560, 23238), (560, 23239), (560, 23240), (560, 23241), (560, 23242), (560, 23243), (560, 23244), (560, 23245), (560, 23246), (560, 23247), (560, 23248), (560, 23249), (560, 24456), (560, 24457), (560, 24458), (560, 24459), (560, 24460), (560, 24461), (560, 24462), (560, 24463), (560, 24464), (560, 24465), (560, 24466), (560, 24467), (560, 26066), (560, 26067), (560, 26068), (560, 26069), (560, 26070)]\n",
      "abby \n"
     ]
    }
   ],
   "source": [
    "print(g_nx.edges(560))\n",
    "print(largest_comp.vp.pl_name[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "ee3c05b5-8a28-4d3b-94d4-af5eb1a96032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices: 38565\n",
      "Number of edges: 6746550\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of vertices:\", largest_comp.num_vertices()) \n",
    "print(\"Number of edges:\", largest_comp.num_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "6b5773f4-4af4-49ea-ba46-781b4fa3aca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deepsnap.dataset.GraphDataset'>\n",
      "Graph(G=[], edge_index=[2, 2320892], edge_label=[1160448], edge_label_index=[2, 1160448], name=[38565], negative_label_val=[1], node_label_index=[38565], uri=[38565])\n",
      "<class 'deepsnap.graph.Graph'>\n",
      "Train set has 580224 supervision (positive) edges\n",
      "Validation set has 362638 supervision (positive) edges\n",
      "Test set has 362642 supervision (positive) edges\n",
      "Train set has 2320892 message passing edges\n",
      "Validation set has 2901116 message passing edges\n",
      "Test set has 3263754 message passing edges\n"
     ]
    }
   ],
   "source": [
    "task = 'link_pred'\n",
    "dataset = GraphDataset([ds_graph], task=task, edge_train_mode='disjoint')\n",
    "\n",
    "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "# dataset_train[0].to('cuda')\n",
    "# dataset_val[0].to('cuda')\n",
    "# dataset_test[0].to('cuda')\n",
    "\n",
    "# dataset_train.to('cuda:0')\n",
    "# dataset_val.to('cuda:0')\n",
    "# dataset_test.to('cuda:0')\n",
    "\n",
    "print(type(dataset_train))\n",
    "print(dataset_train[0])\n",
    "print(type(dataset_train[0]))\n",
    "\n",
    "num_train_edges = dataset_train[0].edge_label_index.shape[1]\n",
    "num_val_edges = dataset_val[0].edge_label_index.shape[1]\n",
    "num_test_edges = dataset_test[0].edge_label_index.shape[1]\n",
    "\n",
    "print(\"Train set has {} supervision (positive) edges\".format(num_train_edges // 2))\n",
    "print(\"Validation set has {} supervision (positive) edges\".format(num_val_edges // 2))\n",
    "print(\"Test set has {} supervision (positive) edges\".format(num_test_edges // 2))\n",
    "\n",
    "print(\"Train set has {} message passing edges\".format(dataset_train[0].edge_index.shape[1]))\n",
    "print(\"Validation set has {} message passing edges\".format(dataset_val[0].edge_index.shape[1]))\n",
    "print(\"Test set has {} message passing edges\".format(dataset_test[0].edge_index.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "5404e392-347d-444e-8cf1-3a898b6eae87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pickle.dump(dataset_train, open('./graphs/train.graph', 'wb'))\n",
    "# pickle.dump(dataset_val, open('./graphs/val.graph', 'wb'))\n",
    "# pickle.dump(dataset_test, open('./graphs/test.graph', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "9e5cb885-4942-439a-8766-a0b7f9ebbc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_train = pickle.load(open('./graphs/train.graph', 'rb'))\n",
    "# dataset_val = pickle.load(open('./graphs/val.graph', 'rb'))\n",
    "# dataset_test = pickle.load(open('./graphs/test.graph', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "b59dec8f-30e8-4c5b-ab48-3cccfff33387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(LightGCNConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        out = self.propagate(edge_index, x=(x, x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        out = x_j\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        node_dim = self.node_dim\n",
    "        out = torch_scatter.scatter(inputs, index, dim=node_dim, reduce='mean')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "63ff20e2-0434-4fd8-89a1-e8db325c3e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCN(torch.nn.Module):\n",
    "    def __init__(self, train_data, num_layers, emb_size=16, initialize_with_words=False):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        assert (num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(num_layers):\n",
    "            self.convs.append(LightGCNConv(input_dim, input_dim))\n",
    "\n",
    "        # Initialize using custom embeddings if provided\n",
    "        num_nodes = train_data.node_label_index.size()[0]\n",
    "        self.embeddings = nn.Embedding(num_nodes, emb_size)\n",
    "        if initialize_with_words:\n",
    "            self.embeddings.weight.data.copy_(train_datanode_features)\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_size = emb_size\n",
    "        self.num_modes = num_nodes\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index, edge_label_index, node_label_index = data.edge_index, data.edge_label_index, data.node_label_index\n",
    "        layer_embeddings = []\n",
    "        \n",
    "        x = self.embeddings(node_label_index)\n",
    "        mean_layer = x\n",
    "\n",
    "        # We take an average of ever layer's node embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            # print(\"x shape\",x.shape)\n",
    "            # print(\"mean_layer shape\",mean_layer.shape)\n",
    "            mean_layer += x\n",
    "\n",
    "        mean_layer /= 4\n",
    "\n",
    "        # Prediction head is simply dot product\n",
    "        nodes_first = torch.index_select(x, 0, edge_label_index[0,:].long())\n",
    "        nodes_second = torch.index_select(x, 0, edge_label_index[1,:].long())\n",
    "\n",
    "        # Since we don't want a rank output, we create a sigmoid of the dot product\n",
    "        out = torch.sum(nodes_first * nodes_second, dim=-1) # FOR RANKING\n",
    "        pred = torch.sigmoid(out)\n",
    "\n",
    "        return torch.flatten(pred)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return self.loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "e8fffe68-e6e4-4892-9b18-13be04810acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "8633e9c3-115c-4ed1-a559-c6d2d9267b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deepsnap.dataset.GraphDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "1ebe5156-60c7-4699-93f5-03466c7433d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'device': 'cuda', 'num_layers': 3, 'emb_size': 32, 'weight_decay': 1e-05, 'lr': 0.01, 'epochs': 600}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_layers' : 3,\n",
    "    'emb_size' : 32,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 600\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'train': dataset_train[0],\n",
    "    'val': dataset_val[0],\n",
    "    'test': dataset_test[0]\n",
    "}\n",
    "            \n",
    "input_dim = datasets['train'].num_node_features\n",
    "print(input_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "ed65aa42-8be4-450f-bdec-aa2206feb866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets['train'].to(args['device'])\n",
    "# datasets['val'].to(args['device'])\n",
    "# datasets['test'].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "941df0fc-fc53-4f9d-b457-b2c562175a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, optimizer, args):\n",
    "    val_max = 0\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        datasets['train'].to(args[\"device\"])\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(datasets['train'])\n",
    "        loss = model.loss(pred, datasets['train'].edge_label.type(pred.dtype))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Loss: {:.5f}, Val Loss: {:.5f}'\n",
    "        score_train, train_loss = test(model, 'train', args)\n",
    "        score_val, val_loss = test(model, 'val', args)\n",
    "        score_test, test_loss = test(model, 'test', args)\n",
    "\n",
    "        losses.append((train_loss, val_loss))\n",
    "\n",
    "        print(log.format(epoch, score_train, score_val, score_test, train_loss, val_loss))\n",
    "        if val_max < score_val:\n",
    "            val_max = score_val\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def test(model, mode, args):\n",
    "    model.eval()\n",
    "    score = 0\n",
    "    loss_score = 0\n",
    "\n",
    "    data = datasets[mode]\n",
    "    data.to(args[\"device\"])\n",
    "\n",
    "    pred = model(data)\n",
    "    loss = model.loss(pred, data.edge_label.type(pred.dtype))\n",
    "    score += roc_auc_score(data.edge_label.flatten().cpu().numpy(), pred.flatten().data.cpu().numpy())\n",
    "    loss_score += loss.item()\n",
    "\n",
    "    return score, loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "8ed78571-2201-450e-a3aa-6bd29ba4529e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.4867, Val: 0.4861, Test: 0.4831, Loss: 0.69831, Val Loss: 0.69832\n",
      "Epoch: 002, Train: 0.5586, Val: 0.5585, Test: 0.5554, Loss: 0.69573, Val Loss: 0.69572\n",
      "Epoch: 003, Train: 0.6083, Val: 0.6083, Test: 0.6056, Loss: 0.69394, Val Loss: 0.69392\n",
      "Epoch: 004, Train: 0.6411, Val: 0.6409, Test: 0.6387, Loss: 0.69277, Val Loss: 0.69274\n",
      "Epoch: 005, Train: 0.6619, Val: 0.6616, Test: 0.6597, Loss: 0.69202, Val Loss: 0.69198\n",
      "Epoch: 006, Train: 0.6756, Val: 0.6750, Test: 0.6732, Loss: 0.69155, Val Loss: 0.69151\n",
      "Epoch: 007, Train: 0.6853, Val: 0.6845, Test: 0.6827, Loss: 0.69124, Val Loss: 0.69119\n",
      "Epoch: 008, Train: 0.6930, Val: 0.6919, Test: 0.6903, Loss: 0.69100, Val Loss: 0.69095\n",
      "Epoch: 009, Train: 0.6999, Val: 0.6987, Test: 0.6970, Loss: 0.69078, Val Loss: 0.69073\n",
      "Epoch: 010, Train: 0.7064, Val: 0.7052, Test: 0.7036, Loss: 0.69055, Val Loss: 0.69050\n",
      "Epoch: 011, Train: 0.7128, Val: 0.7116, Test: 0.7100, Loss: 0.69030, Val Loss: 0.69025\n",
      "Epoch: 012, Train: 0.7191, Val: 0.7179, Test: 0.7163, Loss: 0.69002, Val Loss: 0.68998\n",
      "Epoch: 013, Train: 0.7251, Val: 0.7240, Test: 0.7225, Loss: 0.68972, Val Loss: 0.68968\n",
      "Epoch: 014, Train: 0.7309, Val: 0.7299, Test: 0.7284, Loss: 0.68941, Val Loss: 0.68937\n",
      "Epoch: 015, Train: 0.7363, Val: 0.7353, Test: 0.7339, Loss: 0.68909, Val Loss: 0.68906\n",
      "Epoch: 016, Train: 0.7412, Val: 0.7401, Test: 0.7388, Loss: 0.68877, Val Loss: 0.68874\n",
      "Epoch: 017, Train: 0.7454, Val: 0.7443, Test: 0.7430, Loss: 0.68846, Val Loss: 0.68843\n",
      "Epoch: 018, Train: 0.7491, Val: 0.7480, Test: 0.7467, Loss: 0.68815, Val Loss: 0.68812\n",
      "Epoch: 019, Train: 0.7523, Val: 0.7511, Test: 0.7499, Loss: 0.68784, Val Loss: 0.68782\n",
      "Epoch: 020, Train: 0.7552, Val: 0.7540, Test: 0.7527, Loss: 0.68753, Val Loss: 0.68752\n",
      "Epoch: 021, Train: 0.7579, Val: 0.7568, Test: 0.7554, Loss: 0.68722, Val Loss: 0.68721\n",
      "Epoch: 022, Train: 0.7605, Val: 0.7594, Test: 0.7580, Loss: 0.68690, Val Loss: 0.68689\n",
      "Epoch: 023, Train: 0.7631, Val: 0.7620, Test: 0.7605, Loss: 0.68657, Val Loss: 0.68657\n",
      "Epoch: 024, Train: 0.7657, Val: 0.7647, Test: 0.7631, Loss: 0.68623, Val Loss: 0.68623\n",
      "Epoch: 025, Train: 0.7684, Val: 0.7674, Test: 0.7657, Loss: 0.68588, Val Loss: 0.68588\n",
      "Epoch: 026, Train: 0.7711, Val: 0.7702, Test: 0.7684, Loss: 0.68550, Val Loss: 0.68551\n",
      "Epoch: 027, Train: 0.7738, Val: 0.7730, Test: 0.7710, Loss: 0.68511, Val Loss: 0.68512\n",
      "Epoch: 028, Train: 0.7764, Val: 0.7756, Test: 0.7736, Loss: 0.68470, Val Loss: 0.68471\n",
      "Epoch: 029, Train: 0.7788, Val: 0.7780, Test: 0.7760, Loss: 0.68427, Val Loss: 0.68428\n",
      "Epoch: 030, Train: 0.7808, Val: 0.7800, Test: 0.7779, Loss: 0.68381, Val Loss: 0.68383\n",
      "Epoch: 031, Train: 0.7823, Val: 0.7814, Test: 0.7792, Loss: 0.68334, Val Loss: 0.68335\n",
      "Epoch: 032, Train: 0.7833, Val: 0.7823, Test: 0.7801, Loss: 0.68284, Val Loss: 0.68286\n",
      "Epoch: 033, Train: 0.7841, Val: 0.7829, Test: 0.7806, Loss: 0.68231, Val Loss: 0.68233\n",
      "Epoch: 034, Train: 0.7847, Val: 0.7835, Test: 0.7812, Loss: 0.68175, Val Loss: 0.68178\n",
      "Epoch: 035, Train: 0.7854, Val: 0.7842, Test: 0.7818, Loss: 0.68117, Val Loss: 0.68120\n",
      "Epoch: 036, Train: 0.7861, Val: 0.7849, Test: 0.7825, Loss: 0.68056, Val Loss: 0.68059\n",
      "Epoch: 037, Train: 0.7868, Val: 0.7856, Test: 0.7833, Loss: 0.67991, Val Loss: 0.67994\n",
      "Epoch: 038, Train: 0.7873, Val: 0.7863, Test: 0.7840, Loss: 0.67923, Val Loss: 0.67927\n",
      "Epoch: 039, Train: 0.7876, Val: 0.7867, Test: 0.7845, Loss: 0.67851, Val Loss: 0.67856\n",
      "Epoch: 040, Train: 0.7876, Val: 0.7867, Test: 0.7846, Loss: 0.67776, Val Loss: 0.67781\n",
      "Epoch: 041, Train: 0.7874, Val: 0.7865, Test: 0.7844, Loss: 0.67697, Val Loss: 0.67703\n",
      "Epoch: 042, Train: 0.7869, Val: 0.7861, Test: 0.7841, Loss: 0.67615, Val Loss: 0.67620\n",
      "Epoch: 043, Train: 0.7864, Val: 0.7856, Test: 0.7836, Loss: 0.67528, Val Loss: 0.67534\n",
      "Epoch: 044, Train: 0.7857, Val: 0.7851, Test: 0.7830, Loss: 0.67437, Val Loss: 0.67444\n",
      "Epoch: 045, Train: 0.7850, Val: 0.7845, Test: 0.7823, Loss: 0.67342, Val Loss: 0.67350\n",
      "Epoch: 046, Train: 0.7842, Val: 0.7837, Test: 0.7815, Loss: 0.67244, Val Loss: 0.67252\n",
      "Epoch: 047, Train: 0.7835, Val: 0.7829, Test: 0.7808, Loss: 0.67141, Val Loss: 0.67150\n",
      "Epoch: 048, Train: 0.7828, Val: 0.7822, Test: 0.7801, Loss: 0.67034, Val Loss: 0.67044\n",
      "Epoch: 049, Train: 0.7822, Val: 0.7816, Test: 0.7795, Loss: 0.66923, Val Loss: 0.66933\n",
      "Epoch: 050, Train: 0.7817, Val: 0.7810, Test: 0.7789, Loss: 0.66808, Val Loss: 0.66820\n",
      "Epoch: 051, Train: 0.7813, Val: 0.7806, Test: 0.7784, Loss: 0.66690, Val Loss: 0.66702\n",
      "Epoch: 052, Train: 0.7811, Val: 0.7802, Test: 0.7781, Loss: 0.66568, Val Loss: 0.66581\n",
      "Epoch: 053, Train: 0.7809, Val: 0.7800, Test: 0.7779, Loss: 0.66442, Val Loss: 0.66457\n",
      "Epoch: 054, Train: 0.7809, Val: 0.7799, Test: 0.7778, Loss: 0.66314, Val Loss: 0.66330\n",
      "Epoch: 055, Train: 0.7809, Val: 0.7799, Test: 0.7778, Loss: 0.66183, Val Loss: 0.66200\n",
      "Epoch: 056, Train: 0.7811, Val: 0.7800, Test: 0.7779, Loss: 0.66050, Val Loss: 0.66067\n",
      "Epoch: 057, Train: 0.7814, Val: 0.7803, Test: 0.7782, Loss: 0.65914, Val Loss: 0.65933\n",
      "Epoch: 058, Train: 0.7818, Val: 0.7806, Test: 0.7786, Loss: 0.65777, Val Loss: 0.65797\n",
      "Epoch: 059, Train: 0.7823, Val: 0.7811, Test: 0.7790, Loss: 0.65638, Val Loss: 0.65659\n",
      "Epoch: 060, Train: 0.7828, Val: 0.7816, Test: 0.7795, Loss: 0.65498, Val Loss: 0.65520\n",
      "Epoch: 061, Train: 0.7834, Val: 0.7822, Test: 0.7801, Loss: 0.65357, Val Loss: 0.65381\n",
      "Epoch: 062, Train: 0.7841, Val: 0.7829, Test: 0.7808, Loss: 0.65216, Val Loss: 0.65241\n",
      "Epoch: 063, Train: 0.7849, Val: 0.7837, Test: 0.7816, Loss: 0.65075, Val Loss: 0.65101\n",
      "Epoch: 064, Train: 0.7858, Val: 0.7846, Test: 0.7825, Loss: 0.64934, Val Loss: 0.64961\n",
      "Epoch: 065, Train: 0.7867, Val: 0.7855, Test: 0.7834, Loss: 0.64793, Val Loss: 0.64822\n",
      "Epoch: 066, Train: 0.7877, Val: 0.7865, Test: 0.7844, Loss: 0.64654, Val Loss: 0.64684\n",
      "Epoch: 067, Train: 0.7888, Val: 0.7875, Test: 0.7855, Loss: 0.64516, Val Loss: 0.64547\n",
      "Epoch: 068, Train: 0.7898, Val: 0.7886, Test: 0.7865, Loss: 0.64379, Val Loss: 0.64412\n",
      "Epoch: 069, Train: 0.7909, Val: 0.7896, Test: 0.7876, Loss: 0.64245, Val Loss: 0.64278\n",
      "Epoch: 070, Train: 0.7920, Val: 0.7907, Test: 0.7887, Loss: 0.64112, Val Loss: 0.64147\n",
      "Epoch: 071, Train: 0.7931, Val: 0.7917, Test: 0.7898, Loss: 0.63981, Val Loss: 0.64018\n",
      "Epoch: 072, Train: 0.7941, Val: 0.7928, Test: 0.7908, Loss: 0.63853, Val Loss: 0.63891\n",
      "Epoch: 073, Train: 0.7952, Val: 0.7939, Test: 0.7919, Loss: 0.63727, Val Loss: 0.63766\n",
      "Epoch: 074, Train: 0.7963, Val: 0.7949, Test: 0.7929, Loss: 0.63604, Val Loss: 0.63644\n",
      "Epoch: 075, Train: 0.7973, Val: 0.7959, Test: 0.7940, Loss: 0.63483, Val Loss: 0.63525\n",
      "Epoch: 076, Train: 0.7982, Val: 0.7969, Test: 0.7949, Loss: 0.63365, Val Loss: 0.63408\n",
      "Epoch: 077, Train: 0.7992, Val: 0.7978, Test: 0.7959, Loss: 0.63249, Val Loss: 0.63294\n",
      "Epoch: 078, Train: 0.8001, Val: 0.7987, Test: 0.7968, Loss: 0.63137, Val Loss: 0.63182\n",
      "Epoch: 079, Train: 0.8010, Val: 0.7996, Test: 0.7977, Loss: 0.63027, Val Loss: 0.63074\n",
      "Epoch: 080, Train: 0.8018, Val: 0.8004, Test: 0.7985, Loss: 0.62919, Val Loss: 0.62967\n",
      "Epoch: 081, Train: 0.8026, Val: 0.8012, Test: 0.7993, Loss: 0.62815, Val Loss: 0.62864\n",
      "Epoch: 082, Train: 0.8034, Val: 0.8020, Test: 0.8001, Loss: 0.62713, Val Loss: 0.62763\n",
      "Epoch: 083, Train: 0.8042, Val: 0.8027, Test: 0.8009, Loss: 0.62613, Val Loss: 0.62665\n",
      "Epoch: 084, Train: 0.8049, Val: 0.8034, Test: 0.8016, Loss: 0.62517, Val Loss: 0.62569\n",
      "Epoch: 085, Train: 0.8056, Val: 0.8041, Test: 0.8023, Loss: 0.62422, Val Loss: 0.62476\n",
      "Epoch: 086, Train: 0.8063, Val: 0.8048, Test: 0.8030, Loss: 0.62331, Val Loss: 0.62385\n",
      "Epoch: 087, Train: 0.8069, Val: 0.8055, Test: 0.8036, Loss: 0.62242, Val Loss: 0.62297\n",
      "Epoch: 088, Train: 0.8076, Val: 0.8061, Test: 0.8043, Loss: 0.62155, Val Loss: 0.62211\n",
      "Epoch: 089, Train: 0.8082, Val: 0.8068, Test: 0.8049, Loss: 0.62071, Val Loss: 0.62128\n",
      "Epoch: 090, Train: 0.8088, Val: 0.8074, Test: 0.8055, Loss: 0.61988, Val Loss: 0.62046\n",
      "Epoch: 091, Train: 0.8095, Val: 0.8080, Test: 0.8061, Loss: 0.61909, Val Loss: 0.61967\n",
      "Epoch: 092, Train: 0.8101, Val: 0.8086, Test: 0.8067, Loss: 0.61831, Val Loss: 0.61891\n",
      "Epoch: 093, Train: 0.8106, Val: 0.8092, Test: 0.8073, Loss: 0.61756, Val Loss: 0.61816\n",
      "Epoch: 094, Train: 0.8112, Val: 0.8097, Test: 0.8079, Loss: 0.61682, Val Loss: 0.61743\n",
      "Epoch: 095, Train: 0.8118, Val: 0.8103, Test: 0.8084, Loss: 0.61611, Val Loss: 0.61673\n",
      "Epoch: 096, Train: 0.8123, Val: 0.8108, Test: 0.8090, Loss: 0.61541, Val Loss: 0.61604\n",
      "Epoch: 097, Train: 0.8129, Val: 0.8114, Test: 0.8095, Loss: 0.61474, Val Loss: 0.61537\n",
      "Epoch: 098, Train: 0.8134, Val: 0.8119, Test: 0.8100, Loss: 0.61408, Val Loss: 0.61472\n",
      "Epoch: 099, Train: 0.8139, Val: 0.8124, Test: 0.8106, Loss: 0.61344, Val Loss: 0.61408\n",
      "Epoch: 100, Train: 0.8144, Val: 0.8129, Test: 0.8111, Loss: 0.61281, Val Loss: 0.61346\n",
      "Epoch: 101, Train: 0.8149, Val: 0.8134, Test: 0.8115, Loss: 0.61220, Val Loss: 0.61286\n",
      "Epoch: 102, Train: 0.8154, Val: 0.8139, Test: 0.8120, Loss: 0.61161, Val Loss: 0.61227\n",
      "Epoch: 103, Train: 0.8159, Val: 0.8144, Test: 0.8125, Loss: 0.61103, Val Loss: 0.61170\n",
      "Epoch: 104, Train: 0.8163, Val: 0.8148, Test: 0.8129, Loss: 0.61046, Val Loss: 0.61114\n",
      "Epoch: 105, Train: 0.8167, Val: 0.8152, Test: 0.8133, Loss: 0.60991, Val Loss: 0.61059\n",
      "Epoch: 106, Train: 0.8172, Val: 0.8157, Test: 0.8138, Loss: 0.60937, Val Loss: 0.61006\n",
      "Epoch: 107, Train: 0.8176, Val: 0.8161, Test: 0.8142, Loss: 0.60885, Val Loss: 0.60954\n",
      "Epoch: 108, Train: 0.8180, Val: 0.8165, Test: 0.8146, Loss: 0.60833, Val Loss: 0.60903\n",
      "Epoch: 109, Train: 0.8184, Val: 0.8169, Test: 0.8150, Loss: 0.60783, Val Loss: 0.60853\n",
      "Epoch: 110, Train: 0.8187, Val: 0.8172, Test: 0.8153, Loss: 0.60734, Val Loss: 0.60804\n",
      "Epoch: 111, Train: 0.8191, Val: 0.8176, Test: 0.8157, Loss: 0.60686, Val Loss: 0.60757\n",
      "Epoch: 112, Train: 0.8195, Val: 0.8180, Test: 0.8161, Loss: 0.60639, Val Loss: 0.60711\n",
      "Epoch: 113, Train: 0.8199, Val: 0.8184, Test: 0.8164, Loss: 0.60593, Val Loss: 0.60665\n",
      "Epoch: 114, Train: 0.8202, Val: 0.8187, Test: 0.8168, Loss: 0.60549, Val Loss: 0.60621\n",
      "Epoch: 115, Train: 0.8206, Val: 0.8191, Test: 0.8172, Loss: 0.60505, Val Loss: 0.60577\n",
      "Epoch: 116, Train: 0.8209, Val: 0.8194, Test: 0.8175, Loss: 0.60462, Val Loss: 0.60535\n",
      "Epoch: 117, Train: 0.8212, Val: 0.8197, Test: 0.8178, Loss: 0.60420, Val Loss: 0.60493\n",
      "Epoch: 118, Train: 0.8216, Val: 0.8201, Test: 0.8182, Loss: 0.60379, Val Loss: 0.60453\n",
      "Epoch: 119, Train: 0.8219, Val: 0.8204, Test: 0.8185, Loss: 0.60339, Val Loss: 0.60413\n",
      "Epoch: 120, Train: 0.8222, Val: 0.8207, Test: 0.8188, Loss: 0.60300, Val Loss: 0.60374\n",
      "Epoch: 121, Train: 0.8225, Val: 0.8210, Test: 0.8191, Loss: 0.60261, Val Loss: 0.60336\n",
      "Epoch: 122, Train: 0.8228, Val: 0.8213, Test: 0.8194, Loss: 0.60223, Val Loss: 0.60299\n",
      "Epoch: 123, Train: 0.8231, Val: 0.8216, Test: 0.8197, Loss: 0.60186, Val Loss: 0.60262\n",
      "Epoch: 124, Train: 0.8234, Val: 0.8219, Test: 0.8200, Loss: 0.60150, Val Loss: 0.60226\n",
      "Epoch: 125, Train: 0.8237, Val: 0.8222, Test: 0.8203, Loss: 0.60115, Val Loss: 0.60191\n",
      "Epoch: 126, Train: 0.8239, Val: 0.8225, Test: 0.8206, Loss: 0.60080, Val Loss: 0.60157\n",
      "Epoch: 127, Train: 0.8242, Val: 0.8227, Test: 0.8208, Loss: 0.60046, Val Loss: 0.60123\n",
      "Epoch: 128, Train: 0.8245, Val: 0.8230, Test: 0.8211, Loss: 0.60012, Val Loss: 0.60090\n",
      "Epoch: 129, Train: 0.8247, Val: 0.8233, Test: 0.8213, Loss: 0.59979, Val Loss: 0.60057\n",
      "Epoch: 130, Train: 0.8250, Val: 0.8235, Test: 0.8216, Loss: 0.59947, Val Loss: 0.60025\n",
      "Epoch: 131, Train: 0.8252, Val: 0.8238, Test: 0.8218, Loss: 0.59916, Val Loss: 0.59994\n",
      "Epoch: 132, Train: 0.8255, Val: 0.8240, Test: 0.8221, Loss: 0.59885, Val Loss: 0.59963\n",
      "Epoch: 133, Train: 0.8257, Val: 0.8242, Test: 0.8223, Loss: 0.59854, Val Loss: 0.59933\n",
      "Epoch: 134, Train: 0.8259, Val: 0.8245, Test: 0.8226, Loss: 0.59824, Val Loss: 0.59903\n",
      "Epoch: 135, Train: 0.8262, Val: 0.8247, Test: 0.8228, Loss: 0.59795, Val Loss: 0.59874\n",
      "Epoch: 136, Train: 0.8264, Val: 0.8249, Test: 0.8230, Loss: 0.59766, Val Loss: 0.59846\n",
      "Epoch: 137, Train: 0.8266, Val: 0.8251, Test: 0.8232, Loss: 0.59738, Val Loss: 0.59818\n",
      "Epoch: 138, Train: 0.8268, Val: 0.8254, Test: 0.8234, Loss: 0.59710, Val Loss: 0.59790\n",
      "Epoch: 139, Train: 0.8270, Val: 0.8256, Test: 0.8237, Loss: 0.59682, Val Loss: 0.59763\n",
      "Epoch: 140, Train: 0.8272, Val: 0.8258, Test: 0.8239, Loss: 0.59655, Val Loss: 0.59736\n",
      "Epoch: 141, Train: 0.8274, Val: 0.8260, Test: 0.8241, Loss: 0.59629, Val Loss: 0.59710\n",
      "Epoch: 142, Train: 0.8276, Val: 0.8262, Test: 0.8243, Loss: 0.59603, Val Loss: 0.59684\n",
      "Epoch: 143, Train: 0.8278, Val: 0.8264, Test: 0.8245, Loss: 0.59578, Val Loss: 0.59659\n",
      "Epoch: 144, Train: 0.8280, Val: 0.8266, Test: 0.8246, Loss: 0.59552, Val Loss: 0.59634\n",
      "Epoch: 145, Train: 0.8282, Val: 0.8267, Test: 0.8248, Loss: 0.59528, Val Loss: 0.59610\n",
      "Epoch: 146, Train: 0.8284, Val: 0.8269, Test: 0.8250, Loss: 0.59504, Val Loss: 0.59586\n",
      "Epoch: 147, Train: 0.8286, Val: 0.8271, Test: 0.8252, Loss: 0.59480, Val Loss: 0.59562\n",
      "Epoch: 148, Train: 0.8288, Val: 0.8273, Test: 0.8254, Loss: 0.59456, Val Loss: 0.59539\n",
      "Epoch: 149, Train: 0.8289, Val: 0.8275, Test: 0.8256, Loss: 0.59433, Val Loss: 0.59516\n",
      "Epoch: 150, Train: 0.8291, Val: 0.8276, Test: 0.8257, Loss: 0.59411, Val Loss: 0.59494\n",
      "Epoch: 151, Train: 0.8293, Val: 0.8278, Test: 0.8259, Loss: 0.59388, Val Loss: 0.59472\n",
      "Epoch: 152, Train: 0.8294, Val: 0.8280, Test: 0.8261, Loss: 0.59366, Val Loss: 0.59450\n",
      "Epoch: 153, Train: 0.8296, Val: 0.8281, Test: 0.8262, Loss: 0.59345, Val Loss: 0.59429\n",
      "Epoch: 154, Train: 0.8298, Val: 0.8283, Test: 0.8264, Loss: 0.59323, Val Loss: 0.59407\n",
      "Epoch: 155, Train: 0.8299, Val: 0.8284, Test: 0.8265, Loss: 0.59302, Val Loss: 0.59387\n",
      "Epoch: 156, Train: 0.8301, Val: 0.8286, Test: 0.8267, Loss: 0.59282, Val Loss: 0.59366\n",
      "Epoch: 157, Train: 0.8302, Val: 0.8288, Test: 0.8269, Loss: 0.59261, Val Loss: 0.59346\n",
      "Epoch: 158, Train: 0.8304, Val: 0.8289, Test: 0.8270, Loss: 0.59242, Val Loss: 0.59326\n",
      "Epoch: 159, Train: 0.8305, Val: 0.8290, Test: 0.8271, Loss: 0.59222, Val Loss: 0.59307\n",
      "Epoch: 160, Train: 0.8307, Val: 0.8292, Test: 0.8273, Loss: 0.59202, Val Loss: 0.59288\n",
      "Epoch: 161, Train: 0.8308, Val: 0.8293, Test: 0.8274, Loss: 0.59183, Val Loss: 0.59269\n",
      "Epoch: 162, Train: 0.8309, Val: 0.8295, Test: 0.8276, Loss: 0.59165, Val Loss: 0.59250\n",
      "Epoch: 163, Train: 0.8311, Val: 0.8296, Test: 0.8277, Loss: 0.59146, Val Loss: 0.59232\n",
      "Epoch: 164, Train: 0.8312, Val: 0.8297, Test: 0.8278, Loss: 0.59128, Val Loss: 0.59214\n",
      "Epoch: 165, Train: 0.8314, Val: 0.8299, Test: 0.8280, Loss: 0.59110, Val Loss: 0.59196\n",
      "Epoch: 166, Train: 0.8315, Val: 0.8300, Test: 0.8281, Loss: 0.59092, Val Loss: 0.59179\n",
      "Epoch: 167, Train: 0.8316, Val: 0.8301, Test: 0.8282, Loss: 0.59075, Val Loss: 0.59161\n",
      "Epoch: 168, Train: 0.8317, Val: 0.8303, Test: 0.8284, Loss: 0.59058, Val Loss: 0.59144\n",
      "Epoch: 169, Train: 0.8319, Val: 0.8304, Test: 0.8285, Loss: 0.59041, Val Loss: 0.59128\n",
      "Epoch: 170, Train: 0.8320, Val: 0.8305, Test: 0.8286, Loss: 0.59024, Val Loss: 0.59111\n",
      "Epoch: 171, Train: 0.8321, Val: 0.8306, Test: 0.8287, Loss: 0.59008, Val Loss: 0.59095\n",
      "Epoch: 172, Train: 0.8322, Val: 0.8307, Test: 0.8289, Loss: 0.58991, Val Loss: 0.59079\n",
      "Epoch: 173, Train: 0.8323, Val: 0.8309, Test: 0.8290, Loss: 0.58975, Val Loss: 0.59063\n",
      "Epoch: 174, Train: 0.8325, Val: 0.8310, Test: 0.8291, Loss: 0.58960, Val Loss: 0.59047\n",
      "Epoch: 175, Train: 0.8326, Val: 0.8311, Test: 0.8292, Loss: 0.58944, Val Loss: 0.59032\n",
      "Epoch: 176, Train: 0.8327, Val: 0.8312, Test: 0.8293, Loss: 0.58929, Val Loss: 0.59017\n",
      "Epoch: 177, Train: 0.8328, Val: 0.8313, Test: 0.8294, Loss: 0.58914, Val Loss: 0.59002\n",
      "Epoch: 178, Train: 0.8329, Val: 0.8314, Test: 0.8295, Loss: 0.58899, Val Loss: 0.58987\n",
      "Epoch: 179, Train: 0.8330, Val: 0.8315, Test: 0.8296, Loss: 0.58884, Val Loss: 0.58973\n",
      "Epoch: 180, Train: 0.8331, Val: 0.8316, Test: 0.8297, Loss: 0.58870, Val Loss: 0.58959\n",
      "Epoch: 181, Train: 0.8332, Val: 0.8317, Test: 0.8298, Loss: 0.58856, Val Loss: 0.58944\n",
      "Epoch: 182, Train: 0.8333, Val: 0.8318, Test: 0.8299, Loss: 0.58841, Val Loss: 0.58931\n",
      "Epoch: 183, Train: 0.8334, Val: 0.8319, Test: 0.8300, Loss: 0.58828, Val Loss: 0.58917\n",
      "Epoch: 184, Train: 0.8335, Val: 0.8320, Test: 0.8301, Loss: 0.58814, Val Loss: 0.58903\n",
      "Epoch: 185, Train: 0.8336, Val: 0.8321, Test: 0.8302, Loss: 0.58800, Val Loss: 0.58890\n",
      "Epoch: 186, Train: 0.8337, Val: 0.8322, Test: 0.8303, Loss: 0.58787, Val Loss: 0.58877\n",
      "Epoch: 187, Train: 0.8338, Val: 0.8323, Test: 0.8304, Loss: 0.58774, Val Loss: 0.58864\n",
      "Epoch: 188, Train: 0.8339, Val: 0.8324, Test: 0.8305, Loss: 0.58761, Val Loss: 0.58851\n",
      "Epoch: 189, Train: 0.8340, Val: 0.8325, Test: 0.8306, Loss: 0.58748, Val Loss: 0.58838\n",
      "Epoch: 190, Train: 0.8341, Val: 0.8326, Test: 0.8307, Loss: 0.58736, Val Loss: 0.58826\n",
      "Epoch: 191, Train: 0.8342, Val: 0.8327, Test: 0.8308, Loss: 0.58723, Val Loss: 0.58814\n",
      "Epoch: 192, Train: 0.8342, Val: 0.8328, Test: 0.8309, Loss: 0.58711, Val Loss: 0.58801\n",
      "Epoch: 193, Train: 0.8343, Val: 0.8328, Test: 0.8310, Loss: 0.58699, Val Loss: 0.58789\n",
      "Epoch: 194, Train: 0.8344, Val: 0.8329, Test: 0.8311, Loss: 0.58687, Val Loss: 0.58778\n",
      "Epoch: 195, Train: 0.8345, Val: 0.8330, Test: 0.8311, Loss: 0.58675, Val Loss: 0.58766\n",
      "Epoch: 196, Train: 0.8346, Val: 0.8331, Test: 0.8312, Loss: 0.58663, Val Loss: 0.58754\n",
      "Epoch: 197, Train: 0.8347, Val: 0.8332, Test: 0.8313, Loss: 0.58652, Val Loss: 0.58743\n",
      "Epoch: 198, Train: 0.8347, Val: 0.8333, Test: 0.8314, Loss: 0.58641, Val Loss: 0.58732\n",
      "Epoch: 199, Train: 0.8348, Val: 0.8333, Test: 0.8315, Loss: 0.58629, Val Loss: 0.58721\n",
      "Epoch: 200, Train: 0.8349, Val: 0.8334, Test: 0.8316, Loss: 0.58618, Val Loss: 0.58710\n",
      "Epoch: 201, Train: 0.8350, Val: 0.8335, Test: 0.8316, Loss: 0.58607, Val Loss: 0.58699\n",
      "Epoch: 202, Train: 0.8351, Val: 0.8336, Test: 0.8317, Loss: 0.58597, Val Loss: 0.58688\n",
      "Epoch: 203, Train: 0.8351, Val: 0.8336, Test: 0.8318, Loss: 0.58586, Val Loss: 0.58678\n",
      "Epoch: 204, Train: 0.8352, Val: 0.8337, Test: 0.8319, Loss: 0.58576, Val Loss: 0.58667\n",
      "Epoch: 205, Train: 0.8353, Val: 0.8338, Test: 0.8319, Loss: 0.58565, Val Loss: 0.58657\n",
      "Epoch: 206, Train: 0.8353, Val: 0.8339, Test: 0.8320, Loss: 0.58555, Val Loss: 0.58647\n",
      "Epoch: 207, Train: 0.8354, Val: 0.8339, Test: 0.8321, Loss: 0.58545, Val Loss: 0.58637\n",
      "Epoch: 208, Train: 0.8355, Val: 0.8340, Test: 0.8321, Loss: 0.58535, Val Loss: 0.58627\n",
      "Epoch: 209, Train: 0.8356, Val: 0.8341, Test: 0.8322, Loss: 0.58525, Val Loss: 0.58618\n",
      "Epoch: 210, Train: 0.8356, Val: 0.8341, Test: 0.8323, Loss: 0.58515, Val Loss: 0.58608\n",
      "Epoch: 211, Train: 0.8357, Val: 0.8342, Test: 0.8323, Loss: 0.58506, Val Loss: 0.58598\n",
      "Epoch: 212, Train: 0.8358, Val: 0.8343, Test: 0.8324, Loss: 0.58496, Val Loss: 0.58589\n",
      "Epoch: 213, Train: 0.8358, Val: 0.8343, Test: 0.8325, Loss: 0.58487, Val Loss: 0.58580\n",
      "Epoch: 214, Train: 0.8359, Val: 0.8344, Test: 0.8325, Loss: 0.58478, Val Loss: 0.58571\n",
      "Epoch: 215, Train: 0.8360, Val: 0.8345, Test: 0.8326, Loss: 0.58469, Val Loss: 0.58562\n",
      "Epoch: 216, Train: 0.8360, Val: 0.8345, Test: 0.8327, Loss: 0.58460, Val Loss: 0.58553\n",
      "Epoch: 217, Train: 0.8361, Val: 0.8346, Test: 0.8327, Loss: 0.58451, Val Loss: 0.58544\n",
      "Epoch: 218, Train: 0.8361, Val: 0.8347, Test: 0.8328, Loss: 0.58442, Val Loss: 0.58535\n",
      "Epoch: 219, Train: 0.8362, Val: 0.8347, Test: 0.8329, Loss: 0.58433, Val Loss: 0.58527\n",
      "Epoch: 220, Train: 0.8363, Val: 0.8348, Test: 0.8329, Loss: 0.58425, Val Loss: 0.58518\n",
      "Epoch: 221, Train: 0.8363, Val: 0.8348, Test: 0.8330, Loss: 0.58416, Val Loss: 0.58510\n",
      "Epoch: 222, Train: 0.8364, Val: 0.8349, Test: 0.8330, Loss: 0.58408, Val Loss: 0.58501\n",
      "Epoch: 223, Train: 0.8364, Val: 0.8349, Test: 0.8331, Loss: 0.58400, Val Loss: 0.58493\n",
      "Epoch: 224, Train: 0.8365, Val: 0.8350, Test: 0.8332, Loss: 0.58391, Val Loss: 0.58485\n",
      "Epoch: 225, Train: 0.8365, Val: 0.8351, Test: 0.8332, Loss: 0.58383, Val Loss: 0.58477\n",
      "Epoch: 226, Train: 0.8366, Val: 0.8351, Test: 0.8333, Loss: 0.58375, Val Loss: 0.58469\n",
      "Epoch: 227, Train: 0.8367, Val: 0.8352, Test: 0.8333, Loss: 0.58367, Val Loss: 0.58461\n",
      "Epoch: 228, Train: 0.8367, Val: 0.8352, Test: 0.8334, Loss: 0.58360, Val Loss: 0.58454\n",
      "Epoch: 229, Train: 0.8368, Val: 0.8353, Test: 0.8334, Loss: 0.58352, Val Loss: 0.58446\n",
      "Epoch: 230, Train: 0.8368, Val: 0.8353, Test: 0.8335, Loss: 0.58344, Val Loss: 0.58439\n",
      "Epoch: 231, Train: 0.8369, Val: 0.8354, Test: 0.8335, Loss: 0.58337, Val Loss: 0.58431\n",
      "Epoch: 232, Train: 0.8369, Val: 0.8354, Test: 0.8336, Loss: 0.58329, Val Loss: 0.58424\n",
      "Epoch: 233, Train: 0.8370, Val: 0.8355, Test: 0.8336, Loss: 0.58322, Val Loss: 0.58417\n",
      "Epoch: 234, Train: 0.8370, Val: 0.8355, Test: 0.8337, Loss: 0.58315, Val Loss: 0.58409\n",
      "Epoch: 235, Train: 0.8371, Val: 0.8356, Test: 0.8337, Loss: 0.58308, Val Loss: 0.58402\n",
      "Epoch: 236, Train: 0.8371, Val: 0.8356, Test: 0.8338, Loss: 0.58301, Val Loss: 0.58395\n",
      "Epoch: 237, Train: 0.8372, Val: 0.8357, Test: 0.8338, Loss: 0.58294, Val Loss: 0.58389\n",
      "Epoch: 238, Train: 0.8372, Val: 0.8357, Test: 0.8339, Loss: 0.58287, Val Loss: 0.58382\n",
      "Epoch: 239, Train: 0.8373, Val: 0.8358, Test: 0.8339, Loss: 0.58280, Val Loss: 0.58375\n",
      "Epoch: 240, Train: 0.8373, Val: 0.8358, Test: 0.8340, Loss: 0.58273, Val Loss: 0.58368\n",
      "Epoch: 241, Train: 0.8373, Val: 0.8359, Test: 0.8340, Loss: 0.58267, Val Loss: 0.58362\n",
      "Epoch: 242, Train: 0.8374, Val: 0.8359, Test: 0.8341, Loss: 0.58260, Val Loss: 0.58355\n",
      "Epoch: 243, Train: 0.8374, Val: 0.8359, Test: 0.8341, Loss: 0.58253, Val Loss: 0.58349\n",
      "Epoch: 244, Train: 0.8375, Val: 0.8360, Test: 0.8341, Loss: 0.58247, Val Loss: 0.58342\n",
      "Epoch: 245, Train: 0.8375, Val: 0.8360, Test: 0.8342, Loss: 0.58241, Val Loss: 0.58336\n",
      "Epoch: 246, Train: 0.8376, Val: 0.8361, Test: 0.8342, Loss: 0.58234, Val Loss: 0.58330\n",
      "Epoch: 247, Train: 0.8376, Val: 0.8361, Test: 0.8343, Loss: 0.58228, Val Loss: 0.58324\n",
      "Epoch: 248, Train: 0.8376, Val: 0.8362, Test: 0.8343, Loss: 0.58222, Val Loss: 0.58318\n",
      "Epoch: 249, Train: 0.8377, Val: 0.8362, Test: 0.8344, Loss: 0.58216, Val Loss: 0.58312\n",
      "Epoch: 250, Train: 0.8377, Val: 0.8362, Test: 0.8344, Loss: 0.58210, Val Loss: 0.58306\n",
      "Epoch: 251, Train: 0.8378, Val: 0.8363, Test: 0.8344, Loss: 0.58204, Val Loss: 0.58300\n",
      "Epoch: 252, Train: 0.8378, Val: 0.8363, Test: 0.8345, Loss: 0.58198, Val Loss: 0.58294\n",
      "Epoch: 253, Train: 0.8378, Val: 0.8364, Test: 0.8345, Loss: 0.58192, Val Loss: 0.58288\n",
      "Epoch: 254, Train: 0.8379, Val: 0.8364, Test: 0.8346, Loss: 0.58187, Val Loss: 0.58283\n",
      "Epoch: 255, Train: 0.8379, Val: 0.8364, Test: 0.8346, Loss: 0.58181, Val Loss: 0.58277\n",
      "Epoch: 256, Train: 0.8380, Val: 0.8365, Test: 0.8346, Loss: 0.58175, Val Loss: 0.58271\n",
      "Epoch: 257, Train: 0.8380, Val: 0.8365, Test: 0.8347, Loss: 0.58170, Val Loss: 0.58266\n",
      "Epoch: 258, Train: 0.8380, Val: 0.8365, Test: 0.8347, Loss: 0.58164, Val Loss: 0.58261\n",
      "Epoch: 259, Train: 0.8381, Val: 0.8366, Test: 0.8347, Loss: 0.58159, Val Loss: 0.58255\n",
      "Epoch: 260, Train: 0.8381, Val: 0.8366, Test: 0.8348, Loss: 0.58154, Val Loss: 0.58250\n",
      "Epoch: 261, Train: 0.8381, Val: 0.8366, Test: 0.8348, Loss: 0.58148, Val Loss: 0.58245\n",
      "Epoch: 262, Train: 0.8382, Val: 0.8367, Test: 0.8348, Loss: 0.58143, Val Loss: 0.58240\n",
      "Epoch: 263, Train: 0.8382, Val: 0.8367, Test: 0.8349, Loss: 0.58138, Val Loss: 0.58234\n",
      "Epoch: 264, Train: 0.8382, Val: 0.8367, Test: 0.8349, Loss: 0.58133, Val Loss: 0.58229\n",
      "Epoch: 265, Train: 0.8383, Val: 0.8368, Test: 0.8349, Loss: 0.58128, Val Loss: 0.58224\n",
      "Epoch: 266, Train: 0.8383, Val: 0.8368, Test: 0.8350, Loss: 0.58123, Val Loss: 0.58219\n",
      "Epoch: 267, Train: 0.8383, Val: 0.8368, Test: 0.8350, Loss: 0.58118, Val Loss: 0.58215\n",
      "Epoch: 268, Train: 0.8384, Val: 0.8369, Test: 0.8350, Loss: 0.58113, Val Loss: 0.58210\n",
      "Epoch: 269, Train: 0.8384, Val: 0.8369, Test: 0.8351, Loss: 0.58108, Val Loss: 0.58205\n",
      "Epoch: 270, Train: 0.8384, Val: 0.8369, Test: 0.8351, Loss: 0.58103, Val Loss: 0.58200\n",
      "Epoch: 271, Train: 0.8385, Val: 0.8370, Test: 0.8351, Loss: 0.58099, Val Loss: 0.58196\n",
      "Epoch: 272, Train: 0.8385, Val: 0.8370, Test: 0.8352, Loss: 0.58094, Val Loss: 0.58191\n",
      "Epoch: 273, Train: 0.8385, Val: 0.8370, Test: 0.8352, Loss: 0.58089, Val Loss: 0.58186\n",
      "Epoch: 274, Train: 0.8386, Val: 0.8371, Test: 0.8352, Loss: 0.58085, Val Loss: 0.58182\n",
      "Epoch: 275, Train: 0.8386, Val: 0.8371, Test: 0.8353, Loss: 0.58080, Val Loss: 0.58177\n",
      "Epoch: 276, Train: 0.8386, Val: 0.8371, Test: 0.8353, Loss: 0.58076, Val Loss: 0.58173\n",
      "Epoch: 277, Train: 0.8387, Val: 0.8371, Test: 0.8353, Loss: 0.58071, Val Loss: 0.58169\n",
      "Epoch: 278, Train: 0.8387, Val: 0.8372, Test: 0.8353, Loss: 0.58067, Val Loss: 0.58164\n",
      "Epoch: 279, Train: 0.8387, Val: 0.8372, Test: 0.8354, Loss: 0.58063, Val Loss: 0.58160\n",
      "Epoch: 280, Train: 0.8387, Val: 0.8372, Test: 0.8354, Loss: 0.58058, Val Loss: 0.58156\n",
      "Epoch: 281, Train: 0.8388, Val: 0.8373, Test: 0.8354, Loss: 0.58054, Val Loss: 0.58152\n",
      "Epoch: 282, Train: 0.8388, Val: 0.8373, Test: 0.8355, Loss: 0.58050, Val Loss: 0.58147\n",
      "Epoch: 283, Train: 0.8388, Val: 0.8373, Test: 0.8355, Loss: 0.58046, Val Loss: 0.58143\n",
      "Epoch: 284, Train: 0.8388, Val: 0.8373, Test: 0.8355, Loss: 0.58042, Val Loss: 0.58139\n",
      "Epoch: 285, Train: 0.8389, Val: 0.8374, Test: 0.8355, Loss: 0.58038, Val Loss: 0.58135\n",
      "Epoch: 286, Train: 0.8389, Val: 0.8374, Test: 0.8356, Loss: 0.58034, Val Loss: 0.58131\n",
      "Epoch: 287, Train: 0.8389, Val: 0.8374, Test: 0.8356, Loss: 0.58030, Val Loss: 0.58127\n",
      "Epoch: 288, Train: 0.8389, Val: 0.8374, Test: 0.8356, Loss: 0.58026, Val Loss: 0.58124\n",
      "Epoch: 289, Train: 0.8390, Val: 0.8375, Test: 0.8356, Loss: 0.58022, Val Loss: 0.58120\n",
      "Epoch: 290, Train: 0.8390, Val: 0.8375, Test: 0.8357, Loss: 0.58018, Val Loss: 0.58116\n",
      "Epoch: 291, Train: 0.8390, Val: 0.8375, Test: 0.8357, Loss: 0.58014, Val Loss: 0.58112\n",
      "Epoch: 292, Train: 0.8390, Val: 0.8375, Test: 0.8357, Loss: 0.58011, Val Loss: 0.58109\n",
      "Epoch: 293, Train: 0.8391, Val: 0.8376, Test: 0.8357, Loss: 0.58007, Val Loss: 0.58105\n",
      "Epoch: 294, Train: 0.8391, Val: 0.8376, Test: 0.8358, Loss: 0.58003, Val Loss: 0.58101\n",
      "Epoch: 295, Train: 0.8391, Val: 0.8376, Test: 0.8358, Loss: 0.58000, Val Loss: 0.58098\n",
      "Epoch: 296, Train: 0.8391, Val: 0.8376, Test: 0.8358, Loss: 0.57996, Val Loss: 0.58094\n",
      "Epoch: 297, Train: 0.8392, Val: 0.8377, Test: 0.8358, Loss: 0.57993, Val Loss: 0.58091\n",
      "Epoch: 298, Train: 0.8392, Val: 0.8377, Test: 0.8359, Loss: 0.57989, Val Loss: 0.58087\n",
      "Epoch: 299, Train: 0.8392, Val: 0.8377, Test: 0.8359, Loss: 0.57986, Val Loss: 0.58084\n",
      "Epoch: 300, Train: 0.8392, Val: 0.8377, Test: 0.8359, Loss: 0.57982, Val Loss: 0.58080\n",
      "Epoch: 301, Train: 0.8392, Val: 0.8377, Test: 0.8359, Loss: 0.57979, Val Loss: 0.58077\n",
      "Epoch: 302, Train: 0.8393, Val: 0.8378, Test: 0.8359, Loss: 0.57976, Val Loss: 0.58074\n",
      "Epoch: 303, Train: 0.8393, Val: 0.8378, Test: 0.8360, Loss: 0.57972, Val Loss: 0.58070\n",
      "Epoch: 304, Train: 0.8393, Val: 0.8378, Test: 0.8360, Loss: 0.57969, Val Loss: 0.58067\n",
      "Epoch: 305, Train: 0.8393, Val: 0.8378, Test: 0.8360, Loss: 0.57966, Val Loss: 0.58064\n",
      "Epoch: 306, Train: 0.8394, Val: 0.8378, Test: 0.8360, Loss: 0.57962, Val Loss: 0.58061\n",
      "Epoch: 307, Train: 0.8394, Val: 0.8379, Test: 0.8360, Loss: 0.57959, Val Loss: 0.58058\n",
      "Epoch: 308, Train: 0.8394, Val: 0.8379, Test: 0.8361, Loss: 0.57956, Val Loss: 0.58054\n",
      "Epoch: 309, Train: 0.8394, Val: 0.8379, Test: 0.8361, Loss: 0.57953, Val Loss: 0.58051\n",
      "Epoch: 310, Train: 0.8394, Val: 0.8379, Test: 0.8361, Loss: 0.57950, Val Loss: 0.58048\n",
      "Epoch: 311, Train: 0.8394, Val: 0.8379, Test: 0.8361, Loss: 0.57947, Val Loss: 0.58045\n",
      "Epoch: 312, Train: 0.8395, Val: 0.8380, Test: 0.8361, Loss: 0.57944, Val Loss: 0.58042\n",
      "Epoch: 313, Train: 0.8395, Val: 0.8380, Test: 0.8362, Loss: 0.57941, Val Loss: 0.58039\n",
      "Epoch: 314, Train: 0.8395, Val: 0.8380, Test: 0.8362, Loss: 0.57938, Val Loss: 0.58037\n",
      "Epoch: 315, Train: 0.8395, Val: 0.8380, Test: 0.8362, Loss: 0.57935, Val Loss: 0.58034\n",
      "Epoch: 316, Train: 0.8395, Val: 0.8380, Test: 0.8362, Loss: 0.57932, Val Loss: 0.58031\n",
      "Epoch: 317, Train: 0.8396, Val: 0.8380, Test: 0.8362, Loss: 0.57929, Val Loss: 0.58028\n",
      "Epoch: 318, Train: 0.8396, Val: 0.8381, Test: 0.8362, Loss: 0.57927, Val Loss: 0.58025\n",
      "Epoch: 319, Train: 0.8396, Val: 0.8381, Test: 0.8363, Loss: 0.57924, Val Loss: 0.58022\n",
      "Epoch: 320, Train: 0.8396, Val: 0.8381, Test: 0.8363, Loss: 0.57921, Val Loss: 0.58020\n",
      "Epoch: 321, Train: 0.8396, Val: 0.8381, Test: 0.8363, Loss: 0.57918, Val Loss: 0.58017\n",
      "Epoch: 322, Train: 0.8396, Val: 0.8381, Test: 0.8363, Loss: 0.57916, Val Loss: 0.58014\n",
      "Epoch: 323, Train: 0.8397, Val: 0.8382, Test: 0.8363, Loss: 0.57913, Val Loss: 0.58012\n",
      "Epoch: 324, Train: 0.8397, Val: 0.8382, Test: 0.8363, Loss: 0.57910, Val Loss: 0.58009\n",
      "Epoch: 325, Train: 0.8397, Val: 0.8382, Test: 0.8364, Loss: 0.57908, Val Loss: 0.58006\n",
      "Epoch: 326, Train: 0.8397, Val: 0.8382, Test: 0.8364, Loss: 0.57905, Val Loss: 0.58004\n",
      "Epoch: 327, Train: 0.8397, Val: 0.8382, Test: 0.8364, Loss: 0.57902, Val Loss: 0.58001\n",
      "Epoch: 328, Train: 0.8397, Val: 0.8382, Test: 0.8364, Loss: 0.57900, Val Loss: 0.57999\n",
      "Epoch: 329, Train: 0.8398, Val: 0.8382, Test: 0.8364, Loss: 0.57897, Val Loss: 0.57996\n",
      "Epoch: 330, Train: 0.8398, Val: 0.8383, Test: 0.8364, Loss: 0.57895, Val Loss: 0.57994\n",
      "Epoch: 331, Train: 0.8398, Val: 0.8383, Test: 0.8365, Loss: 0.57893, Val Loss: 0.57991\n",
      "Epoch: 332, Train: 0.8398, Val: 0.8383, Test: 0.8365, Loss: 0.57890, Val Loss: 0.57989\n",
      "Epoch: 333, Train: 0.8398, Val: 0.8383, Test: 0.8365, Loss: 0.57888, Val Loss: 0.57987\n",
      "Epoch: 334, Train: 0.8398, Val: 0.8383, Test: 0.8365, Loss: 0.57885, Val Loss: 0.57984\n",
      "Epoch: 335, Train: 0.8398, Val: 0.8383, Test: 0.8365, Loss: 0.57883, Val Loss: 0.57982\n",
      "Epoch: 336, Train: 0.8399, Val: 0.8383, Test: 0.8365, Loss: 0.57881, Val Loss: 0.57980\n",
      "Epoch: 337, Train: 0.8399, Val: 0.8384, Test: 0.8365, Loss: 0.57878, Val Loss: 0.57977\n",
      "Epoch: 338, Train: 0.8399, Val: 0.8384, Test: 0.8366, Loss: 0.57876, Val Loss: 0.57975\n",
      "Epoch: 339, Train: 0.8399, Val: 0.8384, Test: 0.8366, Loss: 0.57874, Val Loss: 0.57973\n",
      "Epoch: 340, Train: 0.8399, Val: 0.8384, Test: 0.8366, Loss: 0.57872, Val Loss: 0.57971\n",
      "Epoch: 341, Train: 0.8399, Val: 0.8384, Test: 0.8366, Loss: 0.57869, Val Loss: 0.57969\n",
      "Epoch: 342, Train: 0.8399, Val: 0.8384, Test: 0.8366, Loss: 0.57867, Val Loss: 0.57966\n",
      "Epoch: 343, Train: 0.8400, Val: 0.8384, Test: 0.8366, Loss: 0.57865, Val Loss: 0.57964\n",
      "Epoch: 344, Train: 0.8400, Val: 0.8385, Test: 0.8366, Loss: 0.57863, Val Loss: 0.57962\n",
      "Epoch: 345, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57861, Val Loss: 0.57960\n",
      "Epoch: 346, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57859, Val Loss: 0.57958\n",
      "Epoch: 347, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57857, Val Loss: 0.57956\n",
      "Epoch: 348, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57855, Val Loss: 0.57954\n",
      "Epoch: 349, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57853, Val Loss: 0.57952\n",
      "Epoch: 350, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57851, Val Loss: 0.57950\n",
      "Epoch: 351, Train: 0.8400, Val: 0.8385, Test: 0.8367, Loss: 0.57849, Val Loss: 0.57948\n",
      "Epoch: 352, Train: 0.8401, Val: 0.8386, Test: 0.8367, Loss: 0.57847, Val Loss: 0.57946\n",
      "Epoch: 353, Train: 0.8401, Val: 0.8386, Test: 0.8367, Loss: 0.57845, Val Loss: 0.57944\n",
      "Epoch: 354, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57843, Val Loss: 0.57942\n",
      "Epoch: 355, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57841, Val Loss: 0.57940\n",
      "Epoch: 356, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57839, Val Loss: 0.57938\n",
      "Epoch: 357, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57837, Val Loss: 0.57937\n",
      "Epoch: 358, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57835, Val Loss: 0.57935\n",
      "Epoch: 359, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57833, Val Loss: 0.57933\n",
      "Epoch: 360, Train: 0.8401, Val: 0.8386, Test: 0.8368, Loss: 0.57832, Val Loss: 0.57931\n",
      "Epoch: 361, Train: 0.8402, Val: 0.8386, Test: 0.8368, Loss: 0.57830, Val Loss: 0.57929\n",
      "Epoch: 362, Train: 0.8402, Val: 0.8387, Test: 0.8368, Loss: 0.57828, Val Loss: 0.57928\n",
      "Epoch: 363, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57826, Val Loss: 0.57926\n",
      "Epoch: 364, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57825, Val Loss: 0.57924\n",
      "Epoch: 365, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57823, Val Loss: 0.57923\n",
      "Epoch: 366, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57821, Val Loss: 0.57921\n",
      "Epoch: 367, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57820, Val Loss: 0.57919\n",
      "Epoch: 368, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57818, Val Loss: 0.57918\n",
      "Epoch: 369, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57816, Val Loss: 0.57916\n",
      "Epoch: 370, Train: 0.8402, Val: 0.8387, Test: 0.8369, Loss: 0.57815, Val Loss: 0.57914\n",
      "Epoch: 371, Train: 0.8403, Val: 0.8387, Test: 0.8369, Loss: 0.57813, Val Loss: 0.57913\n",
      "Epoch: 372, Train: 0.8403, Val: 0.8388, Test: 0.8369, Loss: 0.57811, Val Loss: 0.57911\n",
      "Epoch: 373, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57810, Val Loss: 0.57910\n",
      "Epoch: 374, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57808, Val Loss: 0.57908\n",
      "Epoch: 375, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57807, Val Loss: 0.57907\n",
      "Epoch: 376, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57805, Val Loss: 0.57905\n",
      "Epoch: 377, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57804, Val Loss: 0.57904\n",
      "Epoch: 378, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57802, Val Loss: 0.57902\n",
      "Epoch: 379, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57801, Val Loss: 0.57901\n",
      "Epoch: 380, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57799, Val Loss: 0.57899\n",
      "Epoch: 381, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57798, Val Loss: 0.57898\n",
      "Epoch: 382, Train: 0.8403, Val: 0.8388, Test: 0.8370, Loss: 0.57796, Val Loss: 0.57896\n",
      "Epoch: 383, Train: 0.8404, Val: 0.8388, Test: 0.8370, Loss: 0.57795, Val Loss: 0.57895\n",
      "Epoch: 384, Train: 0.8404, Val: 0.8389, Test: 0.8370, Loss: 0.57794, Val Loss: 0.57893\n",
      "Epoch: 385, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57792, Val Loss: 0.57892\n",
      "Epoch: 386, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57791, Val Loss: 0.57891\n",
      "Epoch: 387, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57790, Val Loss: 0.57889\n",
      "Epoch: 388, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57788, Val Loss: 0.57888\n",
      "Epoch: 389, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57787, Val Loss: 0.57887\n",
      "Epoch: 390, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57786, Val Loss: 0.57885\n",
      "Epoch: 391, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57784, Val Loss: 0.57884\n",
      "Epoch: 392, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57783, Val Loss: 0.57883\n",
      "Epoch: 393, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57782, Val Loss: 0.57882\n",
      "Epoch: 394, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57780, Val Loss: 0.57880\n",
      "Epoch: 395, Train: 0.8404, Val: 0.8389, Test: 0.8371, Loss: 0.57779, Val Loss: 0.57879\n",
      "Epoch: 396, Train: 0.8405, Val: 0.8389, Test: 0.8371, Loss: 0.57778, Val Loss: 0.57878\n",
      "Epoch: 397, Train: 0.8405, Val: 0.8389, Test: 0.8371, Loss: 0.57777, Val Loss: 0.57877\n",
      "Epoch: 398, Train: 0.8405, Val: 0.8390, Test: 0.8371, Loss: 0.57775, Val Loss: 0.57876\n",
      "Epoch: 399, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57774, Val Loss: 0.57874\n",
      "Epoch: 400, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57773, Val Loss: 0.57873\n",
      "Epoch: 401, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57772, Val Loss: 0.57872\n",
      "Epoch: 402, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57771, Val Loss: 0.57871\n",
      "Epoch: 403, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57770, Val Loss: 0.57870\n",
      "Epoch: 404, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57768, Val Loss: 0.57869\n",
      "Epoch: 405, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57767, Val Loss: 0.57867\n",
      "Epoch: 406, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57766, Val Loss: 0.57866\n",
      "Epoch: 407, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57765, Val Loss: 0.57865\n",
      "Epoch: 408, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57764, Val Loss: 0.57864\n",
      "Epoch: 409, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57763, Val Loss: 0.57863\n",
      "Epoch: 410, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57762, Val Loss: 0.57862\n",
      "Epoch: 411, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57761, Val Loss: 0.57861\n",
      "Epoch: 412, Train: 0.8405, Val: 0.8390, Test: 0.8372, Loss: 0.57760, Val Loss: 0.57860\n",
      "Epoch: 413, Train: 0.8406, Val: 0.8390, Test: 0.8372, Loss: 0.57759, Val Loss: 0.57859\n",
      "Epoch: 414, Train: 0.8406, Val: 0.8391, Test: 0.8372, Loss: 0.57758, Val Loss: 0.57858\n",
      "Epoch: 415, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57757, Val Loss: 0.57857\n",
      "Epoch: 416, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57756, Val Loss: 0.57856\n",
      "Epoch: 417, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57755, Val Loss: 0.57855\n",
      "Epoch: 418, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57754, Val Loss: 0.57854\n",
      "Epoch: 419, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57753, Val Loss: 0.57853\n",
      "Epoch: 420, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57752, Val Loss: 0.57852\n",
      "Epoch: 421, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57751, Val Loss: 0.57851\n",
      "Epoch: 422, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57750, Val Loss: 0.57850\n",
      "Epoch: 423, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57749, Val Loss: 0.57849\n",
      "Epoch: 424, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57748, Val Loss: 0.57848\n",
      "Epoch: 425, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57747, Val Loss: 0.57847\n",
      "Epoch: 426, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57746, Val Loss: 0.57847\n",
      "Epoch: 427, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57745, Val Loss: 0.57846\n",
      "Epoch: 428, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57744, Val Loss: 0.57845\n",
      "Epoch: 429, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57744, Val Loss: 0.57844\n",
      "Epoch: 430, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57743, Val Loss: 0.57843\n",
      "Epoch: 431, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57742, Val Loss: 0.57842\n",
      "Epoch: 432, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57741, Val Loss: 0.57841\n",
      "Epoch: 433, Train: 0.8406, Val: 0.8391, Test: 0.8373, Loss: 0.57740, Val Loss: 0.57841\n",
      "Epoch: 434, Train: 0.8407, Val: 0.8391, Test: 0.8373, Loss: 0.57739, Val Loss: 0.57840\n",
      "Epoch: 435, Train: 0.8407, Val: 0.8392, Test: 0.8373, Loss: 0.57739, Val Loss: 0.57839\n",
      "Epoch: 436, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57738, Val Loss: 0.57838\n",
      "Epoch: 437, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57737, Val Loss: 0.57837\n",
      "Epoch: 438, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57736, Val Loss: 0.57836\n",
      "Epoch: 439, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57735, Val Loss: 0.57836\n",
      "Epoch: 440, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57735, Val Loss: 0.57835\n",
      "Epoch: 441, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57734, Val Loss: 0.57834\n",
      "Epoch: 442, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57733, Val Loss: 0.57833\n",
      "Epoch: 443, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57732, Val Loss: 0.57833\n",
      "Epoch: 444, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57732, Val Loss: 0.57832\n",
      "Epoch: 445, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57731, Val Loss: 0.57831\n",
      "Epoch: 446, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57730, Val Loss: 0.57830\n",
      "Epoch: 447, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57729, Val Loss: 0.57830\n",
      "Epoch: 448, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57729, Val Loss: 0.57829\n",
      "Epoch: 449, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57728, Val Loss: 0.57828\n",
      "Epoch: 450, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57727, Val Loss: 0.57828\n",
      "Epoch: 451, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57727, Val Loss: 0.57827\n",
      "Epoch: 452, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57726, Val Loss: 0.57826\n",
      "Epoch: 453, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57725, Val Loss: 0.57826\n",
      "Epoch: 454, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57724, Val Loss: 0.57825\n",
      "Epoch: 455, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57724, Val Loss: 0.57824\n",
      "Epoch: 456, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57723, Val Loss: 0.57824\n",
      "Epoch: 457, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57722, Val Loss: 0.57823\n",
      "Epoch: 458, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57722, Val Loss: 0.57822\n",
      "Epoch: 459, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57721, Val Loss: 0.57822\n",
      "Epoch: 460, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57721, Val Loss: 0.57821\n",
      "Epoch: 461, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57720, Val Loss: 0.57820\n",
      "Epoch: 462, Train: 0.8407, Val: 0.8392, Test: 0.8374, Loss: 0.57719, Val Loss: 0.57820\n",
      "Epoch: 463, Train: 0.8408, Val: 0.8392, Test: 0.8374, Loss: 0.57719, Val Loss: 0.57819\n",
      "Epoch: 464, Train: 0.8408, Val: 0.8392, Test: 0.8374, Loss: 0.57718, Val Loss: 0.57819\n",
      "Epoch: 465, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57717, Val Loss: 0.57818\n",
      "Epoch: 466, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57717, Val Loss: 0.57817\n",
      "Epoch: 467, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57716, Val Loss: 0.57817\n",
      "Epoch: 468, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57716, Val Loss: 0.57816\n",
      "Epoch: 469, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57715, Val Loss: 0.57816\n",
      "Epoch: 470, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57715, Val Loss: 0.57815\n",
      "Epoch: 471, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57714, Val Loss: 0.57814\n",
      "Epoch: 472, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57713, Val Loss: 0.57814\n",
      "Epoch: 473, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57713, Val Loss: 0.57813\n",
      "Epoch: 474, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57712, Val Loss: 0.57813\n",
      "Epoch: 475, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57712, Val Loss: 0.57812\n",
      "Epoch: 476, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57711, Val Loss: 0.57812\n",
      "Epoch: 477, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57711, Val Loss: 0.57811\n",
      "Epoch: 478, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57710, Val Loss: 0.57811\n",
      "Epoch: 479, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57710, Val Loss: 0.57810\n",
      "Epoch: 480, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57709, Val Loss: 0.57810\n",
      "Epoch: 481, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57709, Val Loss: 0.57809\n",
      "Epoch: 482, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57708, Val Loss: 0.57809\n",
      "Epoch: 483, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57708, Val Loss: 0.57808\n",
      "Epoch: 484, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57707, Val Loss: 0.57808\n",
      "Epoch: 485, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57707, Val Loss: 0.57807\n",
      "Epoch: 486, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57706, Val Loss: 0.57807\n",
      "Epoch: 487, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57706, Val Loss: 0.57806\n",
      "Epoch: 488, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57705, Val Loss: 0.57806\n",
      "Epoch: 489, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57705, Val Loss: 0.57805\n",
      "Epoch: 490, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57704, Val Loss: 0.57805\n",
      "Epoch: 491, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57704, Val Loss: 0.57804\n",
      "Epoch: 492, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57703, Val Loss: 0.57804\n",
      "Epoch: 493, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57703, Val Loss: 0.57803\n",
      "Epoch: 494, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57703, Val Loss: 0.57803\n",
      "Epoch: 495, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57702, Val Loss: 0.57803\n",
      "Epoch: 496, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57702, Val Loss: 0.57802\n",
      "Epoch: 497, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57701, Val Loss: 0.57802\n",
      "Epoch: 498, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57701, Val Loss: 0.57801\n",
      "Epoch: 499, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57700, Val Loss: 0.57801\n",
      "Epoch: 500, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57700, Val Loss: 0.57800\n",
      "Epoch: 501, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57700, Val Loss: 0.57800\n",
      "Epoch: 502, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57699, Val Loss: 0.57800\n",
      "Epoch: 503, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57699, Val Loss: 0.57799\n",
      "Epoch: 504, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57698, Val Loss: 0.57799\n",
      "Epoch: 505, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57698, Val Loss: 0.57798\n",
      "Epoch: 506, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57698, Val Loss: 0.57798\n",
      "Epoch: 507, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57697, Val Loss: 0.57798\n",
      "Epoch: 508, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57697, Val Loss: 0.57797\n",
      "Epoch: 509, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57696, Val Loss: 0.57797\n",
      "Epoch: 510, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57696, Val Loss: 0.57797\n",
      "Epoch: 511, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57696, Val Loss: 0.57796\n",
      "Epoch: 512, Train: 0.8408, Val: 0.8393, Test: 0.8375, Loss: 0.57695, Val Loss: 0.57796\n",
      "Epoch: 513, Train: 0.8408, Val: 0.8393, Test: 0.8376, Loss: 0.57695, Val Loss: 0.57795\n",
      "Epoch: 514, Train: 0.8409, Val: 0.8393, Test: 0.8376, Loss: 0.57695, Val Loss: 0.57795\n",
      "Epoch: 515, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57694, Val Loss: 0.57795\n",
      "Epoch: 516, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57694, Val Loss: 0.57794\n",
      "Epoch: 517, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57693, Val Loss: 0.57794\n",
      "Epoch: 518, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57693, Val Loss: 0.57794\n",
      "Epoch: 519, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57693, Val Loss: 0.57793\n",
      "Epoch: 520, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57692, Val Loss: 0.57793\n",
      "Epoch: 521, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57692, Val Loss: 0.57793\n",
      "Epoch: 522, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57692, Val Loss: 0.57792\n",
      "Epoch: 523, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57691, Val Loss: 0.57792\n",
      "Epoch: 524, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57691, Val Loss: 0.57792\n",
      "Epoch: 525, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57691, Val Loss: 0.57791\n",
      "Epoch: 526, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57691, Val Loss: 0.57791\n",
      "Epoch: 527, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57690, Val Loss: 0.57791\n",
      "Epoch: 528, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57690, Val Loss: 0.57790\n",
      "Epoch: 529, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57690, Val Loss: 0.57790\n",
      "Epoch: 530, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57689, Val Loss: 0.57790\n",
      "Epoch: 531, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57689, Val Loss: 0.57790\n",
      "Epoch: 532, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57689, Val Loss: 0.57789\n",
      "Epoch: 533, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57688, Val Loss: 0.57789\n",
      "Epoch: 534, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57688, Val Loss: 0.57789\n",
      "Epoch: 535, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57688, Val Loss: 0.57788\n",
      "Epoch: 536, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57688, Val Loss: 0.57788\n",
      "Epoch: 537, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57687, Val Loss: 0.57788\n",
      "Epoch: 538, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57687, Val Loss: 0.57788\n",
      "Epoch: 539, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57687, Val Loss: 0.57787\n",
      "Epoch: 540, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57686, Val Loss: 0.57787\n",
      "Epoch: 541, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57686, Val Loss: 0.57787\n",
      "Epoch: 542, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57686, Val Loss: 0.57786\n",
      "Epoch: 543, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57686, Val Loss: 0.57786\n",
      "Epoch: 544, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57685, Val Loss: 0.57786\n",
      "Epoch: 545, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57685, Val Loss: 0.57786\n",
      "Epoch: 546, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57685, Val Loss: 0.57785\n",
      "Epoch: 547, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57685, Val Loss: 0.57785\n",
      "Epoch: 548, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57684, Val Loss: 0.57785\n",
      "Epoch: 549, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57684, Val Loss: 0.57785\n",
      "Epoch: 550, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57684, Val Loss: 0.57784\n",
      "Epoch: 551, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57684, Val Loss: 0.57784\n",
      "Epoch: 552, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57683, Val Loss: 0.57784\n",
      "Epoch: 553, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57683, Val Loss: 0.57784\n",
      "Epoch: 554, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57683, Val Loss: 0.57783\n",
      "Epoch: 555, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57683, Val Loss: 0.57783\n",
      "Epoch: 556, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57682, Val Loss: 0.57783\n",
      "Epoch: 557, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57682, Val Loss: 0.57783\n",
      "Epoch: 558, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57682, Val Loss: 0.57783\n",
      "Epoch: 559, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57682, Val Loss: 0.57782\n",
      "Epoch: 560, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57682, Val Loss: 0.57782\n",
      "Epoch: 561, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57681, Val Loss: 0.57782\n",
      "Epoch: 562, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57681, Val Loss: 0.57782\n",
      "Epoch: 563, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57681, Val Loss: 0.57781\n",
      "Epoch: 564, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57681, Val Loss: 0.57781\n",
      "Epoch: 565, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57680, Val Loss: 0.57781\n",
      "Epoch: 566, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57680, Val Loss: 0.57781\n",
      "Epoch: 567, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57680, Val Loss: 0.57781\n",
      "Epoch: 568, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57680, Val Loss: 0.57780\n",
      "Epoch: 569, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57680, Val Loss: 0.57780\n",
      "Epoch: 570, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57679, Val Loss: 0.57780\n",
      "Epoch: 571, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57679, Val Loss: 0.57780\n",
      "Epoch: 572, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57679, Val Loss: 0.57780\n",
      "Epoch: 573, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57679, Val Loss: 0.57779\n",
      "Epoch: 574, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57679, Val Loss: 0.57779\n",
      "Epoch: 575, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57678, Val Loss: 0.57779\n",
      "Epoch: 576, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57678, Val Loss: 0.57779\n",
      "Epoch: 577, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57678, Val Loss: 0.57779\n",
      "Epoch: 578, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57678, Val Loss: 0.57779\n",
      "Epoch: 579, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57678, Val Loss: 0.57778\n",
      "Epoch: 580, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57678, Val Loss: 0.57778\n",
      "Epoch: 581, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57677, Val Loss: 0.57778\n",
      "Epoch: 582, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57677, Val Loss: 0.57778\n",
      "Epoch: 583, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57677, Val Loss: 0.57778\n",
      "Epoch: 584, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57677, Val Loss: 0.57777\n",
      "Epoch: 585, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57677, Val Loss: 0.57777\n",
      "Epoch: 586, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57677, Val Loss: 0.57777\n",
      "Epoch: 587, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57676, Val Loss: 0.57777\n",
      "Epoch: 588, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57676, Val Loss: 0.57777\n",
      "Epoch: 589, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57676, Val Loss: 0.57777\n",
      "Epoch: 590, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57676, Val Loss: 0.57776\n",
      "Epoch: 591, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57676, Val Loss: 0.57776\n",
      "Epoch: 592, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57676, Val Loss: 0.57776\n",
      "Epoch: 593, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57675, Val Loss: 0.57776\n",
      "Epoch: 594, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57675, Val Loss: 0.57776\n",
      "Epoch: 595, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57675, Val Loss: 0.57776\n",
      "Epoch: 596, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57675, Val Loss: 0.57776\n",
      "Epoch: 597, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57675, Val Loss: 0.57775\n",
      "Epoch: 598, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57675, Val Loss: 0.57775\n",
      "Epoch: 599, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57674, Val Loss: 0.57775\n",
      "Epoch: 600, Train: 0.8409, Val: 0.8394, Test: 0.8376, Loss: 0.57674, Val Loss: 0.57775\n",
      "Train: 0.8409, Val: 0.8394, Test: 0.8376, Train Loss: 0.57674, Val Loss: 0.57775, Test Loss: 0.57862\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(datasets['train'], args['num_layers'], emb_size=args['emb_size']).to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "best_model = train(model, optimizer, args)\n",
    "log = \"Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Train Loss: {:.5f}, Val Loss: {:.5f}, Test Loss: {:.5f}\"\n",
    "best_train_roc, train_loss = test(best_model, 'train', args)\n",
    "best_val_roc, val_loss = test(best_model, 'val', args)\n",
    "best_test_roc, test_loss = test(best_model, 'test', args)\n",
    "print(log.format(best_train_roc, best_val_roc, best_test_roc, train_loss, val_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "639674ee-a6bb-4de5-8d98-6dc8ea3222ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_save\n",
    "# filename = 'lgcn.pkl'\n",
    "# torch.save(model, filename)\n",
    "\n",
    "#datasets save\n",
    "# with open('datasets_deepsnap_dict.pkl', 'wb') as f:\n",
    "#     # pickle the dictionary and write it to the file\n",
    "#     pickle.dump(datasets, f)\n",
    "\n",
    "model = torch.load('lgcn.pkl')\n",
    "with open('datasets_deepsnap_dict.pkl', 'rb') as f:\n",
    "    # load the pickled dictionary from the file\n",
    "    datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "cf06e0ad-076a-4fb0-aa5f-1104ca357558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(losses, title):\n",
    "    train_loss, val_loss = zip(*losses)\n",
    "    steps = list(range(1, len(train_loss) + 1))\n",
    "    \n",
    "    min_val_loss = np.round(np.min(val_loss), 3)\n",
    "    # train_list = [math.log10(x) for x in train_loss]\n",
    "    # val_list = [math.log10(x) for x in val_loss]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(steps, train_loss, '-r', label='Training Loss')\n",
    "    plt.plot(steps, val_loss, '-b', label='Validation Loss')\n",
    "    plt.hlines(min_val_loss, 1, 300, colors='k', linestyles='dotted', label='Min Validation Loss: {}'.format(min_val_loss))\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    # plt.ylim((0.58, 0.71))\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "fb1bc6a1-38ca-49d8-9ecc-1d9be4dd8d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(losses, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "a8b4dfc4-cc06-41fd-a294-bf2cf2690035",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['test']\n",
    "pred = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "4b551daf-fd42-455c-ac80-b9aa0908fe81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7070, 0.6771, 0.6789,  ..., 0.4114, 0.4883, 0.4203], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "77372322-0b1e-4470-92ae-a839a2abfd74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "725284"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "71e03714-d91b-4cc9-b2fb-ceffeb950090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(G=[], edge_index=[2, 3263754], edge_label=[725284], edge_label_index=[2, 725284], name=[38565], negative_label_val=[1], node_label_index=[38565], uri=[38565])"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets['test']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b5a8a5a8-056c-4dc1-a4c0-c4b272c0af85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "0e088514-d2e7-48b0-80a7-def67abae7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds.edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "4ba87275-4205-40ac-b8f6-774768d24d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>edge_label</th>\n",
       "      <th>src_node</th>\n",
       "      <th>dest_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.706988</td>\n",
       "      <td>1</td>\n",
       "      <td>888</td>\n",
       "      <td>33215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.677142</td>\n",
       "      <td>1</td>\n",
       "      <td>796</td>\n",
       "      <td>29493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.678916</td>\n",
       "      <td>1</td>\n",
       "      <td>484</td>\n",
       "      <td>20997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.665643</td>\n",
       "      <td>1</td>\n",
       "      <td>567</td>\n",
       "      <td>22428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.675998</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>22430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      preds  edge_label  src_node  dest_node\n",
       "0  0.706988           1       888      33215\n",
       "1  0.677142           1       796      29493\n",
       "2  0.678916           1       484      20997\n",
       "3  0.665643           1       567      22428\n",
       "4  0.675998           1       560      22430"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1 = {'preds':pred.to('cpu').detach().numpy(), 'edge_label': ds.edge_label.to('cpu').detach().numpy(), 'src_node': ds.edge_label_index[0].to('cpu').detach().numpy(), 'dest_node': ds.edge_label_index[1].to('cpu').detach().numpy()}\n",
    "df = pd.DataFrame(dict1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "ea71fb42-fc5b-4151-931b-dc027d0cf224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>edge_label</th>\n",
       "      <th>src_node</th>\n",
       "      <th>dest_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362642</th>\n",
       "      <td>0.526130</td>\n",
       "      <td>0</td>\n",
       "      <td>27028</td>\n",
       "      <td>22087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362643</th>\n",
       "      <td>0.811317</td>\n",
       "      <td>0</td>\n",
       "      <td>32596</td>\n",
       "      <td>34830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362644</th>\n",
       "      <td>0.470571</td>\n",
       "      <td>0</td>\n",
       "      <td>12490</td>\n",
       "      <td>37364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362645</th>\n",
       "      <td>0.785511</td>\n",
       "      <td>0</td>\n",
       "      <td>12379</td>\n",
       "      <td>10949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362646</th>\n",
       "      <td>0.321483</td>\n",
       "      <td>0</td>\n",
       "      <td>22202</td>\n",
       "      <td>32728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725279</th>\n",
       "      <td>0.412675</td>\n",
       "      <td>0</td>\n",
       "      <td>23203</td>\n",
       "      <td>17151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725280</th>\n",
       "      <td>0.397621</td>\n",
       "      <td>0</td>\n",
       "      <td>8408</td>\n",
       "      <td>23257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725281</th>\n",
       "      <td>0.411401</td>\n",
       "      <td>0</td>\n",
       "      <td>7501</td>\n",
       "      <td>27248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725282</th>\n",
       "      <td>0.488291</td>\n",
       "      <td>0</td>\n",
       "      <td>30544</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725283</th>\n",
       "      <td>0.420324</td>\n",
       "      <td>0</td>\n",
       "      <td>36239</td>\n",
       "      <td>18796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362642 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           preds  edge_label  src_node  dest_node\n",
       "362642  0.526130           0     27028      22087\n",
       "362643  0.811317           0     32596      34830\n",
       "362644  0.470571           0     12490      37364\n",
       "362645  0.785511           0     12379      10949\n",
       "362646  0.321483           0     22202      32728\n",
       "...          ...         ...       ...        ...\n",
       "725279  0.412675           0     23203      17151\n",
       "725280  0.397621           0      8408      23257\n",
       "725281  0.411401           0      7501      27248\n",
       "725282  0.488291           0     30544          9\n",
       "725283  0.420324           0     36239      18796\n",
       "\n",
       "[362642 rows x 4 columns]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0 = df[df['edge_label'] == 0]\n",
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "ffb0e93e-b815-47f1-8a14-6b56b4ac0f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>edge_label</th>\n",
       "      <th>src_node</th>\n",
       "      <th>dest_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362642</th>\n",
       "      <td>0.526130</td>\n",
       "      <td>0</td>\n",
       "      <td>27028</td>\n",
       "      <td>22087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362643</th>\n",
       "      <td>0.811317</td>\n",
       "      <td>0</td>\n",
       "      <td>32596</td>\n",
       "      <td>34830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362645</th>\n",
       "      <td>0.785511</td>\n",
       "      <td>0</td>\n",
       "      <td>12379</td>\n",
       "      <td>10949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362647</th>\n",
       "      <td>0.510759</td>\n",
       "      <td>0</td>\n",
       "      <td>1574</td>\n",
       "      <td>7400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362648</th>\n",
       "      <td>0.512315</td>\n",
       "      <td>0</td>\n",
       "      <td>37935</td>\n",
       "      <td>2377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725267</th>\n",
       "      <td>0.605943</td>\n",
       "      <td>0</td>\n",
       "      <td>17330</td>\n",
       "      <td>21269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725268</th>\n",
       "      <td>0.694120</td>\n",
       "      <td>0</td>\n",
       "      <td>12287</td>\n",
       "      <td>9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725270</th>\n",
       "      <td>0.557932</td>\n",
       "      <td>0</td>\n",
       "      <td>20264</td>\n",
       "      <td>15797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725273</th>\n",
       "      <td>0.872649</td>\n",
       "      <td>0</td>\n",
       "      <td>26852</td>\n",
       "      <td>26885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725276</th>\n",
       "      <td>0.564564</td>\n",
       "      <td>0</td>\n",
       "      <td>7257</td>\n",
       "      <td>3760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101994 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           preds  edge_label  src_node  dest_node\n",
       "362642  0.526130           0     27028      22087\n",
       "362643  0.811317           0     32596      34830\n",
       "362645  0.785511           0     12379      10949\n",
       "362647  0.510759           0      1574       7400\n",
       "362648  0.512315           0     37935       2377\n",
       "...          ...         ...       ...        ...\n",
       "725267  0.605943           0     17330      21269\n",
       "725268  0.694120           0     12287       9784\n",
       "725270  0.557932           0     20264      15797\n",
       "725273  0.872649           0     26852      26885\n",
       "725276  0.564564           0      7257       3760\n",
       "\n",
       "[101994 rows x 4 columns]"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lessthan = df_0[df_0['preds'] > 0.5]\n",
    "df_lessthan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "0549d1f9-a11c-4316-8db8-46f1eba89cef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27028.0"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lessthan.iloc[0]['src_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "9a9737a6-f63e-43e4-86c2-3f7817d9120f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtlasView({1000: {}, 1001: {}, 1002: {}, 1003: {}, 1004: {}, 1005: {}, 1006: {}, 1007: {}, 1008: {}, 1009: {}, 1010: {}, 1011: {}, 1012: {}, 1013: {}, 1014: {}, 1015: {}, 1016: {}, 1017: {}, 1018: {}, 1019: {}, 1020: {}, 1021: {}, 1022: {}, 1023: {}, 1024: {}, 1025: {}, 1026: {}, 1027: {}, 1028: {}, 1029: {}, 1030: {}, 1031: {}, 1032: {}, 1033: {}, 1034: {}, 1035: {}, 1036: {}, 1037: {}, 1038: {}, 1039: {}, 1040: {}, 1041: {}, 1042: {}, 1043: {}, 1044: {}, 1045: {}, 1046: {}, 1047: {}, 1048: {}, 1049: {}, 1050: {}, 1051: {}, 1052: {}, 1053: {}, 1054: {}, 1055: {}, 1056: {}, 1057: {}, 1058: {}, 1059: {}, 1060: {}, 1061: {}, 1062: {}, 1063: {}, 1064: {}, 1065: {}, 1066: {}, 1067: {}, 1068: {}, 1069: {}, 1070: {}, 1071: {}, 1072: {}, 1073: {}, 1074: {}, 1075: {}, 1076: {}, 1077: {}, 1078: {}, 1079: {}, 1080: {}, 1081: {}, 1082: {}, 1083: {}, 1084: {}, 1085: {}, 1086: {}, 1087: {}, 1088: {}, 1089: {}, 1090: {}, 1091: {}, 1092: {}, 1093: {}, 1094: {}, 1095: {}, 1096: {}, 1097: {}, 1098: {}, 1099: {}, 1100: {}, 1101: {}, 1102: {}, 1103: {}, 1104: {}, 1105: {}, 1106: {}, 1107: {}, 1108: {}, 1109: {}, 1110: {}, 1111: {}, 1112: {}, 1113: {}, 1114: {}, 1115: {}, 1116: {}, 1117: {}, 1118: {}, 1119: {}, 1120: {}, 1121: {}, 1122: {}, 1123: {}, 1124: {}, 1125: {}, 1126: {}, 1127: {}, 1128: {}, 1129: {}, 1130: {}, 1131: {}, 1132: {}, 1133: {}, 1134: {}, 1135: {}, 1136: {}, 1137: {}, 1138: {}, 1139: {}, 1140: {}, 1141: {}, 1142: {}, 1143: {}, 1144: {}, 1145: {}, 1146: {}, 1147: {}, 1148: {}, 1149: {}, 1150: {}, 1151: {}, 1152: {}, 1153: {}, 1154: {}, 1155: {}, 1156: {}, 1157: {}, 1158: {}, 1159: {}, 1160: {}, 1161: {}, 1162: {}, 1163: {}, 1164: {}, 1165: {}, 1166: {}, 1167: {}, 1168: {}, 1169: {}, 1170: {}, 1171: {}, 1172: {}, 1173: {}, 1174: {}, 1175: {}, 1176: {}, 1177: {}, 1178: {}, 1179: {}, 1180: {}, 1181: {}, 1182: {}, 1183: {}, 1184: {}, 1185: {}, 1186: {}, 1187: {}, 1188: {}, 1189: {}, 1190: {}, 1191: {}, 1192: {}, 1193: {}, 1194: {}, 1195: {}, 1196: {}, 1197: {}, 1198: {}, 1199: {}, 1200: {}, 1201: {}, 1202: {}, 1203: {}, 1204: {}, 1205: {}, 1206: {}, 1207: {}, 1208: {}, 1209: {}, 1210: {}, 1211: {}, 1212: {}, 1213: {}, 1214: {}, 1215: {}, 1216: {}, 1217: {}, 1218: {}, 1219: {}, 1220: {}, 1221: {}, 1222: {}, 1223: {}, 1224: {}, 1225: {}, 1226: {}, 1227: {}, 1228: {}, 1229: {}, 1230: {}, 1231: {}, 1232: {}, 1233: {}, 1234: {}, 1235: {}})"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_nx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "a2a70249-0d4c-4d83-9356-4927956b0d99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lessthan.iloc[0][\"src_node\"]\n",
    "counter=0\n",
    "neg=0\n",
    "for x in g_nx.nodes:\n",
    "    if(counter!=x):\n",
    "        neg=neg+x-counter\n",
    "        counter=x\n",
    "    counter=counter+1\n",
    "print(len(g_nx.nodes))\n",
    "neg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84feddca-e722-4ba3-88f6-52870d95a38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "bc283a02-b0bc-49ea-8776-0b79f321be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_op(row):\n",
    "\n",
    "    if g_nx.nodes[row['src_node']]['name'] == g_nx.nodes[row['dest_node']]['name'] or g_nx.nodes[row['src_node']]['uri'] == g_nx.nodes[row['dest_node']]['uri']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df_lessthan['is_valid'] = df_lessthan.apply(row_op,axis=1)\n",
    "df_filter=df_lessthan[df_lessthan['is_valid']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "6e9d3ae7-48bf-480d-a8de-883fa4c6085f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getId(row):\n",
    "    return pd.Series([g_nx.nodes[row['src_node']]['name'],g_nx.nodes[row['src_node']]['uri'],g_nx.nodes[row['src_node']]['type'],g_nx.nodes[row['dest_node']]['name'],g_nx.nodes[row['dest_node']]['uri'],g_nx.nodes[row['dest_node']]['type']])\n",
    "    \n",
    "df_filter[[\"src_name\",\"src_uri\",\"src_type\",\"dest_name\",\"dest_uri\",\"dest_type\"]]=df_filter[df_lessthan['is_valid']].apply(getId,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "95775e86-1b83-4f29-b8b5-373e2f0de91e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>edge_label</th>\n",
       "      <th>src_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>src_name</th>\n",
       "      <th>src_uri</th>\n",
       "      <th>src_type</th>\n",
       "      <th>dest_name</th>\n",
       "      <th>dest_uri</th>\n",
       "      <th>dest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362647</th>\n",
       "      <td>0.510759</td>\n",
       "      <td>0</td>\n",
       "      <td>1574</td>\n",
       "      <td>7400</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:7gUDfyfjTIcrFijLRvv6AX</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362649</th>\n",
       "      <td>0.655622</td>\n",
       "      <td>0</td>\n",
       "      <td>31870</td>\n",
       "      <td>37070</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:6Rw3g4D3BCiipubFnmHocK</td>\n",
       "      <td>track</td>\n",
       "      <td>2016</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362686</th>\n",
       "      <td>0.509911</td>\n",
       "      <td>0</td>\n",
       "      <td>1656</td>\n",
       "      <td>7428</td>\n",
       "      <td>True</td>\n",
       "      <td>😈😈😈</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:6tg8tJPrrFuofADSiZPsWw</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362711</th>\n",
       "      <td>0.538797</td>\n",
       "      <td>0</td>\n",
       "      <td>30486</td>\n",
       "      <td>36983</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:0Yqkp4ScyWSKTlm7hpnDQN</td>\n",
       "      <td>track</td>\n",
       "      <td>Sleep</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362776</th>\n",
       "      <td>0.507164</td>\n",
       "      <td>0</td>\n",
       "      <td>10576</td>\n",
       "      <td>1109</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:7Av3vXv9nv2pVT29QZX4B6</td>\n",
       "      <td>track</td>\n",
       "      <td>Brasileiras</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725119</th>\n",
       "      <td>0.577482</td>\n",
       "      <td>0</td>\n",
       "      <td>1989</td>\n",
       "      <td>5221</td>\n",
       "      <td>True</td>\n",
       "      <td>Michelle</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:2qF8Zpn7iJh9mk4RfwLbjM</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725157</th>\n",
       "      <td>0.604030</td>\n",
       "      <td>0</td>\n",
       "      <td>1259</td>\n",
       "      <td>2930</td>\n",
       "      <td>True</td>\n",
       "      <td>brad paisley</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:0j6YwWZVHsIFKr5FDEu68o</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725186</th>\n",
       "      <td>0.512304</td>\n",
       "      <td>0</td>\n",
       "      <td>36357</td>\n",
       "      <td>1149</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:5wpDupdKxD4aKNV8kbtdYt</td>\n",
       "      <td>track</td>\n",
       "      <td>Top hits</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725242</th>\n",
       "      <td>0.584185</td>\n",
       "      <td>0</td>\n",
       "      <td>1576</td>\n",
       "      <td>4524</td>\n",
       "      <td>True</td>\n",
       "      <td>sound</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:53i9QgdZ2X0DdvqDOvt7r4</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725261</th>\n",
       "      <td>0.503086</td>\n",
       "      <td>0</td>\n",
       "      <td>15498</td>\n",
       "      <td>1605</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:52ojopYMUzeNcudsoz7O9D</td>\n",
       "      <td>track</td>\n",
       "      <td>Country</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15364 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           preds  edge_label  src_node  dest_node  is_valid       src_name  \\\n",
       "362647  0.510759           0      1574       7400      True      Party mix   \n",
       "362649  0.655622           0     31870      37070      True                  \n",
       "362686  0.509911           0      1656       7428      True            😈😈😈   \n",
       "362711  0.538797           0     30486      36983      True                  \n",
       "362776  0.507164           0     10576       1109      True                  \n",
       "...          ...         ...       ...        ...       ...            ...   \n",
       "725119  0.577482           0      1989       5221      True      Michelle    \n",
       "725157  0.604030           0      1259       2930      True  brad paisley    \n",
       "725186  0.512304           0     36357       1149      True                  \n",
       "725242  0.584185           0      1576       4524      True          sound   \n",
       "725261  0.503086           0     15498       1605      True                  \n",
       "\n",
       "                                     src_uri  src_type    dest_name  \\\n",
       "362647                                        playlist                \n",
       "362649  spotify:track:6Rw3g4D3BCiipubFnmHocK     track         2016   \n",
       "362686                                        playlist                \n",
       "362711  spotify:track:0Yqkp4ScyWSKTlm7hpnDQN     track        Sleep   \n",
       "362776  spotify:track:7Av3vXv9nv2pVT29QZX4B6     track  Brasileiras   \n",
       "...                                      ...       ...          ...   \n",
       "725119                                        playlist                \n",
       "725157                                        playlist                \n",
       "725186  spotify:track:5wpDupdKxD4aKNV8kbtdYt     track     Top hits   \n",
       "725242                                        playlist                \n",
       "725261  spotify:track:52ojopYMUzeNcudsoz7O9D     track      Country   \n",
       "\n",
       "                                    dest_uri dest_type  \n",
       "362647  spotify:track:7gUDfyfjTIcrFijLRvv6AX     track  \n",
       "362649                                        playlist  \n",
       "362686  spotify:track:6tg8tJPrrFuofADSiZPsWw     track  \n",
       "362711                                        playlist  \n",
       "362776                                        playlist  \n",
       "...                                      ...       ...  \n",
       "725119  spotify:track:2qF8Zpn7iJh9mk4RfwLbjM     track  \n",
       "725157  spotify:track:0j6YwWZVHsIFKr5FDEu68o     track  \n",
       "725186                                        playlist  \n",
       "725242  spotify:track:53i9QgdZ2X0DdvqDOvt7r4     track  \n",
       "725261                                        playlist  \n",
       "\n",
       "[15364 rows x 11 columns]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "68f80f71-0bae-46b0-bb2d-411ee0015f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = df_filter[(df_filter['src_name']==\"Party mix\") | ( df_filter['dest_name']==\"Party mix\") ]\n",
    "df_final.sort_values('preds',ascending=False)\n",
    "uri_list=list(df_final.apply(lambda x: x['src_uri'] if x['src_uri']!='' else x['dest_uri'], axis=1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "cde8a113-86ca-4a1d-96d3-4fba616b28c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spotipy\n",
      "  Downloading spotipy-2.22.1-py3-none-any.whl (28 kB)\n",
      "Collecting redis>=3.5.3\n",
      "  Downloading redis-4.5.4-py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.25.0 in /home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages (from spotipy) (2.28.2)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages (from spotipy) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages (from spotipy) (1.26.14)\n",
      "Collecting async-timeout>=4.0.2\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/asd27/.conda/envs/cudatorch/lib/python3.11/site-packages (from requests>=2.25.0->spotipy) (2022.12.7)\n",
      "Installing collected packages: async-timeout, redis, spotipy\n",
      "Successfully installed async-timeout-4.0.2 redis-4.5.4 spotipy-2.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "74b5ba22-cf68-4f75-9e03-22f582a16315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uri_list=[]\n",
    "uri_list = list(df_final[\"dest_uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "6083828e-03cc-4ce3-9494-fd35a83ae9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>edge_label</th>\n",
       "      <th>src_node</th>\n",
       "      <th>dest_node</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>src_name</th>\n",
       "      <th>src_uri</th>\n",
       "      <th>src_type</th>\n",
       "      <th>dest_name</th>\n",
       "      <th>dest_uri</th>\n",
       "      <th>dest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362647</th>\n",
       "      <td>0.510759</td>\n",
       "      <td>0</td>\n",
       "      <td>1574</td>\n",
       "      <td>7400</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:7gUDfyfjTIcrFijLRvv6AX</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403861</th>\n",
       "      <td>0.632110</td>\n",
       "      <td>0</td>\n",
       "      <td>27010</td>\n",
       "      <td>575</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:50Pu9PTbnujupgl2LLwn5y</td>\n",
       "      <td>track</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450506</th>\n",
       "      <td>0.509937</td>\n",
       "      <td>0</td>\n",
       "      <td>1574</td>\n",
       "      <td>7667</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:3uy0jtkM8QYVTsBazkli1x</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459937</th>\n",
       "      <td>0.502560</td>\n",
       "      <td>0</td>\n",
       "      <td>16597</td>\n",
       "      <td>1574</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:705u4LfXcTBuDOB8UCy0A7</td>\n",
       "      <td>track</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460682</th>\n",
       "      <td>0.551944</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>29008</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:2R6RRpbjQ9zSSzdqHMoisV</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485977</th>\n",
       "      <td>0.510032</td>\n",
       "      <td>0</td>\n",
       "      <td>7638</td>\n",
       "      <td>1574</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:13VU9JTEVLrDfFUi7ZhuA5</td>\n",
       "      <td>track</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494971</th>\n",
       "      <td>0.598414</td>\n",
       "      <td>0</td>\n",
       "      <td>27865</td>\n",
       "      <td>575</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:3PfBUAunz826hdvH225T0K</td>\n",
       "      <td>track</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498630</th>\n",
       "      <td>0.521360</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>29667</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:1GINsWBySdj1yI0vtjzlQz</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532552</th>\n",
       "      <td>0.544246</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>29159</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:7D3zhmFnxJSgI7cvkXan4l</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566967</th>\n",
       "      <td>0.654191</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>26311</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:2V8J29IIjB0SbwhvKMijrV</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641748</th>\n",
       "      <td>0.509674</td>\n",
       "      <td>0</td>\n",
       "      <td>1574</td>\n",
       "      <td>8115</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:4Vxu50qVrQcycjRyJQaZLC</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675285</th>\n",
       "      <td>0.686479</td>\n",
       "      <td>0</td>\n",
       "      <td>23626</td>\n",
       "      <td>575</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:6WNZ22iJe51C0CfcT97GKC</td>\n",
       "      <td>track</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722548</th>\n",
       "      <td>0.538181</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>19498</td>\n",
       "      <td>True</td>\n",
       "      <td>Party mix</td>\n",
       "      <td></td>\n",
       "      <td>playlist</td>\n",
       "      <td></td>\n",
       "      <td>spotify:track:3l0pdat6N0XcotNr2b7rNl</td>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           preds  edge_label  src_node  dest_node  is_valid   src_name  \\\n",
       "362647  0.510759           0      1574       7400      True  Party mix   \n",
       "403861  0.632110           0     27010        575      True              \n",
       "450506  0.509937           0      1574       7667      True  Party mix   \n",
       "459937  0.502560           0     16597       1574      True              \n",
       "460682  0.551944           0       575      29008      True  Party mix   \n",
       "485977  0.510032           0      7638       1574      True              \n",
       "494971  0.598414           0     27865        575      True              \n",
       "498630  0.521360           0       575      29667      True  Party mix   \n",
       "532552  0.544246           0       575      29159      True  Party mix   \n",
       "566967  0.654191           0       575      26311      True  Party mix   \n",
       "641748  0.509674           0      1574       8115      True  Party mix   \n",
       "675285  0.686479           0     23626        575      True              \n",
       "722548  0.538181           0       575      19498      True  Party mix   \n",
       "\n",
       "                                     src_uri  src_type  dest_name  \\\n",
       "362647                                        playlist              \n",
       "403861  spotify:track:50Pu9PTbnujupgl2LLwn5y     track  Party mix   \n",
       "450506                                        playlist              \n",
       "459937  spotify:track:705u4LfXcTBuDOB8UCy0A7     track  Party mix   \n",
       "460682                                        playlist              \n",
       "485977  spotify:track:13VU9JTEVLrDfFUi7ZhuA5     track  Party mix   \n",
       "494971  spotify:track:3PfBUAunz826hdvH225T0K     track  Party mix   \n",
       "498630                                        playlist              \n",
       "532552                                        playlist              \n",
       "566967                                        playlist              \n",
       "641748                                        playlist              \n",
       "675285  spotify:track:6WNZ22iJe51C0CfcT97GKC     track  Party mix   \n",
       "722548                                        playlist              \n",
       "\n",
       "                                    dest_uri dest_type  \n",
       "362647  spotify:track:7gUDfyfjTIcrFijLRvv6AX     track  \n",
       "403861                                        playlist  \n",
       "450506  spotify:track:3uy0jtkM8QYVTsBazkli1x     track  \n",
       "459937                                        playlist  \n",
       "460682  spotify:track:2R6RRpbjQ9zSSzdqHMoisV     track  \n",
       "485977                                        playlist  \n",
       "494971                                        playlist  \n",
       "498630  spotify:track:1GINsWBySdj1yI0vtjzlQz     track  \n",
       "532552  spotify:track:7D3zhmFnxJSgI7cvkXan4l     track  \n",
       "566967  spotify:track:2V8J29IIjB0SbwhvKMijrV     track  \n",
       "641748  spotify:track:4Vxu50qVrQcycjRyJQaZLC     track  \n",
       "675285                                        playlist  \n",
       "722548  spotify:track:3l0pdat6N0XcotNr2b7rNl     track  "
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "e555332b-4acd-4e25-8649-8c794c3fbb23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spotify:track:7gUDfyfjTIcrFijLRvv6AX',\n",
       " 'spotify:track:3uy0jtkM8QYVTsBazkli1x',\n",
       " 'spotify:track:2R6RRpbjQ9zSSzdqHMoisV',\n",
       " 'spotify:track:1GINsWBySdj1yI0vtjzlQz',\n",
       " 'spotify:track:7D3zhmFnxJSgI7cvkXan4l',\n",
       " 'spotify:track:2V8J29IIjB0SbwhvKMijrV',\n",
       " 'spotify:track:4Vxu50qVrQcycjRyJQaZLC',\n",
       " 'spotify:track:3l0pdat6N0XcotNr2b7rNl',\n",
       " 'spotify:track:50Pu9PTbnujupgl2LLwn5y',\n",
       " 'spotify:track:705u4LfXcTBuDOB8UCy0A7',\n",
       " 'spotify:track:13VU9JTEVLrDfFUi7ZhuA5',\n",
       " 'spotify:track:3PfBUAunz826hdvH225T0K',\n",
       " 'spotify:track:6WNZ22iJe51C0CfcT97GKC']"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "8f07ffd0-0996-4152-8860-2de9e3b8762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mamá Me Lo Contó', 'Gente De Zona'), ('Cómo te echo de menos', 'Alejandro Sanz'), ('Planet Rock', 'Afrika Bambaataa'), ('Kush & Corinthians', 'Kendrick Lamar'), ('Never Ever', 'Wiz Khalifa'), ('Satisfaction - Afrojack Remix', 'Benny Benassi'), ('Fix Me', 'Jasmine Thompson'), ('Willie, Waylon and Me', 'David Allan Coe'), ('Airborne', 'Running Cadence'), ('愛我請留言 - (劇集 \"愛我請留言\" 主題曲)', 'Jinny Ng'), ('Life Changes', 'Thomas Rhett'), ('Ms. Gravystone (feat. Mia Gladstone)', 'Yung Gravy'), ('A Violent Sky', 'Apparat')]\n"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "def gettrackname(uri_list):\n",
    "    # Replace the values below with your own Spotify API credentials\n",
    "    client_id = 'd5566a60926740f3a8070889731a2d21'\n",
    "    client_secret = 'eb5fc0638a1241c3a611186ff8d167e3'\n",
    "\n",
    "    # Initialize the Spotify API client\n",
    "    client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "    sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "    info = []\n",
    "    for uri in uri_list:\n",
    "        # Use the track method to get information about the track\n",
    "        track_info = sp.track(uri)\n",
    "\n",
    "        # Get the track name from the track information\n",
    "        track_name = track_info['name']\n",
    "        track_info = sp.track(uri)\n",
    "\n",
    "        # Get the artist name from the track information\n",
    "        artist_name = track_info['artists'][0]['name']\n",
    "        info.append((track_name, artist_name))\n",
    "        \n",
    "    return info\n",
    "\n",
    "print(gettrackname(uri_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97554ce9-7e68-4b62-8185-f6266bf56a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81b8d7-8780-46d1-8f13-6b622f83928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b4c06-d86a-4359-9420-63bdb8742c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df22e9-6716-4101-a0a9-7a06fdace80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0b57a-939a-43c7-b90e-81f7a512aa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d4504-c4c3-41a6-a184-7c10579874af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
